{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫简介\n",
    "\n",
    "爬虫一般指从网站上获得需要的数据，这个过程其实是建造一个网站的逆向过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Flask建立简单网站\n",
    "\n",
    "所谓知己知彼，则百战不殆。既然爬虫主要是从网站上爬取数据，那么只有对建造网站的步骤一清二楚，才能在变幻多端的网站环境下，获取想要的数据。\n",
    "\n",
    "然而实际建立网站的过程是非常繁琐而复杂的，写HTML、CSS仅仅是一个网站的冰山一角，这其中至少涉及到网站架构设计、数据库设计、后台服务设计、前端设计等等，此外根据不同的架构设计，还有很多中间件、缓存数据库等等复杂的设计。\n",
    "\n",
    "不过，对于比较简单的网站，Python本身就有很多成熟的框架可以方便我们快速搭建一个简单的网站。这其中，Flask由于其比较精美的设计架构以及简单的模板等应用，是非常受欢迎的轻量级Web框架。在这里，我们不妨使用Flask搭建一个最简单的网站（因为在Jupyter里面，所以我把run()给注释掉了，如果需要执行，请直接执行html/web.py）：\n",
    "```python\n",
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "import random\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    with open(\"example4.html\") as f:\n",
    "        html=f.read()\n",
    "    return html\n",
    "\n",
    "@app.route('/dynamic')\n",
    "def dynamic():\n",
    "    with open(\"dynamic.html\") as f:\n",
    "        html=f.read()\n",
    "    return html\n",
    "\n",
    "@app.route('/dynamic_response')\n",
    "def dynamic_response():\n",
    "    return str(random.random())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #app.run()\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在执行以上命令之前，需要首先使用pip install flask安装Flask。\n",
    "\n",
    "在上面的程序中，@代表修饰器（decorator），是Python编程的一个高级特性，我们暂且不管。我们需要知道的仅仅是，通过@app.route函数，声明了一个路径，该路径即访问接下来定义的页面的路径。\n",
    "\n",
    "在@app.route下方，我们定义了几个函数，这几个函数的作用是返回一个字符串，这些字符串会通过网络传递给访问的浏览器。\n",
    "\n",
    "如果运行html/web.py，会提示：\n",
    ">  Running on http://127.0.0.1:5000/\n",
    "\n",
    "此时，如果在浏览器中输入以上网址，服务器就会执行index()函数，该函数会读取example4.html，并将其返回，从而我们在浏览器上就看到了example4.html。\n",
    "\n",
    "同理，如果访问 http://127.0.0.1:5000/dynamic ，服务器就会执行dynamic()函数，将dynamic.html的内容返回。\n",
    "\n",
    "而如果访问 http://127.0.0.1:5000/dynamic_response ，服务器会执行dynamic_response()函数，该函数生成一个随机数并返回给浏览器。\n",
    "\n",
    "以上就是一个简单的网站。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 发送请求\n",
    "\n",
    "知道了服务器端如何处理页面，那么现在的问题是，客户端怎样与服务器端通讯呢？\n",
    "\n",
    "这里就要介绍以下HTTP协议的概念了。\n",
    "\n",
    "HTTP协议是**Hyper Text Transfer Protocol**（**超文本传输协议**）的缩写，一种服务器/客户端范式的传输协议，我们访问网址（URL）时，都是以http:// 开头的，或者https:// 开头，代表的就是使用http协议。\n",
    "\n",
    "一个最简单的传输模型是，客户端通过URL地址向服务器端发送**请求**（**request**），服务器根据所请求的地址、头部信息（headers）做出**响应**（**response**）。\n",
    "\n",
    "这里有几个概念要特别注意：\n",
    "\n",
    "## 网址URL\n",
    "\n",
    "一个常见的网址通常具有如下形式：\n",
    "\n",
    "> http://econpaper.cn:8080/article/article.jsp?id=56987&userid=3455\n",
    "\n",
    "其中：\n",
    "\n",
    "* http:// 代表协议\n",
    "* econpaper.cn 代表域名\n",
    "* :8080 代表端口号，默认为80端口\n",
    "* /article/article.jsp 部分为请求的页面路径\n",
    "* ?id=56987&userid=3455 为需要传递给这个页面的参数\n",
    "\n",
    "## 请求（request）\n",
    "\n",
    "客户端向服务器端发送请求，要按照一定的格式，请求消息由以下三部分组成：\n",
    "\n",
    "* 请求行（request line）\n",
    "* 头部（header）\n",
    "* 请求数据\n",
    "\n",
    "![](pic/request.png)\n",
    "\n",
    "比如，以下是一个典型的请求头部：\n",
    "\n",
    "![](pic/request_headers.png)\n",
    "\n",
    "这些信息传递给服务器后，服务器根据这些信息进行处理。\n",
    "\n",
    "实际上，在我们刚刚运行的Flask中，在客户端可以看到每一次的请求。\n",
    "\n",
    "此外，上面从URL地址中已经看到，?id=56987&userid=3455 为需要传递给这个页面的参数，实际上，这是一个GET的请求方法。为了从客户端向服务器端传数据，以下两种方法是最常用的：\n",
    "\n",
    "* GET：像上面一样，请求的数据明文写在URL上\n",
    "* POST：数据包含在请求体中\n",
    "\n",
    "当然，两种方法也可以结合起来使用。不过最为常用的仍然是GET方法。\n",
    "\n",
    "## 响应（response）\n",
    "\n",
    "有了请求，就会有服务器的响应。同样，响应也有响应头和数据体。在响应头中，最重要的是响应的状态码以及内容类型（html文档或者图片、pdf等），常见的状态码比如：\n",
    "\n",
    "* 200 OK                        客户端请求成功\n",
    "* 400 Bad Request               客户端请求有语法错误，不能被服务器所理解\n",
    "* 401 Unauthorized              请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用 \n",
    "* 403 Forbidden                 服务器收到请求，但是拒绝提供服务\n",
    "* 404 Not Found                 请求资源不存在，eg：输入了错误的URL\n",
    "* 500 Internal Server Error     服务器发生不可预期的错误\n",
    "* 503 Server Unavailable        服务器当前不能处理客户端的请求，一段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python中发送请求\n",
    "\n",
    "在Python中，有很多工具可以帮助我们向服务器发送请求，包括但不限于：urllib、urllib2、urllib3、httplib2、requests等等等等。出于个人偏好原因，在这里我们以requests为例，介绍如何发送请求。\n",
    "\n",
    "为了使用requests，需要先进行安装：pip install requests\n",
    "\n",
    "使用时直接导入：\n",
    "```python\n",
    "import requests\n",
    "```\n",
    "\n",
    "接着，最简单的请求即GET请求：\n",
    "```python\n",
    "r=requests.get(url)\n",
    "```\n",
    "\n",
    "其中url为需要请求的地址。\n",
    "\n",
    "有时我们在发送请求时可能需要控制发送请求的头部，头部可以使用一个字典表示，比如：\n",
    "```python\n",
    "headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36 OPR/60.0.3255.70\",\n",
    "        \"Accept-Encoding\":\"gzip, deflate, lzma, sdch\",\n",
    "        \"Accept-Language\":\"en-US,en;q=0.8\",\n",
    "        \"Connection\":\"keep-alive\",\n",
    "        \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"}\n",
    "```\n",
    "\n",
    "以上字典定义了头部的内容，接下来只要在get()函数中加入该头部就可以了：\n",
    "```python\n",
    "r=requests.get(url, headers=hearders)\n",
    "```\n",
    "\n",
    "返回结果：\n",
    "\n",
    "* r.text 可以读取返回的HTML\n",
    "* r.json() 获得Json数据转换后的字典数据\n",
    "* r.encoding 记录了返回数据的字符编码\n",
    "* r.status_code 记录了响应的状态码\n",
    "\n",
    "如果需要发送POST请求，需要首先将请求的数据写成字典形式，在使用urllib包中的urlencode函数将其编码，比如：\n",
    "```python\n",
    "data=dict(name=\"Joe\", comment=\"A test comment\")\n",
    "r=requests.post(url, data=data)\n",
    "```\n",
    "\n",
    "如果需要设置自定义的cookie到服务器，可以使用：\n",
    "```python\n",
    "cookies = dict(cookies_are='working')\n",
    "r=requests.get(url, cookies=cookies)\n",
    "```\n",
    "\n",
    "如果我们希望设置最长的等待时间，可以使用超时选项：\n",
    "```python\n",
    "r=requests.get(url, timeout=10)\n",
    "```\n",
    "\n",
    "即设定等待时间超过10s则放弃连接。\n",
    "\n",
    "最后，如果需要使用代理，可以使用：\n",
    "```python\n",
    "proxies = {\n",
    "  'http': 'http://user@password@10.10.1.10:3128',\n",
    "  'https': 'http://10.10.1.10:1080',\n",
    "}\n",
    "r=requests.get(url, proxies=proxies)\n",
    "```\n",
    "\n",
    "比如，使用一个获取微博热搜的API："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 200,\n",
       " 'msg': 'success',\n",
       " 'data': [{'hot_word': '张子枫梁靖康吻戏', 'hot_word_num': '2913276'},\n",
       "  {'hot_word': '操场埋尸案校长狱中忏悔', 'hot_word_num': '1763477'},\n",
       "  {'hot_word': '小米11超大杯', 'hot_word_num': '1421447'},\n",
       "  {'hot_word': '彭昱畅什么时候能有吻戏', 'hot_word_num': '1237277'},\n",
       "  {'hot_word': '5杯水如何分给6个领导', 'hot_word_num': '1192472'},\n",
       "  {'hot_word': '净化肌肤 极光水', 'hot_word_num': '1182377'},\n",
       "  {'hot_word': '妹妹长大了', 'hot_word_num': '1168995'},\n",
       "  {'hot_word': '郑州财经学院一男生在公寓死亡', 'hot_word_num': '1124776'},\n",
       "  {'hot_word': '创造营微博评论', 'hot_word_num': '940485'},\n",
       "  {'hot_word': '最大规模集中疫苗接种', 'hot_word_num': '928968'}],\n",
       " 'author': {'name': 'Alone88',\n",
       "  'desc': '由Alone88提供的免费API 服务，官方文档：www.alapi.cn'}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://v1.alapi.cn/api/new/wbtop?num=10\"\n",
    "import requests\n",
    "r=requests.get(url)\n",
    "data=r.json()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大多数接口都需要用一个token进行身份验证，一般会根据token进行一定的数量限制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码实现了从链家上爬去房价数据。值得注意的是，如果没有设定headers，可能会爬不下来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['107103573974', '3室1厅', '81.1平米', '松江大学城', '440万', '松江大学城/3室1厅/81.1平米/南/简装']\n",
      "['107103575873', '2室2厅', '82.8平米', '九亭', '455万', '九亭/2室2厅/82.8平米/南 北/精装']\n",
      "['107103682168', '3室2厅', '135.66平米', '莘闵别墅', '530万', '莘闵别墅/3室2厅/135.66平米/南/精装']\n",
      "['107103650449', '3室2厅', '109.21平米', '莘闵别墅', '445万', '莘闵别墅/3室2厅/109.21平米/南/精装']\n",
      "['107103664161', '2室2厅', '105.02平米', '松江新城', '370万', '松江新城/2室2厅/105.02平米/南 北/精装']\n",
      "['107103699591', '3室2厅', '89.96平米', '莘闵别墅', '368万', '莘闵别墅/3室2厅/89.96平米/南/精装']\n",
      "['107103722894', '3室2厅', '88.62平米', '泗泾', '350万', '泗泾/3室2厅/88.62平米/南 北/精装']\n",
      "['107103705588', '2室1厅', '108.54平米', '泗泾', '400万', '泗泾/2室1厅/108.54平米/南 北/简装']\n",
      "['107103675642', '4室2厅', '172.84平米', '莘闵别墅', '880万', '莘闵别墅/4室2厅/172.84平米/南/精装']\n",
      "['107102398373', '6室3厅', '978.62平米', '佘山', '3280万', '佘山/6室3厅/978.62平米/东 南 西 北/精装']\n",
      "['107103483159', '4室2厅', '213.57平米', '松江新城', '1270万', '松江新城/4室2厅/213.57平米/南/精装']\n",
      "['107103709103', '3室1厅', '63.35平米', '九亭', '360万', '九亭/3室1厅/63.35平米/南/精装']\n",
      "['107103703200', '1室1厅', '54平米', '泗泾', '225万', '泗泾/1室1厅/54平米/南/毛坯']\n",
      "['107103626046', '3室2厅', '100.44平米', '松江新城', '330万', '松江新城/3室2厅/100.44平米/南/简装']\n",
      "['107103676741', '2室2厅', '77.75平米', '泗泾', '280万', '泗泾/2室2厅/77.75平米/南 北/精装']\n",
      "['107103687542', '3室2厅', '85.56平米', '泗泾', '489万', '泗泾/3室2厅/85.56平米/南/精装']\n",
      "['107103679593', '3室2厅', '92.18平米', '松江大学城', '455万', '松江大学城/3室2厅/92.18平米/南/精装']\n",
      "['107103665332', '3室2厅', '126.13平米', '松江新城', '385万', '松江新城/3室2厅/126.13平米/南/简装']\n",
      "['107103608790', '3室2厅', '112.66平米', '九亭', '465万', '九亭/3室2厅/112.66平米/南/精装']\n",
      "['107103616398', '2室2厅', '81.75平米', '九亭', '435万', '九亭/2室2厅/81.75平米/南 北/精装']\n",
      "['107103623978', '2室1厅', '88.53平米', '莘闵别墅', '345万', '莘闵别墅/2室1厅/88.53平米/南/精装']\n",
      "['107103671437', '2室2厅', '69.36平米', '松江大学城', '289万', '松江大学城/2室2厅/69.36平米/南/简装']\n",
      "['107103633305', '3室2厅', '147.6平米', '莘闵别墅', '680万', '莘闵别墅/3室2厅/147.6平米/南/精装']\n",
      "['107103668872', '3室2厅', '134.25平米', '九亭', '579万', '九亭/3室2厅/134.25平米/南/精装']\n",
      "['107102306098', '5室3厅', '350.04平米', '九亭', '1600万', '九亭/5室3厅/350.04平米/南 北/精装']\n",
      "['107103536767', '5室2厅', '216平米', '莘闵别墅', '1200万', '莘闵别墅/5室2厅/216平米/南/精装']\n",
      "['107103595266', '5室3厅', '330平米', '九亭', '1350万', '九亭/5室3厅/330平米/南/毛坯']\n",
      "['107103546132', '3室1厅', '100.79平米', '九亭', '535万', '九亭/3室1厅/100.79平米/南/精装']\n",
      "['107103701245', '1室1厅', '40.9平米', '九亭', '220万', '九亭/1室1厅/40.9平米/南/精装']\n",
      "['107103705691', '1室2厅', '66.2平米', '九亭', '315万', '九亭/1室2厅/66.2平米/南/简装']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36 OPR/60.0.3255.70\",\n",
    "        \"Accept-Encoding\":\"gzip, deflate, lzma, sdch\",\n",
    "        \"Accept-Language\":\"en-US,en;q=0.8\",\n",
    "        \"Connection\":\"keep-alive\",\n",
    "        \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"}\n",
    "url='https://sh.lianjia.com/ershoufang/songjiang/'\n",
    "r=requests.get(url, headers=headers)\n",
    "html=r.text\n",
    "bs=BeautifulSoup(html,\"html.parser\")\n",
    "iterms=bs.find_all(name='div',attrs={\"class\":\"item\"})\n",
    "info=[]\n",
    "for it in iterms:\n",
    "    house_id=it.get(\"data-houseid\")\n",
    "    prop=it.find(name='div',attrs={\"class\":\"info\"}).text\n",
    "    shi_ting=re.search(r\"\\d室\\d厅\",prop).group()\n",
    "    area=re.search(r\"\\d*(\\.\\d*)?平米\",prop).group()\n",
    "    location=prop[:prop.find('/')]\n",
    "    price=it.find(name='div',attrs={\"class\":\"price\"}).text\n",
    "    print([house_id,shi_ting,area,location,price,prop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们使用一个更加综合的例子：百度新闻的爬取："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "### funcs.py\n",
    "#!/usr/bin/python3\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "## write log file.\n",
    "def writeLog(content):\n",
    "    try:\n",
    "        logfile=open('News.log','a')\n",
    "        ltime=time.localtime()\n",
    "        trstr=lambda s:('0'+str(s))[-2:]\n",
    "        logfile.write(str(ltime.tm_year)+'-'+trstr(ltime.tm_mon)+'-'+trstr(ltime.tm_mday)+' '\n",
    "                      +trstr(ltime.tm_hour)+':'+trstr(ltime.tm_min)+':'+trstr(ltime.tm_sec)+'-->'+\n",
    "                      content+'\\n')\n",
    "        logfile.close()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "## get htmltext\n",
    "def getPage(url,trytimes=10,tot=10):\n",
    "    trytime=1\n",
    "    headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36 OPR/60.0.3255.70\",\n",
    "        \"Accept-Encoding\":\"gzip, deflate, lzma, sdch\",\n",
    "        \"Accept-Language\":\"en-US,en;q=0.8\",\n",
    "        \"Connection\":\"keep-alive\",\n",
    "        \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"\n",
    "        }\n",
    "    while True:\n",
    "        try:\n",
    "            r=requests.get(url, headers=headers, timeout=tot, stream=False)\n",
    "            htmlText=r.text\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if trytime>=trytimes:\n",
    "                print(e)\n",
    "                writeLog('Error happened in reading '+url+':'+str(e)+'.')\n",
    "                return ''\n",
    "            else:\n",
    "                trytime=trytime+1\n",
    "                time.sleep(10)\n",
    "    return htmlText\n",
    "# get current date\n",
    "def getDate():\n",
    "    ts=time.localtime()\n",
    "    trstr=lambda s:('0'+str(s))[-2:]\n",
    "    return '-'.join((str(ts.tm_year),trstr(ts.tm_mon),trstr(ts.tm_mday)))+' '+':'.join((trstr(ts.tm_hour),trstr(ts.tm_min)))\n",
    "# get the longest content\n",
    "def getLongText(url):\n",
    "    html=getPage(url,trytimes=2)\n",
    "    bs=BeautifulSoup(html,\"html.parser\")\n",
    "    content=\"\"\n",
    "    ## delete all scripts\n",
    "    while bs.script!=None:\n",
    "        bs.script.extract()\n",
    "    while bs.style!=None:\n",
    "        bs.style.extract()\n",
    "    ## get content, method 1\n",
    "    divs=bs.find_all('div')\n",
    "    text=\"\"\n",
    "    minlen=5\n",
    "    for d in divs:\n",
    "        # if d.find_all(\"div\")==[]:\n",
    "        text=d.get_text().strip('\\n\\r ')\n",
    "        if text is not None and text not in content:\n",
    "            if text.count('。')>=1 and len(text)>minlen:\n",
    "                content=content+text\n",
    "    ## get content, method 2\n",
    "    ps=bs.find_all('p')\n",
    "    content2=\"\"\n",
    "    if ps!=[]:\n",
    "        for p in ps:\n",
    "            text=p.get_text()\n",
    "            if text is not None and text not in content2:\n",
    "                if text.count('。')>=1 and len(text)>minlen:\n",
    "                    content2=content2+text\n",
    "    ## compare\n",
    "    if len(content)<=len(content2):\n",
    "        content=content2\n",
    "    ## filtering\n",
    "    content=content.split(\"\\n\")\n",
    "    longtext=\"\"\n",
    "    for c in content:\n",
    "        if len(c)>=10 and (\"。\" in c or \"！\" in c or \"？\" in c):\n",
    "            longtext=longtext+\"\\n\"+c\n",
    "    return longtext.replace(\"百度首页登录个人中心帐号设置意见反馈退出\",'')\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "### News.py\n",
    "#!/usr/bin/python3\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "from funs import writeLog\n",
    "from funs import getPage\n",
    "from funs import getDate\n",
    "from funs import getLongText\n",
    "import time\n",
    "import sys\n",
    "## get page\n",
    "def getSubPage(a,longtxt=1):\n",
    "    ## string handler\n",
    "    treatStr=lambda s: s.replace('\\'','\\'\\'')\n",
    "    ## nega words\n",
    "    negaurlwords=('letv','youku','tudou','iqiyi','jfinfo','gmw.cn')\n",
    "    negatitlewords=(\"(图)\",\"（图）\",\"车展\",\"重磅\",\"健康问答\",\"养生\",\"(组图)\",\"（组图）\",\"专治\")\n",
    "    ## url\n",
    "    url=a.get(\"href\")\n",
    "    title=a.get_text().strip(\"\\r\\n \")\n",
    "    if title!=None and url!=None:\n",
    "        if url.find('http://')>=0 and len(title)>4:\n",
    "            ## exist?\n",
    "            sql=\"SELECT count(*) FROM news WHERE url=\\'\"+treatStr(url)+\"\\' LIMIT 1\"\n",
    "            cur.execute(sql)\n",
    "            exist=cur.fetchall()[0][0]\n",
    "            if exist==0:\n",
    "                skip=0\n",
    "                for w in negaurlwords:\n",
    "                    if url.find(w)>=0:\n",
    "                        skip=1\n",
    "                        break\n",
    "                if skip==0:\n",
    "                    for w in negatitlewords:\n",
    "                        if title.find(w)>=0:\n",
    "                            skip=1\n",
    "                            break\n",
    "                if skip==0:\n",
    "                    content=getLongText(url)\n",
    "                    if content!=None and len(content)>=80:\n",
    "                        return (getDate(),title,url,content)\n",
    "    return None\n",
    "## init database\n",
    "conn=sqlite3.connect('News.db')\n",
    "cur=conn.cursor()\n",
    "createTable=(\"CREATE TABLE IF NOT EXISTS news (\"\n",
    "             \"id INTEGER PRIMARY KEY AUTOINCREMENT, \"\n",
    "             \"date DATE, \"\n",
    "             \"title VARCHAR(500),\"\n",
    "             \"url VARCHAR(800),\"\n",
    "             \"content LONGTEXT);\")\n",
    "conn.execute(createTable)\n",
    "## homepage\n",
    "html=getPage(\"http://news.baidu.com\")\n",
    "bs=BeautifulSoup(html,\"html.parser\")\n",
    "Divs=bs.find_all('div')\n",
    "for d in Divs:\n",
    "    if d.get(\"id\")==\"pane-news\":\n",
    "        links=d.find_all(\"a\")\n",
    "        for a in links:\n",
    "            print(a)\n",
    "            content=getSubPage(a,longtxt=2)\n",
    "            if content!=None:\n",
    "                ## write into database\n",
    "                column=('date','title','url','content')\n",
    "                sql=\"INSERT INTO news (\"+','.join(column)+\") VALUES(\"+','.join('?'*len(column))+\");\"\n",
    "                cur.execute(sql,content)\n",
    "                conn.commit()\n",
    "## sub page\n",
    "Times=int(sys.argv[1])\n",
    "HomePages=((\"http://finance.baidu.com\",2),\n",
    "    (\"http://guonei.news.baidu.com\",3),\n",
    "    (\"http://guoji.news.baidu.com\",3),\n",
    "    (\"http://shehui.news.baidu.com\",5),\n",
    "    (\"http://mil.news.baidu.com\",6),\n",
    "    (\"http://tech.baidu.com\",7),\n",
    "    (\"http://internet.baidu.com\",7))\n",
    "for page in HomePages:\n",
    "    url=page[0]\n",
    "    times=page[1]\n",
    "    if Times%times==0:\n",
    "        html=getPage(url)\n",
    "        bs=BeautifulSoup(html,\"html.parser\")\n",
    "        links=bs.find_all(\"a\")\n",
    "        for a in links:\n",
    "            content=getSubPage(a)\n",
    "            if content!=None:\n",
    "                ## write into database\n",
    "                column=('date','title','url','content')\n",
    "                sql=\"INSERT INTO news (\"+','.join(column)+\") VALUES(\"+','.join('?'*len(column))+\");\"\n",
    "                cur.execute(sql,content)\n",
    "                conn.commit()\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用selenium爬取动态页面\n",
    "\n",
    "现在，很多网页通过使用JavaScript的Ajax技术实现了网页内容的动态改变。\n",
    "\n",
    "比如，一个很简单的例子，如果访问我们之前建立的web.py中的网址： http://127.0.0.1:5000/dynamic 上面会有一个按钮，每按一次，该页面就会向服务器再发一个请求，服务器收到请求后会随机产生一个数字，并返回。所以每一次点击按钮，网页内容都会动态改变。\n",
    "\n",
    "在碰到动态页面时，一种方法是精通JavaScript，并使用浏览器跟踪浏览器行为，分析JavaScript脚本，进而使用以上的方法模拟浏览器请求。但是这种方法非常复杂，很多时候JavaScript可能会复杂到一定程度，使得分析异常困难。\n",
    "\n",
    "而另外一种方法，即使用selenium直接调用浏览器，浏览器自动执行JavaScript，然后调用浏览器的HTML即可。这种方法非常方便，但是速度异常之慢。\n",
    "\n",
    "为了实现这一方法，我们首先要安装selenium：pip install selenium\n",
    "\n",
    "除此之外，还要安装浏览器的支持。几种常见的浏览器支持插件的下载地址：\n",
    "\n",
    "* firefox：https://github.com/mozilla/geckodriver/releases （对于Windows，下载后放到C:\\Windows下；对于Linux/Mac，放到/usr/local/bin下。）\n",
    "* chrome： https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "* safari： https://webkit.org/blog/6900/webdriver-support-in-safari-10/\n",
    "* edge：   https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/\n",
    "\n",
    "安装完成之后，使用：\n",
    "```python\n",
    "from selenium import webdriver\n",
    "driver=webdriver.Firefox()\n",
    "## 做一些事情\n",
    "driver.close()\n",
    "```\n",
    "\n",
    "就可以打开网络驱动器，此时我们可以看到一个Firefox窗口被打开。同样注意的是，用完之后记得关掉。\n",
    "\n",
    "比如，淘宝的网页上，价格数据是动态加载的，我们可以使用如下代码找到价格："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: use options instead of firefox_options\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "及木家具 北欧简约  榉木 白橡 黑胡桃 长方形抽屉实木茶几CJ030 5598.00 - 8336.00\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "url=\"https://detail.tmall.com/item.htm?id=524649065000&sku_properties=29112:97926\"\n",
    "### 如果不需要Firefox窗口打开：\n",
    "from selenium.webdriver import FirefoxOptions\n",
    "opts = FirefoxOptions()\n",
    "opts.add_argument(\"--headless\")\n",
    "driver = webdriver.Firefox(firefox_options=opts)\n",
    "###\n",
    "## 如果需要Firefox窗口打开，直接：driver=webdriver.Firefox()\n",
    "driver.get(url)\n",
    "html=driver.page_source\n",
    "driver.close()\n",
    "## 产品名\n",
    "bs=BeautifulSoup(html,\"html.parser\")\n",
    "meta=bs.find_all(\"meta\")\n",
    "for m in meta:\n",
    "    if m.get(\"name\")==\"keywords\":\n",
    "        title=m.get(\"content\")\n",
    "## 价格\n",
    "pattern=re.compile(\"TShop.Setup\\(.+?\\);\")\n",
    "res=pattern.finditer(html.replace('\\r', '').replace('\\n', ''))\n",
    "for r in res:\n",
    "    json_text=r.group()\n",
    "    json_text=json_text[json_text.find('{'):-2]\n",
    "\n",
    "info=json.loads(json_text)\n",
    "print(title,info['detail']['defaultItemPrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selenium另外一个更常用的功能是自动填充、点击。为了实现这一目的，首先需要能够找到相应的按钮、输入框等，有如下方法可以使用：\n",
    "\n",
    "* find_element_by_name\n",
    "* find_element_by_id\n",
    "* find_element_by_xpath\n",
    "* find_element_by_link_text\n",
    "* find_element_by_partial_link_text\n",
    "* find_element_by_tag_name\n",
    "* find_element_by_class_name\n",
    "* find_element_by_css_selector\n",
    "\n",
    "理解以上函数需要一些HTML、CSS、JavaScript的背景知识，我们在此不再详细讨论。在此，展示一个简单的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: use options instead of firefox_options\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python爬虫开发教程,一看就懂! /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd9TZMvi2W3uAoYXaSIL_XJxkmEjFJuzFhOMqFf5rWu9KiKiMYewYZFTXuKBoZPNv8HiUdDE87OdQR3taBxVRbMNap09NcnDqL_ZqTWtN8_3bEX4Mnou1_58DiUZ4Zs0oLdwKEQ-5IiIeFBPCsvYpxvReqe7NC02gBgBpyHwrqR9DX0OK9DLfgmPA..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n",
      "python爬虫入门:什么是爬虫,怎么玩爬虫? /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd9tmfVEOQRmQM8w59c5ta3Z-dnBbN0YBlnJTVziiaXiFQp9y1TIlT9jueJ_ZTqmNl2HWkGux-zNmj5_7-JnTIGGOZSOx6FqshaCRPyK0ARfwx1KYNcZCymjelWhbTlu1EV2hgNWXW4j1gRS3p9Wv9czbWXknrN8M8Ni3V3QwCfarA5eBgmN3LoYQ..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n",
      "python爬虫从入门,10分钟就够了 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd9ev3iuVDTLwRFCI70JCdjk75kx3X4KW4msAtUZDWQmHReeJghD-GRWQTIamCIoE2XuuHaYH0KPyKRV0dszyoD1l1aJoJ_5BO2fZ9SY9HZ33FzheLmjJGV-6twxw5I0-WX3_QdQ10y3OYLXXxGnLzzq3LmdnhqgV-MvY6-KvH0bify08OLTBhW0A..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n",
      "Python爬虫入门并不难,甚至入门也很简单 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd9KOzIpsWiw3TCY2wZQcmEagtJGvecktwbhbVNUi65k06dpfDTQBzl__BoMGflJ_A7e-aBWBE-LqQzGVcxzpt-jobnK1HwUrwSLbdHZlFHBMDw2EklJnJHIupmLYbMF-Lzrp5uQN3177lX4tCh7tZ4JScbmVgj2jMFFLESiyhUIZ_yPfCoem7FzA..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n",
      "Python爬虫 | 一条高效的学习路径 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd97IQjFXFScRkTFnPCBh7P7ykrc-la1jYfSnjfJekC-I7V9yzJD7PeVA35uJbMOmqS8UnkM-exwLmQHQcF7ETLu-ii7oFq3oqeN3oQGxGMUt6xVUNQ7HCbWqAwl6Y5Z-WNOwgOjFqfCoq1jfVtkNUy_-Ccf3KjZIxCaqRWd45qAXtOJBjQH7pCxQ..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n",
      "Python 爬虫“学前班”!学会免踩坑! /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd9eo4z5bQUlsyn6LOuG5zxNmWh9xd-xmGUkPaWZ2iKoKLyL9s6eiEqZMnYiDU30OmevuyLSb0d89ut0PWmF_LZnxtVYDD_W39vta8BSDZaU-tcnGcULHYNNQAQ1BbBL0jFn71nJFJOgo6Slt_FlC72IQTvQoW1D3pDEAjyf0lR_f15Nu7CnoSqqg..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n",
      "Python 爬虫介绍 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd9FUyVtdTcKnViH0q9cRyB5iSsy1sAcotWO_-iYEpQF_1FRrOEO7GApFyHbKFLtCmLNbWuTuWDD2CoKnrN62kWNggnD3c3PCHWCLCSUm3zYB-uhzKISaoPVNC_RvbFgxlVqLj8mAOS1uJPXiRGgt8p2Ct59T2LIwzjAdXmz54h0iECYioxHkzTmA..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n",
      "Python爬虫能做什么 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd9UI7A7_DGvXgYm57CqymxwA4j3lfUN0_D3axVvR2xKLWV4DIe_UBn4aPy1U21Dt0LOs4yVvY9sbQ0v8snW8HGUDsYiKktpj5oWodf52YI7r3dOK2taaf5Vt19YskLlwtS7ELWZ32AfYk2lnFYpU5ZUWDNVFL8AgvasYaNrdgt5A0SYTqT4eyLmA..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n",
      "在知乎上学 Python - 爬虫篇 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd9_StP4p0bgbTSWEM0yrT4Jby9SA3lfzjXW72ljS_FwuEXsCbwS_mEIEDQAstjwcHxInAsYnSo83ZJyGrKrcYLFJJuTBfxGIWcKM3CRMttoK7QNApvYhCKp2hFm9WtMiI2dARPTcq-gE4GMrg54vT1KfwiQ4H6hCd1pIezF4pp66MC2Qmj7RgGwQ..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n",
      "Python爬虫知识点梳理 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd9cLEbGMnQqtR7vK7xzkQzvmf3XltvCWTBJv0bnwnLu2KM-kVHn_QEJiOpx4eszC-mU0zvmC_fXPhNIG4UaNWVtq5AaJLcdQ2HNLmpr32wqRN0m23-cFh82wY1ywMn3xUN7lkDgjjjjfWn_LNTcdMCy0fWUZLubeWXT3ZQHn-_iPyE8RsmObDbtQ..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "url=\"http://weixin.sogou.com\"\n",
    "### 如果不需要Firefox窗口打开：\n",
    "from selenium.webdriver import FirefoxOptions\n",
    "opts = FirefoxOptions()\n",
    "opts.add_argument(\"--headless\")\n",
    "driver = webdriver.Firefox(firefox_options=opts)\n",
    "###\n",
    "## 如果需要Firefox窗口打开，直接：driver=webdriver.Firefox()\n",
    "driver.get(url)\n",
    "driver.find_element_by_id('query').send_keys(\"Python爬虫\") #模仿填写搜索内容\n",
    "driver.find_element_by_class_name(\"swz\").click() #模仿点击搜索按钮\n",
    "html=driver.page_source\n",
    "driver.close()\n",
    "bs=BeautifulSoup(html,\"html.parser\")\n",
    "iterms=bs.find_all(name='h3')\n",
    "for it in iterms:\n",
    "    print(it.a.text,it.a['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 后记：关于爬虫的一些补充\n",
    "\n",
    "关于爬虫，这里有一些需要补充的话。\n",
    "\n",
    "首先是关于爬虫的道德问题。实际上爬虫一直处在一个灰色地带：违法或者不违法，道德或者不道德。在此一些原则希望与大家分享：\n",
    "\n",
    "* 按照我国法律的相关规定，如果网页内容、API接口使用了加密，破解加密是违法行为。\n",
    "* 如果将爬取的数据商用，法律风险非常大\n",
    "* 如果网站提供了接口（比如豆瓣API），不使用接口而直接爬取网页也是不道德的。\n",
    "* 尽量爬取的速率不要太高，不要给他人的服务器造成太大负担。\n",
    "\n",
    "当然，还有一些技术出于时间所限、个人能力所限，我是没有讲到的，这其中很多都是与反爬虫有关。比如：\n",
    "\n",
    "* 如何破解验证码\n",
    "* 如何破解短信验证码\n",
    "* 消息队列\n",
    "* 并行爬虫\n",
    "\n",
    "等等。如果有需要，可以自行学习。\n",
    "\n",
    "最后，爬虫和反爬一直是相互伴生的两个技术，我们这里提供一些常用的反爬思路：\n",
    "\n",
    "* 如果request到的内容与自己使用浏览器看到的内容不符：\n",
    "    - 首先查看请求头部，最极端情况下，完全复制浏览器的请求头部，看看能不能得到相同的反应。\n",
    "    - 是否需要cookies登录？\n",
    "    - 是否有重定向？\n",
    "    - 仔细查看request得到的html，里面可能有玄机\n",
    "    - 实在不行，selenium\n",
    "* 如果request中的内容没有自己想要的信息：\n",
    "    - 是否是动态加载的？\n",
    "        * 对比request得到的HTML和浏览器的HTML是否不同，如果是不同的，可能时动态加载的\n",
    "        * 仔细分析JavaScript，或者浏览器发送的每一次请求\n",
    "        * 尝试直接发送动态请求\n",
    "    - 内容是否是加密的？\n",
    "        * 仔细分析加密方法\n",
    "* 爬取几个页面后被禁止访问：\n",
    "    - 可能爬取的频率太频繁\n",
    "    - 间隔时间小一点\n",
    "    - 使用代理池（比如该项目： https://github.com/SpiderClub/haipproxy ）\n",
    "    - 使用ADSL\n",
    "* 需要登录：\n",
    "    - 不频繁的登录通常使用cookies，模拟一次登录之后获得cookies即可\n",
    "    - 频繁的登录或者验证码，可能需要结合图像识别之类的方法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
