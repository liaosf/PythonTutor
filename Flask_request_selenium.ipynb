{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫简介\n",
    "\n",
    "爬虫一般指从网站上获得需要的数据，这个过程其实是建造一个网站的逆向过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Flask建立简单网站\n",
    "\n",
    "所谓知己知彼，则百战不殆。既然爬虫主要是从网站上爬取数据，那么只有对建造网站的步骤一清二楚，才能在变幻多端的网站环境下，获取想要的数据。\n",
    "\n",
    "然而实际建立网站的过程是非常繁琐而复杂的，写HTML、CSS仅仅是一个网站的冰山一角，这其中至少涉及到网站架构设计、数据库设计、后台服务设计、前端设计等等，此外根据不同的架构设计，还有很多中间件、缓存数据库等等复杂的设计。\n",
    "\n",
    "不过，对于比较简单的网站，Python本身就有很多成熟的框架可以方便我们快速搭建一个简单的网站。这其中，Flask由于其比较精美的设计架构以及简单的模板等应用，是非常受欢迎的轻量级Web框架。在这里，我们不妨使用Flask搭建一个最简单的网站（因为在Jupyter里面，所以我把run()给注释掉了，如果需要执行，请直接执行html/web.py）：\n",
    "```python\n",
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "import random\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    with open(\"example4.html\") as f:\n",
    "        html=f.read()\n",
    "    return html\n",
    "\n",
    "@app.route('/dynamic')\n",
    "def dynamic():\n",
    "    with open(\"dynamic.html\") as f:\n",
    "        html=f.read()\n",
    "    return html\n",
    "\n",
    "@app.route('/dynamic_response')\n",
    "def dynamic_response():\n",
    "    return str(random.random())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #app.run()\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在执行以上命令之前，需要首先使用pip install flask安装Flask。\n",
    "\n",
    "在上面的程序中，@代表修饰器（decorator），是Python编程的一个高级特性，我们暂且不管。我们需要知道的仅仅是，通过@app.route函数，声明了一个路径，该路径即访问接下来定义的页面的路径。\n",
    "\n",
    "在@app.route下方，我们定义了几个函数，这几个函数的作用是返回一个字符串，这些字符串会通过网络传递给访问的浏览器。\n",
    "\n",
    "如果运行html/web.py，会提示：\n",
    ">  Running on http://127.0.0.1:5000/\n",
    "\n",
    "此时，如果在浏览器中输入以上网址，服务器就会执行index()函数，该函数会读取example4.html，并将其返回，从而我们在浏览器上就看到了example4.html。\n",
    "\n",
    "同理，如果访问 http://127.0.0.1:5000/dynamic ，服务器就会执行dynamic()函数，将dynamic.html的内容返回。\n",
    "\n",
    "而如果访问 http://127.0.0.1:5000/dynamic_response ，服务器会执行dynamic_response()函数，该函数生成一个随机数并返回给浏览器。\n",
    "\n",
    "以上就是一个简单的网站。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 发送请求\n",
    "\n",
    "知道了服务器端如何处理页面，那么现在的问题是，客户端怎样与服务器端通讯呢？\n",
    "\n",
    "这里就要介绍以下HTTP协议的概念了。\n",
    "\n",
    "HTTP协议是**Hyper Text Transfer Protocol**（**超文本传输协议**）的缩写，一种服务器/客户端范式的传输协议，我们访问网址（URL）时，都是以http:// 开头的，或者https:// 开头，代表的就是使用http协议。\n",
    "\n",
    "一个最简单的传输模型是，客户端通过URL地址向服务器端发送**请求**（**request**），服务器根据所请求的地址、头部信息（headers）做出**响应**（**response**）。\n",
    "\n",
    "这里有几个概念要特别注意：\n",
    "\n",
    "## 网址URL\n",
    "\n",
    "一个常见的网址通常具有如下形式：\n",
    "\n",
    "> http://econpaper.cn:8080/article/article.jsp?id=56987&userid=3455\n",
    "\n",
    "其中：\n",
    "\n",
    "* http:// 代表协议\n",
    "* econpaper.cn 代表域名\n",
    "* :8080 代表端口号，默认为80端口\n",
    "* /article/article.jsp 部分为请求的页面路径\n",
    "* ?id=56987&userid=3455 为需要传递给这个页面的参数\n",
    "\n",
    "## 请求（request）\n",
    "\n",
    "客户端向服务器端发送请求，要按照一定的格式，请求消息由以下三部分组成：\n",
    "\n",
    "* 请求行（request line）\n",
    "* 头部（header）\n",
    "* 请求数据\n",
    "\n",
    "![](pic/request.png)\n",
    "\n",
    "比如，以下是一个典型的请求头部：\n",
    "\n",
    "![](pic/request_headers.png)\n",
    "\n",
    "这些信息传递给服务器后，服务器根据这些信息进行处理。\n",
    "\n",
    "实际上，在我们刚刚运行的Flask中，在客户端可以看到每一次的请求。\n",
    "\n",
    "此外，上面从URL地址中已经看到，?id=56987&userid=3455 为需要传递给这个页面的参数，实际上，这是一个GET的请求方法。为了从客户端向服务器端传数据，以下两种方法是最常用的：\n",
    "\n",
    "* GET：像上面一样，请求的数据明文写在URL上\n",
    "* POST：数据包含在请求体中\n",
    "\n",
    "当然，两种方法也可以结合起来使用。不过最为常用的仍然是GET方法。\n",
    "\n",
    "## 响应（response）\n",
    "\n",
    "有了请求，就会有服务器的响应。同样，响应也有响应头和数据体。在响应头中，最重要的是响应的状态码以及内容类型（html文档或者图片、pdf等），常见的状态码比如：\n",
    "\n",
    "* 200 OK                        客户端请求成功\n",
    "* 400 Bad Request               客户端请求有语法错误，不能被服务器所理解\n",
    "* 401 Unauthorized              请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用 \n",
    "* 403 Forbidden                 服务器收到请求，但是拒绝提供服务\n",
    "* 404 Not Found                 请求资源不存在，eg：输入了错误的URL\n",
    "* 500 Internal Server Error     服务器发生不可预期的错误\n",
    "* 503 Server Unavailable        服务器当前不能处理客户端的请求，一段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python中发送请求\n",
    "\n",
    "在Python中，有很多工具可以帮助我们向服务器发送请求，包括但不限于：urllib、urllib2、urllib3、httplib2、requests等等等等。出于个人偏好原因，在这里我们以requests为例，介绍如何发送请求。\n",
    "\n",
    "为了使用requests，需要先进行安装：pip install requests\n",
    "\n",
    "使用时直接导入：\n",
    "```python\n",
    "import requests\n",
    "```\n",
    "\n",
    "接着，最简单的请求即GET请求：\n",
    "```python\n",
    "r=requests.get(url)\n",
    "```\n",
    "\n",
    "其中url为需要请求的地址。\n",
    "\n",
    "有时我们在发送请求时可能需要控制发送请求的头部，头部可以使用一个字典表示，比如：\n",
    "```python\n",
    "headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36 OPR/60.0.3255.70\",\n",
    "        \"Accept-Encoding\":\"gzip, deflate, lzma, sdch\",\n",
    "        \"Accept-Language\":\"en-US,en;q=0.8\",\n",
    "        \"Connection\":\"keep-alive\",\n",
    "        \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"}\n",
    "```\n",
    "\n",
    "以上字典定义了头部的内容，接下来只要在get()函数中加入该头部就可以了：\n",
    "```python\n",
    "r=requests.get(url, headers=hearders)\n",
    "```\n",
    "\n",
    "返回结果：\n",
    "\n",
    "* r.text 可以读取返回的HTML\n",
    "* r.json() 获得Json数据转换后的字典数据\n",
    "* r.encoding 记录了返回数据的字符编码\n",
    "* r.status_code 记录了响应的状态码\n",
    "\n",
    "如果需要发送POST请求，需要首先将请求的数据写成字典形式，在使用urllib包中的urlencode函数将其编码，比如：\n",
    "```python\n",
    "data=dict(name=\"Joe\", comment=\"A test comment\")\n",
    "r=requests.post(url, data=data)\n",
    "```\n",
    "\n",
    "如果需要设置自定义的cookie到服务器，可以使用：\n",
    "```python\n",
    "cookies = dict(cookies_are='working')\n",
    "r=requests.get(url, cookies=cookies)\n",
    "```\n",
    "\n",
    "如果我们希望设置最长的等待时间，可以使用超时选项：\n",
    "```python\n",
    "r=requests.get(url, timeout=10)\n",
    "```\n",
    "\n",
    "即设定等待时间超过10s则放弃连接。\n",
    "\n",
    "最后，如果需要使用代理，可以使用：\n",
    "```python\n",
    "proxies = {\n",
    "  'http': 'http://user@password@10.10.1.10:3128',\n",
    "  'https': 'http://10.10.1.10:1080',\n",
    "}\n",
    "r=requests.get(url, proxies=proxies)\n",
    "```\n",
    "\n",
    "比如，使用一个获取微博热搜的API："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 200,\n",
       " 'msg': 'success',\n",
       " 'data': [{'hot_word': '潘玮柏', 'hot_word_num': '4519470'},\n",
       "  {'hot_word': '事业单位科研人员奖金可不受限', 'hot_word_num': '1630377'},\n",
       "  {'hot_word': '真我GT Neo', 'hot_word_num': '1614748'},\n",
       "  {'hot_word': '科比女儿被南加大录取', 'hot_word_num': '1608364'},\n",
       "  {'hot_word': '长歌行', 'hot_word_num': '1500914'},\n",
       "  {'hot_word': '耳朵进水后引发的社死现场', 'hot_word_num': '1220568'},\n",
       "  {'hot_word': '剧本杀线下门店已突破3万家', 'hot_word_num': '977978'},\n",
       "  {'hot_word': '假如让你带薪休假一年', 'hot_word_num': '882207'},\n",
       "  {'hot_word': '汪卓成晒私生敲门视频', 'hot_word_num': '862291'},\n",
       "  {'hot_word': '迪丽热巴长歌行单人cut', 'hot_word_num': '799955'}],\n",
       " 'author': {'name': 'Alone88',\n",
       "  'desc': '由Alone88提供的免费API 服务，官方文档：www.alapi.cn'}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://v1.alapi.cn/api/new/wbtop?num=10\"\n",
    "import requests\n",
    "r=requests.get(url)\n",
    "data=r.json()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大多数接口都需要用一个token进行身份验证，一般会根据token进行一定的数量限制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码实现了从链家上爬去房价数据。值得注意的是，如果没有设定headers，可能会爬不下来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['107103559446', '2室2厅', '91.26平米', '九亭', '425万', '九亭/2室2厅/91.26平米/南 北/简装']\n",
      "['107103676779', '2室1厅', '73.47平米', '九亭', '378万', '九亭/2室1厅/73.47平米/南/精装']\n",
      "['107103719930', '2室2厅', '91.68平米', '九亭', '450万', '九亭/2室2厅/91.68平米/南/精装']\n",
      "['107103745356', '2室1厅', '69.16平米', '泗泾', '275万', '泗泾/2室1厅/69.16平米/南/简装']\n",
      "['107103707697', '3室2厅', '148.65平米', '松江新城', '496万', '松江新城/3室2厅/148.65平米/南/简装']\n",
      "['107103668872', '3室2厅', '134.25平米', '九亭', '579万', '九亭/3室2厅/134.25平米/南/精装']\n",
      "['107103740096', '3室2厅', '136.43平米', '莘闵别墅', '640万', '莘闵别墅/3室2厅/136.43平米/南/精装']\n",
      "['107103724802', '2室1厅', '77.99平米', '泗泾', '325万', '泗泾/2室1厅/77.99平米/南 北/毛坯']\n",
      "['107103743119', '3室2厅', '93.3平米', '泗泾', '485万', '泗泾/3室2厅/93.3平米/南/其他']\n",
      "['107103745823', '3室2厅', '127.85平米', '松江大学城', '575万', '松江大学城/3室2厅/127.85平米/南/精装']\n",
      "['107103771809', '2室2厅', '90.2平米', '九亭', '460万', '九亭/2室2厅/90.2平米/南/精装']\n",
      "['107103716972', '6室3厅', '311.4平米', '九亭', '1850万', '九亭/6室3厅/311.4平米/南/精装']\n",
      "['107103674739', '2室1厅', '68.37平米', '泗泾', '275万', '泗泾/2室1厅/68.37平米/南/简装']\n",
      "['107103680863', '3室2厅', '133平米', '松江新城', '440万', '松江新城/3室2厅/133平米/南 北/简装']\n",
      "['107103702676', '2室2厅', '110.46平米', '九亭', '485万', '九亭/2室2厅/110.46平米/南/精装']\n",
      "['107103727415', '3室2厅', '108.56平米', '莘闵别墅', '455万', '莘闵别墅/3室2厅/108.56平米/南/简装']\n",
      "['107102398373', '6室3厅', '978.62平米', '佘山', '3280万', '佘山/6室3厅/978.62平米/东 南 西 北/精装']\n",
      "['107103729918', '2室2厅', '89.82平米', '松江大学城', '469万', '松江大学城/2室2厅/89.82平米/南 北/毛坯']\n",
      "['107103729108', '2室2厅', '77.81平米', '九亭', '440万', '九亭/2室2厅/77.81平米/南/精装']\n",
      "['107103770260', '2室2厅', '81.17平米', '松江新城', '308万', '松江新城/2室2厅/81.17平米/南/简装']\n",
      "['107103483159', '4室2厅', '213.57平米', '松江新城', '1270万', '松江新城/4室2厅/213.57平米/南/精装']\n",
      "['107103740261', '3室2厅', '124.05平米', '松江新城', '470万', '松江新城/3室2厅/124.05平米/南 北/简装']\n",
      "['107103750179', '2室2厅', '108.36平米', '泗泾', '438万', '泗泾/2室2厅/108.36平米/南 北/精装']\n",
      "['107103723650', '1室1厅', '55.19平米', '泗泾', '204万', '泗泾/1室1厅/55.19平米/东/简装']\n",
      "['107103675642', '4室2厅', '172.84平米', '莘闵别墅', '889万', '莘闵别墅/4室2厅/172.84平米/南/精装']\n",
      "['107103698980', '3室2厅', '124.87平米', '莘闵别墅', '480万', '莘闵别墅/3室2厅/124.87平米/南 北/精装']\n",
      "['107103646322', '3室2厅', '121.55平米', '九亭', '638万', '九亭/3室2厅/121.55平米/南/精装']\n",
      "['107103728110', '3室2厅', '134.97平米', '莘闵别墅', '635万', '莘闵别墅/3室2厅/134.97平米/南/精装']\n",
      "['107103741598', '2室1厅', '74.74平米', '松江老城', '248万', '松江老城/2室1厅/74.74平米/南/简装']\n",
      "['107103750660', '2室1厅', '39.83平米', '松江老城', '175万', '松江老城/2室1厅/39.83平米/南/精装']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36 OPR/60.0.3255.70\",\n",
    "        \"Accept-Encoding\":\"gzip, deflate, lzma, sdch\",\n",
    "        \"Accept-Language\":\"en-US,en;q=0.8\",\n",
    "        \"Connection\":\"keep-alive\",\n",
    "        \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"}\n",
    "url='https://sh.lianjia.com/ershoufang/songjiang/'\n",
    "r=requests.get(url, headers=headers)\n",
    "html=r.text\n",
    "bs=BeautifulSoup(html,\"html.parser\")\n",
    "iterms=bs.find_all(name='div',attrs={\"class\":\"item\"})\n",
    "info=[]\n",
    "for it in iterms:\n",
    "    house_id=it.get(\"data-houseid\")\n",
    "    prop=it.find(name='div',attrs={\"class\":\"info\"}).text\n",
    "    shi_ting=re.search(r\"\\d室\\d厅\",prop).group()\n",
    "    area=re.search(r\"\\d*(\\.\\d*)?平米\",prop).group()\n",
    "    location=prop[:prop.find('/')]\n",
    "    price=it.find(name='div',attrs={\"class\":\"price\"}).text\n",
    "    print([house_id,shi_ting,area,location,price,prop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用selenium爬取动态页面\n",
    "\n",
    "现在，很多网页通过使用JavaScript的Ajax技术实现了网页内容的动态改变。\n",
    "\n",
    "比如，一个很简单的例子，如果访问我们之前建立的web.py中的网址： http://127.0.0.1:5000/dynamic 上面会有一个按钮，每按一次，该页面就会向服务器再发一个请求，服务器收到请求后会随机产生一个数字，并返回。所以每一次点击按钮，网页内容都会动态改变。\n",
    "\n",
    "在碰到动态页面时，一种方法是精通JavaScript，并使用浏览器跟踪浏览器行为，分析JavaScript脚本，进而使用以上的方法模拟浏览器请求。但是这种方法非常复杂，很多时候JavaScript可能会复杂到一定程度，使得分析异常困难。\n",
    "\n",
    "而另外一种方法，即使用selenium直接调用浏览器，浏览器自动执行JavaScript，然后调用浏览器的HTML即可。这种方法非常方便，但是速度异常之慢。\n",
    "\n",
    "为了实现这一方法，我们首先要安装selenium：pip install selenium\n",
    "\n",
    "除此之外，还要安装浏览器的支持。几种常见的浏览器支持插件的下载地址：\n",
    "\n",
    "* firefox：https://github.com/mozilla/geckodriver/releases （对于Windows，下载后放到C:\\Windows下；对于Linux/Mac，放到/usr/local/bin下。）\n",
    "* chrome： https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "* safari： https://webkit.org/blog/6900/webdriver-support-in-safari-10/\n",
    "* edge：   https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/\n",
    "\n",
    "安装完成之后，使用：\n",
    "```python\n",
    "from selenium import webdriver\n",
    "driver=webdriver.Firefox()\n",
    "## 做一些事情\n",
    "driver.close()\n",
    "```\n",
    "\n",
    "就可以打开网络驱动器，此时我们可以看到一个Firefox窗口被打开。同样注意的是，用完之后记得关掉。\n",
    "\n",
    "比如，淘宝的网页上，价格数据是动态加载的，我们可以使用如下代码找到价格："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: use options instead of firefox_options\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "及木家具 北欧简约  榉木 樱桃 黑胡桃 长方形抽屉实木茶几CJ030 600.00 - 8998.00\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "url=\"https://detail.tmall.com/item.htm?id=524649065000&sku_properties=29112:97926\"\n",
    "### 如果不需要Firefox窗口打开：\n",
    "from selenium.webdriver import FirefoxOptions\n",
    "opts = FirefoxOptions()\n",
    "opts.add_argument(\"--headless\")\n",
    "driver = webdriver.Firefox(firefox_options=opts)\n",
    "###\n",
    "## 如果需要Firefox窗口打开，直接：driver=webdriver.Firefox()\n",
    "driver.get(url)\n",
    "html=driver.page_source\n",
    "driver.close()\n",
    "## 产品名\n",
    "bs=BeautifulSoup(html,\"html.parser\")\n",
    "meta=bs.find_all(\"meta\")\n",
    "for m in meta:\n",
    "    if m.get(\"name\")==\"keywords\":\n",
    "        title=m.get(\"content\")\n",
    "## 价格\n",
    "pattern=re.compile(\"TShop.Setup\\(.+?\\);\")\n",
    "res=pattern.finditer(html.replace('\\r', '').replace('\\n', ''))\n",
    "for r in res:\n",
    "    json_text=r.group()\n",
    "    json_text=json_text[json_text.find('{'):-2]\n",
    "\n",
    "info=json.loads(json_text)\n",
    "print(title,info['detail']['defaultItemPrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selenium另外一个更常用的功能是自动填充、点击。为了实现这一目的，首先需要能够找到相应的按钮、输入框等，有如下方法可以使用：\n",
    "\n",
    "* find_element_by_name\n",
    "* find_element_by_id\n",
    "* find_element_by_xpath\n",
    "* find_element_by_link_text\n",
    "* find_element_by_partial_link_text\n",
    "* find_element_by_tag_name\n",
    "* find_element_by_class_name\n",
    "* find_element_by_css_selector\n",
    "\n",
    "理解以上函数需要一些HTML、CSS、JavaScript的背景知识，我们在此不再详细讨论。在此，展示一个简单的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: use options instead of firefox_options\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python爬虫入门:什么是爬虫,怎么玩爬虫? /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS5f43PTqFCzdMT-_4R348K9SawowXnXHX1qXa8Fplpd99ZLCyW9hQby6SJvwjQaeBRbMhK0w61ah7IT62HQoIP-TRy4Ya44WUqz8H1n6k_tQS-xd-u2VZGMX2MkoBQzbZUEbMjd12-NXUJtaLUi4b69yVl0S75LHhm_drt3mruKcjqFojEGMdrD8R3EzcTispL934TKiMMMkB8S-wuBG4G72CsG6-xMgzQ..&type=2&query=Python%E7%88%AC%E8%99%AB&token=962B7AF2413E02F9C4C67A0AB9C8053FC4DBEBB260640A3A\n",
      "Python爬虫实战 由易到难(图文解析) /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS5f43PTqFCzdMT-_4R348K9SawowXnXHX1qXa8Fplpd9GGp-d6gZemqelzgpwbS_aq2C05R7VhZIZNb4bx_nkutHqVbP1WBM9OxlVtLh83dvEr3e16qTsefNBh-bFAKCH3xAkpTaiyRTItGR6hvxvYZfM9floUeECb66BjlfD46EcTLInqhRj3JkkaFpSChVAH4mV_0gkoH55BgpyKU0CTECn9lOoZLP2Q..&type=2&query=Python%E7%88%AC%E8%99%AB&token=962B7AF2413E02F9C4C67A0AB9C8053FC4DBEBB260640A3A\n",
      "Python爬虫:一些常用的爬虫技巧总结 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6ft3wfAVofsP5Peu-UiA4DANmzq3vhuKnSiFOtQOZrEiVgrFeYQugDnh7SlEsauWT5BsWpA7A0Z0HWvXvccwX52bCVaUzkWQUztkhfMGE4GKMlZSeCuAGIY7GYrbNxu4JP9tunsiQ8v0YkZeCddCFEUwYjR3LB_wwn6AwpWhbdGVv7yJAlFqy4zD8DO7mxwE-K-8uJTs-s07G1MkqlJei61p4LBqDLl7NM-bwqJbVhVmfA6ZFH2u8E_iu56dWxw0T8exjRsF3Sze0onzokKIbzipIOz63DrwSiZqwg0yoRFw.&type=2&query=Python%E7%88%AC%E8%99%AB&token=962B7AF2413E02F9C4C67A0AB9C8053FC4DBEBB260640A3A\n",
      "简单python爬虫编写 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS5f43PTqFCzdMT-_4R348K9SawowXnXHX1qXa8Fplpd9I_2ChFZDaqFsV9uK9R3p1XzjnScRTfqTLu1pli03pbERL1sfcREESF3JVm3ps_LTjLXdT4nICsTENd9SdGpxdL2yCjXmoKy2NzKtZhbour5BImftRBM2Lo40bEzVZYvPmJ0aLRRS8MarCMFMxpqJzPsIj-ofPGGr5RLukPZkH5Rj3x9Nw6p-Fg..&type=2&query=Python%E7%88%AC%E8%99%AB&token=962B7AF2413E02F9C4C67A0AB9C8053FC4DBEBB260640A3A\n",
      "Python爬虫入门并不难,甚至入门也很简单 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS5f43PTqFCzdMT-_4R348K9SawowXnXHX1qXa8Fplpd9RWhg0PXxzRiIrBRgO90NSBSEtPG9_FU37JJEequQrx7jOFRfkbShoiWKZDlWVemSvDK8mTBW0TI-iAZjEg5z0tT2Ca-pZyOGymLQ4Qap4MhzJPsX5IixiR1G7YJpB-VKrj3lItMjjid94enPtC47EbPgBsMUclAbd6C4a3F1MI7wdVqCwyPdzA..&type=2&query=Python%E7%88%AC%E8%99%AB&token=962B7AF2413E02F9C4C67A0AB9C8053FC4DBEBB260640A3A\n",
      "Python爬虫实例实战!! /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS5f43PTqFCzdMT-_4R348K9SawowXnXHX1qXa8Fplpd93KutgXCGv7VlZSyT6v0SG5nfxb1IgHUUKfFC0UvDiTJ9B2mbEBHfZ964XfbJwoa9cmZkGqb3--0m6ONvEbr9Kl56PnhjjMFQ1Hzfp4H8R5O_bWhalFRpCFS9p9eGm8dVNJdbkJWGNShFJfHdTo34TEpspAKtTjA-p7DCZVEsj6SFW6cwuUnkdA..&type=2&query=Python%E7%88%AC%E8%99%AB&token=962B7AF2413E02F9C4C67A0AB9C8053FC4DBEBB260640A3A\n",
      "Python爬虫能做什么 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS5f43PTqFCzdMT-_4R348K9SawowXnXHX1qXa8Fplpd9MFMtruTJNtk_zTOsIgTy0ejmgtVufOxQGfxD1k4XVj0lTwBKaa6xtrggMdUDnp1CjCrdOfoSkdGqwBVFcfnCxD5ZY6FEM_wl66-HCBXXB9ke-K8eFYmzb9YUdYvC9JFwe32_jWE5jvmpIk1yNYsN6SCCoPY1iE9qJBHMqcLk8Qdr1dbZWbKUeQ..&type=2&query=Python%E7%88%AC%E8%99%AB&token=962B7AF2413E02F9C4C67A0AB9C8053FC4DBEBB260640A3A\n",
      "Python爬虫案例教与学(教学大纲+教案+视频)) /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS5f43PTqFCzdMT-_4R348K9SawowXnXHX1qXa8Fplpd9b0jDHnApTC2xz2_wo4iiKFD9lwD6EpbV_eVOhYkPKHcRi-E4B-RCXCfV3Es_gXFr7JplCuFFVIFokeNUBzAA2tWUJ_kIsrL7IQNXr1Crs2ke0wH-0gw-LQk8brmkE3bGo4_uVTTXGQEK6oHsUvTUUlRpRTDm3KtCNO7NjWa_Gb5f_71bqkOtiw..&type=2&query=Python%E7%88%AC%E8%99%AB&token=962B7AF2413E02F9C4C67A0AB9C8053FC4DBEBB260640A3A\n",
      "Python爬虫系列——入门到精通 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS5f43PTqFCzdMT-_4R348K9SawowXnXHX1qXa8Fplpd9Nt37hvCXkpxzzkb2t9mUyOu_tOs3VvZ-xvCKILY8gls_ke2AUsXGYy_CWtCQPkJkK5t183s3SXJrkj8SSdPFp5V6RJFCQ9vKPmmQO5Ux3bPyVpBi33t1SXimh7j88lnNtMIP9ETP-RQemy2K43giZW01zwW6vKLa-z8zhFaGuUACn9lOoZLP2Q..&type=2&query=Python%E7%88%AC%E8%99%AB&token=962B7AF2413E02F9C4C67A0AB9C8053FC4DBEBB260640A3A\n",
      "如何利用Python爬虫,高效获取大规模数据 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS5f43PTqFCzdMT-_4R348K9SawowXnXHX1qXa8Fplpd9H6-J_JMhv-b6hEoM2eujGNpQvrAhqrlflZ-ANSZ5eW9GqPVa3UIPXdCdlvCS2ImeRrlDE2TVo6nbgZdpF4z5AMGAJ0CevpdLjp8Ws6dPEvAd-a0xP20epccl_IXM3izR_fsLXgJNGfgufb8cgsH8b8sjh3q-nDVunKLc7Y8zfVBflztgI7RZ1g..&type=2&query=Python%E7%88%AC%E8%99%AB&token=962B7AF2413E02F9C4C67A0AB9C8053FC4DBEBB260640A3A\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "url=\"http://weixin.sogou.com\"\n",
    "### 如果不需要Firefox窗口打开：\n",
    "from selenium.webdriver import FirefoxOptions\n",
    "opts = FirefoxOptions()\n",
    "opts.add_argument(\"--headless\")\n",
    "driver = webdriver.Firefox(firefox_options=opts)\n",
    "###\n",
    "## 如果需要Firefox窗口打开，直接：driver=webdriver.Firefox()\n",
    "driver.get(url)\n",
    "driver.find_element_by_id('query').send_keys(\"Python爬虫\") #模仿填写搜索内容\n",
    "driver.find_element_by_class_name(\"swz\").click() #模仿点击搜索按钮\n",
    "html=driver.page_source\n",
    "driver.close()\n",
    "bs=BeautifulSoup(html,\"html.parser\")\n",
    "iterms=bs.find_all(name='h3')\n",
    "for it in iterms:\n",
    "    print(it.a.text,it.a['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 后记：关于爬虫的一些补充\n",
    "\n",
    "关于爬虫，这里有一些需要补充的话。\n",
    "\n",
    "首先是关于爬虫的道德问题。实际上爬虫一直处在一个灰色地带：违法或者不违法，道德或者不道德。在此一些原则希望与大家分享：\n",
    "\n",
    "* 按照我国法律的相关规定，如果网页内容、API接口使用了加密，破解加密是违法行为。\n",
    "* 如果将爬取的数据商用，法律风险非常大\n",
    "* 如果网站提供了接口（比如豆瓣API），不使用接口而直接爬取网页也是不道德的。\n",
    "* 尽量爬取的速率不要太高，不要给他人的服务器造成太大负担。\n",
    "\n",
    "当然，还有一些技术出于时间所限、个人能力所限，我是没有讲到的，这其中很多都是与反爬虫有关。比如：\n",
    "\n",
    "* 如何破解验证码\n",
    "* 如何破解短信验证码\n",
    "* 消息队列\n",
    "* 并行爬虫\n",
    "\n",
    "等等。如果有需要，可以自行学习。\n",
    "\n",
    "最后，爬虫和反爬一直是相互伴生的两个技术，我们这里提供一些常用的反爬思路：\n",
    "\n",
    "* 如果request到的内容与自己使用浏览器看到的内容不符：\n",
    "    - 首先查看请求头部，最极端情况下，完全复制浏览器的请求头部，看看能不能得到相同的反应。\n",
    "    - 是否需要cookies登录？\n",
    "    - 是否有重定向？\n",
    "    - 仔细查看request得到的html，里面可能有玄机\n",
    "    - 实在不行，selenium\n",
    "* 如果request中的内容没有自己想要的信息：\n",
    "    - 是否是动态加载的？\n",
    "        * 对比request得到的HTML和浏览器的HTML是否不同，如果是不同的，可能时动态加载的\n",
    "        * 仔细分析JavaScript，或者浏览器发送的每一次请求\n",
    "        * 尝试直接发送动态请求\n",
    "    - 内容是否是加密的？\n",
    "        * 仔细分析加密方法\n",
    "* 爬取几个页面后被禁止访问：\n",
    "    - 可能爬取的频率太频繁\n",
    "    - 间隔时间小一点\n",
    "    - 使用代理池（比如该项目： https://github.com/SpiderClub/haipproxy ）\n",
    "    - 使用ADSL\n",
    "* 需要登录：\n",
    "    - 不频繁的登录通常使用cookies，模拟一次登录之后获得cookies即可\n",
    "    - 频繁的登录或者验证码，可能需要结合图像识别之类的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一个综合例子：爬取MBA智库的名词作为词典\n",
    "\n",
    "我们接下来在进行文本分析时，一个重要的步骤是分词，也就是把句子分成一个个的词。然而经济、金融以及各个行业有很多专有名词，所以我们的想法是希望能够找到一个“词典”，里面包含了经济金融的常用词汇，这样我们在进行文本分析时就可以更可靠的区分出句子中的每个词。\n",
    "\n",
    "我们想到的办法是从“MBA智库”中把所有的名词都爬取下来。\n",
    "\n",
    "为此，我们首先定义一些后面遇到的函数，包括写错误日志的函数以及去除文本稳健重复行的函数：\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "经济词典中所使用的通用函数。包括：\n",
    "\n",
    "errlog：错误日志\n",
    "getdic：从github上下载已经整理好的数据文件\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def errlog(message):\n",
    "    \"\"\"\n",
    "    写入错误日志到logfile.txt\n",
    "    需要的输入：错误信息message\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import time\n",
    "    with open(\n",
    "            os.path.dirname(os.path.realpath(__file__)) + \"/logfile.txt\",\n",
    "            'a') as f:\n",
    "        f.write(\n",
    "            time.strftime(\"%Y-%m-%d %H:%M:%S  \", time.localtime()) + message +\n",
    "            '\\n')\n",
    "\n",
    "\n",
    "def getdic(filename):\n",
    "    \"\"\"\n",
    "    从GitHub上下载已经整理好的词典文件。\n",
    "    需要的输入：文件名filename\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import requests\n",
    "    import sys\n",
    "    url = \"https://raw.githubusercontent.com/sijichun/jingjidic/master/sub_dics/\" + filename\n",
    "    PWD = os.path.dirname(os.path.realpath(__file__))\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        with open(PWD + '/sub_dics/' + filename, 'w') as f:\n",
    "            f.write(html)\n",
    "    except Exception as e:\n",
    "        print(\"错误：\" + str(e))\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def remove_duplicates(filename, csv=False):\n",
    "    \"\"\"\n",
    "    去除文件filename中重复的行。\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        if csv:\n",
    "            import csv\n",
    "            rows = []\n",
    "            csv_file = csv.DictReader(f)\n",
    "            for r in csv_file:\n",
    "                rows.append(r)\n",
    "            rows = list(set(rows))\n",
    "        else:\n",
    "            content = f.readlines()\n",
    "            content = list(set(content))\n",
    "            content.sort()\n",
    "    with open(filename, 'w') as f:\n",
    "        if csv:\n",
    "            csv_file = csv.DictWriter(f, list(rows[0].keys()))\n",
    "            csv_file.writeheader()\n",
    "            csv_file.writerows(rows)\n",
    "        else:\n",
    "            for w in content:\n",
    "                f.write(w.strip() + '\\n')\n",
    "```\n",
    "\n",
    "接下来是爬取的过程：\n",
    "```python\n",
    "#!/usr/bin/python3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "from functions import errlog\n",
    "from functions import remove_duplicates\n",
    "import csv\n",
    "from queue import Queue\n",
    "\n",
    "glossary = []\n",
    "tasks = Queue()\n",
    "Iterm_sets = set()\n",
    "\n",
    "## get the current path\n",
    "PWD = os.path.dirname(os.path.realpath(__file__))\n",
    "SUB_PATH = PWD + '/sub_dics'\n",
    "Dic_File = SUB_PATH + \"/glossary.txt\"\n",
    "Csv_File = SUB_PATH + \"/glossary.csv\"\n",
    "if os.path.exists(SUB_PATH) is not True:\n",
    "    os.mkdir(SUB_PATH)\n",
    "\n",
    "\n",
    "def get_glossary_sub_page(retry=3):\n",
    "    \"\"\"\n",
    "    从Queue中挨个取出元素并爬取\n",
    "    \"\"\"\n",
    "    URL = \"https://wiki.mbalib.com%s\"\n",
    "    while not tasks.empty():\n",
    "        father_iterm = tasks.get()\n",
    "        for i in range(retry):\n",
    "            try:\n",
    "                if \"https://\" not in father_iterm['link']:\n",
    "                    url = URL % father_iterm['link']\n",
    "                html = requests.get(url, timeout=10).text\n",
    "                # print(html)\n",
    "                bs = BeautifulSoup(html, \"html.parser\")\n",
    "                ## 获取所有子类\n",
    "                links = bs.find_all(name=\"a\")\n",
    "                for l in links:\n",
    "                    href = l.get('href')\n",
    "                    if href != None and href.find(\n",
    "                            \"Category:\") > 0 and l.text[-2:] not in ('标志',\n",
    "                                                                     '图像'):\n",
    "                        if l.text not in Iterm_sets:\n",
    "                            iterm = {\n",
    "                                'iterm': l.text,\n",
    "                                'link': href,\n",
    "                                'father': father_iterm['iterm'],\n",
    "                                'tag': 'Category'\n",
    "                            }\n",
    "                            glossary.append(iterm)\n",
    "                            tasks.put(iterm)\n",
    "                            Iterm_sets.add(l.text)\n",
    "                            print(iterm)\n",
    "                ## 获取所有词语列表\n",
    "                div = bs.find_all(name=\"div\", attrs={\"class\": \"page_ul\"})[0]\n",
    "                links = div.find_all(name='a')\n",
    "                for l in links:\n",
    "                    href = l.get('href')\n",
    "                    if href != None and href.find(\"/wiki/\") == 0:\n",
    "                        if l.text not in Iterm_sets:\n",
    "                            iterm = {\n",
    "                                'iterm': l.text,\n",
    "                                'link': href,\n",
    "                                'father': father_iterm['iterm'],\n",
    "                                'tag': 'Iterm'\n",
    "                            }\n",
    "                            glossary.append(iterm)\n",
    "                            Iterm_sets.add(l.text)\n",
    "                            with open(Dic_File, 'a') as f:\n",
    "                                f.write(l.text + '\\n')\n",
    "                            print(iterm)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"glossary错误：\" + str(e) + \"：\" + url +\n",
    "                      \"：函数get_glossary_sub_page\")\n",
    "                errlog(\"glossary错误：\" + str(e) + \"：\" + url +\n",
    "                       \"：函数get_glossary_sub_page\")\n",
    "                os.system(\"pppoe-stop; pppoe-start\")\n",
    "                time.sleep(random.random() * 10 + i * 10)\n",
    "        time.sleep(3 + 10 * random.random())\n",
    "    return\n",
    "\n",
    "\n",
    "def get_glossary(retry=3):\n",
    "    \"\"\"\n",
    "    从MBA智库百科中爬取专业词汇。\n",
    "    \"\"\"\n",
    "    URL = \"https://wiki.mbalib.com%s\"\n",
    "    url = URL % \"/wiki/MBA智库百科:分类索引\"\n",
    "    for i in range(retry):\n",
    "        try:\n",
    "            html = requests.get(url, timeout=10).text\n",
    "            bs = BeautifulSoup(html, \"html.parser\")\n",
    "            links = bs.find_all(name=\"a\")\n",
    "            for l in links:\n",
    "                href = l.get('href')\n",
    "                if href != None and href.find(\n",
    "                        \"Category:\") > 0 and l.text[-2:] not in ('标志', '图像'):\n",
    "                    if l.text not in Iterm_sets:\n",
    "                        iterm = {\n",
    "                            'iterm': l.text,\n",
    "                            'link': href,\n",
    "                            'father': 'ROOT',\n",
    "                            'tag': 'Category'\n",
    "                        }\n",
    "                        glossary.append(iterm)\n",
    "                        tasks.put(iterm)\n",
    "                        Iterm_sets.add(l.text)\n",
    "                        with open(Dic_File, 'a') as f:\n",
    "                            f.write(l.text + '\\n')\n",
    "                        print(iterm)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"glossary错误：\" + str(e) + \"：\" + url + \"：函数get_glossary\")\n",
    "            errlog(\"glossary错误：\" + str(e) + \"：\" + url + \"：函数get_glossary\")\n",
    "            os.system(\"pppoe-stop; pppoe-start\")\n",
    "            time.sleep(random.random() * 10 + i * 10)\n",
    "    ## 开始爬取子页面\n",
    "    get_glossary_sub_page(retry=retry)\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ## 获取命令行参数\n",
    "    Need_Craw = False\n",
    "    args = sys.argv[1:]\n",
    "    for arg in args:\n",
    "        if arg == \"-update\":\n",
    "            Need_Craw = True\n",
    "        else:\n",
    "            print(\"无效的参数：\" + arg)\n",
    "            sys.exit(1)\n",
    "\n",
    "    if Need_Craw:\n",
    "        ## 从网络爬取\n",
    "        get_glossary()\n",
    "        ## 写入文件\n",
    "        remove_duplicates(Dic_File)\n",
    "        ## 写入csv文件\n",
    "        csv_exist = os.path.exists(Csv_File)\n",
    "        with open(Csv_File, 'a') as f:\n",
    "            header = ['iterm', 'link', 'father', 'tag']\n",
    "            f_csv = csv.DictWriter(f, header)\n",
    "            if not csv_exist:\n",
    "                f_csv.writeheader()\n",
    "            f_csv.writerows(glossary)\n",
    "    else:\n",
    "        from functions import getdic\n",
    "        getdic(\"glossary.txt\")\n",
    "        print(\"Complete. See ./sub_dics/glossary.txt\")\n",
    "```\n",
    "\n",
    "注意我们使用了**队列**（**Queue**）来组织我们的爬取任务，每次得到需要爬取的页面就放在队列里面，然后不断的从队列里面取出任务再爬取子页面。\n",
    "\n",
    "除此之外，我们这里租用了VPS服务器进行爬取，该服务器可以使用：\n",
    "```shell\n",
    "pppoe-stop\n",
    "pppoe-start\n",
    "```\n",
    "的方式重新进行PPPoE拨号，每次拨号IP地址都会变化，从而避免了IP被封。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
