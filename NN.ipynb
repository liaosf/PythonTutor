{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络初步\n",
    "\n",
    "在监督学习中，我们的目标是使用特征$x$的函数，$f(x)$，对某个输出$y$进行拟合。我们之前所学的线性回归、非线性回归、Logistic回归等方法都可以看作是设定了一个$f(x)$的函数形式：$f(x,\\theta)$，通过最小化损失的方式找到一个$\\theta^*$使得$f(x,\\theta^*)$的预测最准确。而分类树、支持向量机等方法则是寻求非参数的方法确定$f(x)$。\n",
    "\n",
    "而**神经网络**（**neural networks**）则是使用了第一种方法的思想，不过为了减轻函数形式的假设，通过模拟生物的神经系统的方法，组合大量的“**神经元**（**neuron**）”，找到了一族非常特殊而又具有极强表达模型能力的函数形式。从这点而言，简单的神经网络模型其实与我们所学的回归模型等方法并没有本质上的不同，只不过神经网络使用了神经元的网络结构构造了更加灵活的函数形式而已。\n",
    "\n",
    "最常用、最简单的神经网络是所谓**深度前馈网络**（**deep feed-forward networks**）或者**多层感知机**（**multilayer perceptrons, MLP**）。所谓“前馈”，即从输入（input），通过一定的计算得到隐含变量，最后再进行计算得到输出（output）的过程，而没有反馈（feedback）。\n",
    "\n",
    "其中，隐含变量是通过输入计算得到的，我们通常称其为**隐含层**（**hidden layer**），比如一个简单的单层网络：\n",
    "![](pic/nn_1_hidden.gv.png \"单层前馈网络\")\n",
    "其中第一层为输入层（input layer），中间一层使用输入层得到了隐含层（hidden layer），最后一层使用隐藏层计算了输出层（output layer）。此外注意输出层可以不止有一个输出，可以由更多的输出，就像我们在计量经济学中的系统估计、多元Probit、多元Logistic等一样，将紧密相关的几个不同输出放在一个模型中进行训练会给模型带来更多的信息。\n",
    "\n",
    "当然，也可以继续加入隐含层，比如以下是一个二层网络：\n",
    "![](pic/nn_2_hidden.gv.png \"双层前馈网络\")\n",
    "在以上单层网络的基础上，我们又加入了一个隐含层，就变成了双层前馈网络。以上过程还可以继续，隐含层的层数我们将其称之为这个网络的**深度**（**depth**），所以“深度学习”的简单理解就是深度比较高的神经网络。\n",
    "\n",
    "其中，隐含层的每一个节点都是一个神经元。所谓神经元，就是使用输入通过一定的函数计算得到一个输出。观察以上的两个网络，每一个隐含层都会使用上一层的所有输出作为这一层的输入，而且不存在跨层，我们称这类网络为**全连接网络**（**fully-connected**）。每一层的神经元的个数成为“**宽度**”（**width**）。\n",
    "\n",
    "如果我们记$f_{li}\\left(h_{l-1}\\right)$记为该函数，其中$h_{l-1}$为第$l-1$层的输出，而$f_{li}$则为第$l$层的第$i$个神经元，$h_{0}$即为输入$x$，同时记$$f_l\\left(h_{l-1}\\right)=\\left[f_{l1},f_{l2},...,f_{l,K_l}\\right]'$$其中$K_l$为第$l$层的神经元个数，那么神经网络可以使用一个嵌套函数：$$y=f_L\\left(f_{L-1}\\left(f_{L-2}\\left(\\cdots f_1\\left(x\\right)\\right)\\right)\\right)$$值得注意的是，从以上公式中我们可以看出，所有的函数并不能是线性函数：如果所有的$f$都是线性函数，那么最终整个模型也不过就是一个线性函数而已。所以一般而言，每个神经元的计算函数$f_{li}$不能取线性函数。一般情况下，$f_{li}$会选取如下形式：$$f_{li}=g\\left(W_{li}'h_{l-1}+b\\right)$$其中$b$为“常数项”，而$W_{li}$为将上一层的输出进行线性组合的“**权重**（**weights**）”，而非线性性来源于非线性函数$g\\left(\\cdot\\right)$，即激活函数。我们将在下面详细激活函数的选择。\n",
    "\n",
    "所有的神经元构成了一个有向无环图，给定一个**损失函数**（**loss function**），比如$$\\sum_{i=1}^N ||y_i-\\hat{y_i}||_2$$其中$\\hat{y_i}$为神经网络的预测，那么最小化以上损失函数就可以得到$f_{li}$的估计，从而最终得到预测函数。而正如前面所学的，为了最小化以上的损失函数，通常使用梯度下降法，而计算导数的过程是逆向求导的：求导的方向是从输出层到输入层，所以求解该最优化问题的方法也叫做“**反向传播**”（**back-propagation**，简称**BP**）算法。\n",
    "\n",
    "而这个过程完全可以使用PyTorch进行：我们只需要将计算图表达出来，利用PyTorch的自动求导功能，并使用PyTorch中的各种优化方法，就可以找到$\\hat{y_i}$的最优参数。\n",
    "\n",
    "根据**万能近似定理**（**universal approximation theorem**），具有足够多隐藏神经元的网络（具有足够深度、宽度）可以逼近任何的Borel可测特别是连续函数。当然，逼近的效果取决于网络的结构，包括宽度、深度以及神经元的设计。接下来我们将主要介绍神经网络的结构设计。在此之前，我们不妨先通过一个简单的例子理解神经网络的工作原理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一个简单的例子：学习异或函数\n",
    "\n",
    "**异或函数**（**XOR**）是一个常用的逻辑函数，其定义为：$$0\\ \\ XOR\\ \\ 0=0 \\\\ 1\\ \\ XOR\\ \\ 0=1 \\\\ 0\\ \\ XOR\\ 1\\ =1 \\\\ 1\\ \\ XOR\\ \\ 1=0$$即如果两个输入全是0或者全是1，则输出为0；只有当两个输入中一个为1一个为0时，输出为1。\n",
    "\n",
    "比如，我们可以先生成一组随机的数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [1, 1],\n",
       "       ...,\n",
       "       [1, 0],\n",
       "       [1, 1],\n",
       "       [0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N=1000\n",
    "X=np.random.random((N,2))>0.5\n",
    "X=X.astype('int32')\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意为了放到PyTorch中我们将数值类型转化为Float32，然后生成标签："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 1., 1., 1., 0., 1., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=((X.sum(axis=1))==1).astype('float32')\n",
    "X=X.astype('float32')\n",
    "Y[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用了函数$y=1\\left\\{x_1+x_2==1\\right\\}$生成$y$，然而注意到以上函数并不是一个线性函数。\n",
    "\n",
    "我们不妨使用一个简单的只有一层隐含层，两个隐含神经元的网络：\n",
    "![](pic/nn_xor_hidden.gv.png \"学习异或\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以想象，如果我们令：$$h_1=b_{10}+b_{11}x_1+b_{12}x_2 \\\\ h_1=b_{20}+b_{21}x_1+b_{22}x_2 \\\\ y=b_0+b_1 h_1+b_2 h_2$$那么$$y=b_0+b_1\\left(b_{10}+b_{11}x_1+b_{12}x_2\\right)+b_2\\left(b_{20}+b_{21}x_1+b_{22}x_2\\right)$$仍然是一个线性函数，不可能学习到异或函数。\n",
    "\n",
    "为此，我们转而将以上函数取为非线性函数，比如，一个简单的方法是使用**激活函数**（**activation function**），即把来自于上一层的输出的线性组合，通过一个一元非线性函数变换得到该神经元的输出。比如，一个常用的激活函数是**线性整流函数**（**Rectified Linear Unit**, **ReLU**）：$$g\\left(x\\right)=\\begin{cases}\n",
    "0 & x<0\\\\\n",
    "x & x\\geq0\n",
    "\\end{cases}$$\n",
    "该函数是一个**分段**（**piecewise**）函数，函数图像为：\n",
    "![](pic/ReLU.png \"ReLU函数\")\n",
    "\n",
    "使用该函数，我们可以将其写为：$$h_1=g\\left(b_{10}+b_{11}x_1+b_{12}x_2\\right) \\\\ h_1=g\\left(b_{20}+b_{21}x_1+b_{22}x_2\\right) \\\\ y=b_0+b_1 h_1+b_2 h_2$$\n",
    "观察数据生成过程$y=1\\left\\{x_1+x_2==1\\right\\}$可以看到，$y$的生成依赖于两个$x$相加，我们不妨猜测$b_{11}=b_{12}=b_{21}=b_{22}=1$，从而得到：$$h_1=g\\left(b_{10}+x_1+x_2\\right) \\\\ h_1=g\\left(b_{20}+x_1+x_2\\right) \\\\ y=b_0+b_1 h_1+b_2 h_2$$对于输入：$$\\left[\\begin{array}{cc}\n",
    "0 & 0\\\\\n",
    "0 & 1\\\\\n",
    "1 & 0\\\\\n",
    "1 & 1\n",
    "\\end{array}\\right]$$正确的输出结果为：$$\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "1\\\\\n",
    "1\\\\\n",
    "0\n",
    "\\end{array}\\right]$$第一步经过相加后，输入就变成了：$$\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "1\\\\\n",
    "1\\\\\n",
    "2\n",
    "\\end{array}\\right]$$我们可以先结合ReLU函数的特点，把两个特殊的值：$0,2$取出来，比如，可以令$b_{10}=-0.5,b_{20}=-1$，经过计算后得到：结果分别为：$$\\left[\\begin{array}{c}\n",
    "-0.5\\\\\n",
    "0.5\\\\\n",
    "0.5\\\\\n",
    "1.5\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "-1\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "1\n",
    "\\end{array}\\right]$$经过ReLU函数的激活就变成了：$$\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "0.5\\\\\n",
    "0.5\\\\\n",
    "1.5\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "1\n",
    "\\end{array}\\right]$$注意到：$$2\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "0.5\\\\\n",
    "0.5\\\\\n",
    "1.5\n",
    "\\end{array}\\right]-3\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "1\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "1\\\\\n",
    "1\\\\\n",
    "0\n",
    "\\end{array}\\right]$$从而，令$b_0=0,b_2=2,b_3=-3$就可以得使得以上网络能够学习到异或函数了。当然，从统计学角度，以上的所有参数都是不可识别的：有不止一组解能够达到与以上参数相同的结果。所以实际数据计算的结果可能会与以上结果有偏差，但是预测结果应该是一样的。\n",
    "\n",
    "结合以上例子，对于神经网络的一个最简单的理解就是：每一个神经元就像一个小“开关”一样，当该神经元的输入（线性组合）达到了一定的阈值，那么就处于开启的状态（激活了），否则就处于关闭的状态。我们可以通过很多很多这样的小开关来判断当期的状态，进而判断此时的$y$的值。\n",
    "\n",
    "我们可以使用PyTorch对以上数据进行训练，首先准备数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class xor_data(Dataset):\n",
    "    def __len__(self):\n",
    "        return X.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        x=X[i,:]\n",
    "        y=Y[i]\n",
    "        data=torch.from_numpy(x)\n",
    "        label=torch.tensor(y)\n",
    "        return data, label\n",
    "data=xor_data()\n",
    "xor_dl=DataLoader(data, batch_size=data.__len__(), pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8089, 0.8440, 0.6170], requires_grad=True)\n",
      "tensor([-0.1390,  0.3127,  0.4002], requires_grad=True)\n",
      "tensor([0.1877, 0.3061, 0.5462], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "# 设置初始值\n",
    "b1=torch.rand(3, requires_grad=True)\n",
    "b2=torch.rand(3, requires_grad=True)\n",
    "b=torch.rand(3, requires_grad=True)\n",
    "optimizer=optim.Adagrad([b1,b2,b], lr=0.01)\n",
    "Zero=torch.tensor(0.0)\n",
    "for i in range(100):\n",
    "    for x,y in xor_dl:\n",
    "        xb1=torch.mv(x,b1[:-1])+b1[-1]\n",
    "        xb1=torch.max(Zero,xb1)\n",
    "        xb2=torch.mv(x,b2[:-1])+b2[-1]\n",
    "        xb2=torch.max(Zero,xb2)\n",
    "        xb=b[0]*xb1+b[1]*xb2+b[2]\n",
    "        loss=torch.sum((y-xb)**2)\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 迭代\n",
    "        optimizer.step()\n",
    "print(b1)\n",
    "print(b2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上我们使用了前面所学的PyTorch的计算图对以上模型进行了计算。然而在大型的神经网络中，如果使用以上方法一个个定义参数，将会非常麻烦。PyTorch中，可以使用神经网络工具箱torch.nn使用简单的编程语言来对以上模型进行建模，就像搭积木一样把整个模型搭建起来。首先导入神经网络工具箱："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "# 如果模型放在GPU上，如果计算中出现错误，报错信息不好查看。这个时候可以把模型放在CPU上，报错信息就更加容易查看了\n",
    "# device=torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn工具箱的具体使用可以查看https://pytorch.org/docs/stable/nn.html ，我们在这里介绍一些主要用法。\n",
    "\n",
    "为了构建神经网络模型，需要创建一个神经网络模型的类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XOR_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XOR_net, self).__init__()\n",
    "        # 第一层：线性组合，2个输入2个输出\n",
    "        self.layer1=nn.Sequential(nn.Linear(2,2),nn.ReLU(inplace=True))\n",
    "        # 第二层：线性组合，2个输入1个输出\n",
    "        self.layer2=nn.Sequential(nn.Linear(2,1))\n",
    "    def forward(self,x):\n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "        return x\n",
    "\n",
    "model=XOR_net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们解释一下上面的类。首先，该类继承自nn.Module，nn.Module是一个抽象类，我们需要将该类的函数进行重载从而定义模型。\n",
    "\n",
    "接下来，使用：\n",
    "```python\n",
    "super(XOR_net, self).__init__()\n",
    "```\n",
    "进行了初始化。\n",
    "\n",
    "nn.Sequential()可以看做是创建了一个计算的“容器”，即按照顺序对输入进行处理，并得到输出。其中，nn.Linear(m,n)即构建一个线性组合，有m个输入以及n个输出。而nn.ReLU即对上一层的计算结果进行ReLU的变换。\n",
    "\n",
    "使用如上的方法，我们构造了两个层，第一层有2个输入2个输出，第二层有2个输入1个输出。\n",
    "\n",
    "如果需要给每个层命名，在构造Sequantial时也可以使用有序词典：\n",
    "```python\n",
    "self.layer1=nn.Sequential(OrderedDict([\n",
    "            ('Layer1_linear_combine', nn.Linear(2,2)),\n",
    "            ('Layer1_ReLU', nn.ReLU(inplace=True))\n",
    "        ]))\n",
    "```\n",
    "\n",
    "\n",
    "接下来，定义forward()方法，将输入x通过不同层的计算得到输出的前向传播过程。\n",
    "\n",
    "如此我们就定义了一个简单的网络。\n",
    "\n",
    "接下来，可以继续定义损失函数，并进行迭代："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2969, -0.0265],\n",
      "        [-0.6808,  0.8729]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0833, -0.2387], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1733,  1.0567]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3298], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAEvCAYAAAB2a9QGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAdnklEQVR4nO3dfXBd9X3n8c9XV7p6frD1hC35KdgeEMHYrOKGh4RAHGJaakOXtkDYQocZOtsw05ZkdugyA7t0urOF6SbtwO7AbtkkZLMEaCAmayAESGlSSG1jY7AdY2EwfsLyA7Ily5Is6bt/3GNzUWTrSvccnSud92tG43t+5xzpK/3G8se/3+/+jrm7AAAAEK6iuAsAAACYjghZAAAAESBkAQAARICQBQAAEAFCFgAAQAQIWQAAABEojruAkRoaGnz+/PlxlwEAADCmDRs2HHL3xtHOFVzImj9/vtavXx93GQAAAGMys11nOsd0IQAAQAQIWQAAABEgZAEAAESAkAUAABABQhYAAEAECFkAAAARIGQBAABEgJAFAAAQAUIWAABABBIZsp7esEcbdh2JuwwAADCNJTJk3f/cFq3ZtC/uMgAAwDSWyJBVXVai7r7BuMsAAADTWEJDVrGOEbIAAECEEhmyaspK1N13Mu4yAADANJbIkFVdVsx0IQAAiFQiQ1ZNeYm6+xnJAgAA0UlkyKouK9axE4xkAQCA6CQ2ZPX0D8rd4y4FAABMUwkNWSUaGnb1DgzFXQoAAJimEhqyiiWJxe8AACAyCQ1ZJZLENg4AACAyCQ1ZmZEsNiQFAABRSWTIqjk9XchIFgAAiEZCQ9ap6UJGsgAAQDQSGbKqCVkAACBiCQ1ZTBcCAIBoJTJkVaRTShUZI1kAACAyiQxZZqaq0mIdYyQLAABEJJEhS5LqKkp09AQhCwAARCO5Iau8RF29hCwAABCNnEKWma00s+1m1mFmd49y/i4z22pmm83sZTObl3Vurpn91My2BdfMD7H+CautSKuLkSwAABCRMUOWmaUkPSzpGkltkm4ys7YRl22U1O7uSyQ9LemBrHPfk/Sgu58vabmkzjAKz1ddeYmO9g7EXQYAAJimchnJWi6pw913uvuApCckrc6+wN1fdffe4PANSa2SFISxYnd/KbiuJ+u6WNVVlDCSBQAAIpNLyGqRtDvreE/Qdia3S3o+eL1YUpeZ/cjMNprZg8HIWOzqyjML34eHPe5SAADANBTqwnczu0VSu6QHg6ZiSV+Q9E1Jn5P0GUm3jXLfHWa23szWHzx4MMySzqi2Ii13dn0HAADRyCVk7ZU0J+u4NWj7FDNbIekeSavcvT9o3iNpUzDVOCjpWUkXj7zX3R9193Z3b29sbBzntzAxdeWZR+t0nWBdFgAACF8uIWudpEVmtsDM0pJulLQm+wIzWybpEWUCVueIe+vM7FRyukrS1vzLzl9dRSZksVcWAACIwpghKxiBulPSi5K2SXrS3beY2f1mtiq47EFJVZKeMrNNZrYmuHdImanCl83sbUkm6X9G8H2M26mQxV5ZAAAgCsW5XOTuayWtHdF2b9brFWe59yVJSyZaYFRqy9OSxDsMAQBAJJK74/up6UL2ygIAABFIbMiqLWe6EAAARCexIaskVaTKdIrpQgAAEInEhixJqqtIM5IFAAAikeiQVVteoqPskwUAACKQ6JBVV1HCSBYAAIgEIYs1WQAAIAKJDlm15azJAgAA0Uh0yKqryKzJcve4SwEAANNMskNWeYlODrl6B4biLgUAAEwzyQ5Zp55fyLosAAAQskSHrNPPL+TROgAAIGSJDlmfPL+QkSwAABAuQpaYLgQAAOFLdsg6PV1IyAIAAOFKdsg6PZLFmiwAABCuRIesspKUSouLWJMFAABCl+iQJWVGsz7m3YUAACBkiQ9ZMyrS+piRLAAAEDJCVkWafbIAAEDoEh+yZlamdeQ4IQsAAIQr8SFrRmUJ04UAACB0hKxgunBo2OMuBQAATCOErIq0hl06xq7vAAAgRIkPWTMrM7u+s40DAAAIU+JD1gxCFgAAiAAhK3i0zpHjTBcCAIDwELIqgpEstnEAAAAhSnzIYk0WAACIQuJDVkU6pXRxkY4QsgAAQIgSH7LMTDMqSpguBAAAoUp8yJIy67JY+A4AAMJEyFJmXRYPiQYAAGEiZCmzVxZrsgAAQJgIWRJrsgAAQOgIWZJmVqTVdeIkD4kGAAChIWQpM13oPCQaAACEiJClTzYkZV0WAAAICyFLUh2P1gEAACEjZCmzJkuSjhCyAABASHIKWWa20sy2m1mHmd09yvm7zGyrmW02s5fNbN6I8zVmtsfMHgqr8DDNqCyRxPMLAQBAeMYMWWaWkvSwpGsktUm6yczaRly2UVK7uy+R9LSkB0ac/ytJr+VfbjQ+eUg0C98BAEA4chnJWi6pw913uvuApCckrc6+wN1fdffe4PANSa2nzpnZv5HULOmn4ZQcvvKSzEOiWZMFAADCkkvIapG0O+t4T9B2JrdLel6SzKxI0t9K+uZEC5wMZqaZFWkdJmQBAICQFIf5yczsFkntkq4Imv5U0lp332NmZ7vvDkl3SNLcuXPDLClnMyvTjGQBAIDQ5BKy9kqak3XcGrR9ipmtkHSPpCvcvT9ovkTSF8zsTyVVSUqbWY+7f2rxvLs/KulRSWpvb49l2/X6qrQOEbIAAEBIcglZ6yQtMrMFyoSrGyXdnH2BmS2T9Iikle7eeard3b+Wdc1tyiyO/413JxaCxqpS7Tx4PO4yAADANDHmmix3H5R0p6QXJW2T9KS7bzGz+81sVXDZg8qMVD1lZpvMbE1kFUekviqtw8f75c7zCwEAQP5yWpPl7mslrR3Rdm/W6xU5fI7vSPrO+MqbPPVVpeo7OazegSFVloa6VA0AACQQO74H6oO9sg73sC4LAADkj5AVaKgulSQdOt4/xpUAAABjI2QFGiqDkNVNyAIAAPkjZAXqq4LpQrZxAAAAISBkBWaeXpPFSBYAAMgfIStQVpJSdVmxDrHwHQAAhICQlaWhqlSHGMkCAAAhIGRlqa9Ms4UDAAAIBSEry6ld3wEAAPJFyMrSUFXKSBYAAAgFIStLfVWpjvQOaGiY5xcCAID8ELKyNFSl5S4dYa8sAACQJ0JWlvpg13fWZQEAgHwRsrI0VPGQaAAAEA5CVpb6quD5heyVBQAA8kTIysJIFgAACAshK0tNWYmKi4yRLAAAkDdCVpaiIstsSMpIFgAAyBMha4T6ylIdZCQLAADkiZA1QnNNqTq7++IuAwAATHGErBGaqsvUeYyRLAAAkB9C1gjNNaU61NPPo3UAAEBeCFkjNNaUadilw6zLAgAAeSBkjdBcndmQ9ABThgAAIA+ErBGaasokicXvAAAgL4SsEZprGMkCAAD5I2SN0FBVKjNGsgAAQH4IWSOUpIpUX5lmJAsAAOSFkDWKxuoyHWQkCwAA5IGQNYrmmlJGsgAAQF4IWaNori5jTRYAAMgLIWsUTTWlOtjNru8AAGDiCFmjaDq16/txpgwBAMDEELJG0RTs+s6DogEAwEQRskbRzK7vAAAgT4SsUTTx/EIAAJAnQtYoGpkuBAAAeSJkjeL0ru9MFwIAgAkiZJ1BU02ZOo8RsgAAwMQQss5gdm2Z9nURsgAAwMTkFLLMbKWZbTezDjO7e5Tzd5nZVjPbbGYvm9m8oH2pmb1uZluCc38Y9jcQldl15dp39ETcZQAAgClqzJBlZilJD0u6RlKbpJvMrG3EZRsltbv7EklPS3ogaO+V9EfufoGklZK+bWZ1IdUeqdl15erqPanj/YNxlwIAAKagXEaylkvqcPed7j4g6QlJq7MvcPdX3b03OHxDUmvQ/q677whe75PUKakxrOKjNLsus1fWfkazAADABOQSslok7c463hO0ncntkp4f2WhmyyWlJb03ngLj0lJXLkmsywIAABNSHOYnM7NbJLVLumJE+yxJj0u61d2HR7nvDkl3SNLcuXPDLGnCZp8OWYxkAQCA8ctlJGuvpDlZx61B26eY2QpJ90ha5e79We01kv6fpHvc/Y3RvoC7P+ru7e7e3thYGLOJTdWlShUZIQsAAExILiFrnaRFZrbAzNKSbpS0JvsCM1sm6RFlAlZnVnta0jOSvufuT4dXdvSKU0U6p6ZMe5kuBAAAEzBmyHL3QUl3SnpR0jZJT7r7FjO738xWBZc9KKlK0lNmtsnMToWwP5D0RUm3Be2bzGxp6N9FRGbXlTGSBQAAJiSnNVnuvlbS2hFt92a9XnGG+74v6fv5FBinWbXlemtPV9xlAACAKYgd389idl259nf1aXjY4y4FAABMMYSss2ipK9PA0LAOHe8f+2IAAIAshKyzmM1eWQAAYIIIWWfBXlkAAGCiCFlnQcgCAAATRcg6i5qyYlWVFmsvIQsAAIwTIesszEyz68q092NCFgAAGB9C1hhaZ1RoDyELAACMEyFrDPPqK7Tr8HG5s1cWAADIHSFrDPPrK3V8YEiHegbiLgUAAEwhhKwxzK2vkCR9eOR4zJUAAICphJA1hvn1lZKkDw71xlwJAACYSghZY2ipK1eqyLTrMCNZAAAgd4SsMaSLi9RSV64PDjOSBQAAckfIysGpdxgCAADkipCVg3n1Fdp1hJEsAACQO0JWDubXV6qr96S6etnGAQAA5IaQlYN5wTsMd7EuCwAA5IiQlYP5wV5ZH7AuCwAA5IiQlYM5MzMhi5EsAACQK0JWDspKUppVW0bIAgAAOSNk5WhefYXeP9QTdxkAAGCKIGTlaGFTlXZ09sjd4y4FAABMAYSsHC1urlZ336AOHOuPuxQAADAFELJytKipWpL07oHumCsBAABTASErR4ubqyQRsgAAQG4IWTmqrypVfWVaOw6w+B0AAIyNkDUOi5qr9G4nI1kAAGBshKxxWNxcrY4DvMMQAACMjZA1Douaq9XdP6j9R/viLgUAABQ4QtY4LG5i8TsAAMgNIWscFjdntnFg8TsAABgLIWscZlSm1VBVykgWAAAYEyFrnBY3VxGyAADAmAhZ49Q2q0bbPurWyaHhuEsBAAAFjJA1The21mpgcJh1WQAA4KwIWeP02ZZaSdI7e4/GXAkAAChkhKxxWlBfqarSYr1NyAIAAGdByBqnoiLTBbNrCFkAAOCsCFkTcGFLrbbtP6ZBFr8DAIAzyClkmdlKM9tuZh1mdvco5+8ys61mttnMXjazeVnnbjWzHcHHrWEWH5cLW2vVPzisHZ0sfgcAAKMbM2SZWUrSw5KukdQm6SYzaxtx2UZJ7e6+RNLTkh4I7p0p6T5JvyVpuaT7zGxGeOXH49Tid6YMAQDAmeQykrVcUoe773T3AUlPSFqdfYG7v+ruvcHhG5Jag9dflfSSux9x948lvSRpZTilx+fU4nfeYQgAAM4kl5DVIml31vGeoO1Mbpf0/ATvnRJOLX7fvIeQBQAARhfqwnczu0VSu6QHx3nfHWa23szWHzx4MMySInPRnDpt3XdM/YNDcZcCAAAKUC4ha6+kOVnHrUHbp5jZCkn3SFrl7v3judfdH3X3dndvb2xszLX2WF08t04DQ8Pasu9Y3KUAAIAClEvIWidpkZktMLO0pBslrcm+wMyWSXpEmYDVmXXqRUlXm9mMYMH71UHblHfx3Mz6/Td3fRxzJQAAoBAVj3WBuw+a2Z3KhKOUpMfcfYuZ3S9pvbuvUWZ6sErSU2YmSR+6+yp3P2Jmf6VMUJOk+939SCTfySRrqilTS125Nn7YFXcpAACgAI0ZsiTJ3ddKWjui7d6s1yvOcu9jkh6baIGFrH3+DP2y45CGhl2pIou7HAAAUEDY8T0PXz6/WYd6BrSBKUMAADACISsPV53XpHRxkR77xfvsmQUAAD6FkJWHqtJiXbG4US9s+UirHvqFXnjno7hLAgAABYKQlaf/cv2Fevz25bpoTp2++dRb6u47GXdJAACgABCy8tRYXaovLGrUfb97gXr6B/XMxt/YBgwAACQQISskS+fU6aLWWj3++q64SwEAAAWAkBWi65e1aEdnj3YdPh53KQAAIGaErBB9cXHmkUCv7TgUcyUAACBuhKwQLWioVEtduV57d2o85BoAAESHkBUiM9MXFzfq9fcOa2jY4y4HAADEiJAVsvZ5M9TTP6iOzp64SwEAADEiZIVs2dw6SdKm3TxqBwCAJCNkhWxBQ6Vqy0u08cOuuEsBAAAxImSFzMy0dE4dIQsAgIQjZEVg2dw6vdvZrZ7+wbhLAQAAMSFkRWDZ3Blylzbv7oq7FAAAEBNCVgSWttZJkjYSsgAASCxCVgRqK0r0mcZK1mUBAJBghKyIZBa/f6zBoeG4SwEAADEgZEVk5QXn6PDxAT23eV/cpQAAgBgQsiKy4vxmnXdOtR56pUPDPGIHAIDEIWRFpKjI9O+/dK7eO3hcr+3ggdEAACQNIStC13x2lhqqSvX467viLgUAAEwyQlaE0sVFunn5HL2yvVO7j/TGXQ4AAJhEhKyI3fxb81Rkpu+/wWgWAABJQsiK2Dm1Zbq6rVlPrNutvpNDcZcDAAAmCSFrEty0fK6OnjipV37dGXcpAABgkhCyJsFlCxvUVF2qH725N+5SAADAJCFkTYJUkem6ZS36+fZOHe7pj7scAAAwCQhZk+T6ZS0aHHb9ZPP+uEsBAACTgJA1Sc6fVaPzZ9XoRxuZMgQAIAkIWZPo95a16K3dXXrvYE/cpQAAgIgRsibR6qWzVWTSMyyABwBg2iNkTaKmmjJdvqhRz2zcy0OjAQCY5ghZk+zfXtyivV0ntO6DI3GXAgAAIkTImmRfaWtWRTqlZzfti7sUAAAQIULWJKtIF+vqtmatfXu/+gd5zA4AANMVISsG1y1r0dETJ/Xz7QfjLgUAAESEkBWDyxc2qKEqrR9v4l2GAABMV4SsGBSninTtktn62bZOHes7GXc5AAAgAjmFLDNbaWbbzazDzO4e5fwXzexNMxs0sxtGnHvAzLaY2TYz+3szs7CKn8quW9aigcFhvfD2R3GXAgAAIjBmyDKzlKSHJV0jqU3STWbWNuKyDyXdJukHI+69VNJlkpZI+qykz0m6Iu+qp4GLWms1v75CzzJlCADAtJTLSNZySR3uvtPdByQ9IWl19gXu/oG7b5Y0POJel1QmKS2pVFKJpAN5Vz0NmJlWL23R6zsP66OjfXGXAwAAQpZLyGqRtDvreE/QNiZ3f13Sq5L2Bx8vuvu28RY5XV23rEXu0pq3GM0CAGC6iXThu5ktlHS+pFZlgtlVZvaFUa67w8zWm9n6gweTs63BgoZKXTSnTs9uZGNSAACmm1xC1l5Jc7KOW4O2XFwv6Q1373H3HknPS7pk5EXu/qi7t7t7e2NjY46fenq4bulsbd1/TO8e6I67FAAAEKJcQtY6SYvMbIGZpSXdKGlNjp//Q0lXmFmxmZUos+id6cIs1y6ZrVSR6dmNTBkCADCdjBmy3H1Q0p2SXlQmID3p7lvM7H4zWyVJZvY5M9sj6fclPWJmW4Lbn5b0nqS3Jb0l6S13fy6C72PKaqwu1eULG/TjTfs0POxxlwMAAEJSnMtF7r5W0toRbfdmvV6nzDTiyPuGJP1JnjVOe9cva9Gf/3CTfvX+EV1ybn3c5QAAgBCw43sB+OoF56i6tFhPrt899sUAAGBKIGQVgPJ0SquWztbat/er8xh7ZgEAMB0QsgrEH182X0Vmuu1/r1PvwGDc5QAAgDwRsgrEwqZq/fevXayt+4/pkX/aGXc5AAAgT4SsAnLleU36nSWz9Mhr76mzm2lDAACmMkJWgfmLFYvVd3JYz721P+5SAABAHghZBWZhU5XaZtXoJ5t51A4AAFMZIasAXXvRLG38sEsdnT1xlwIAACaIkFWAbri4VbXlJfqLH27SwOBw3OUAAIAJIGQVoKaaMj1wwxK9vfeo/uaFX8ddDgAAmABCVoH66gXn6NZL5ukffvG+Xt52IO5yAADAOBGyCthf/vb5aptVo2889Zb2dZ2IuxwAADAOhKwCVlaS0kM3L9PgkOtPHt+gvpNDcZcEAAByRMgqcJ9prNK3/3Cp3tl3VHf/42a5e9wlAQCAHBCypoAVbc36xlcW69lN+/Ttn+2IuxwAAJCD4rgLQG6+fuVCfXC4V3/38g41VKX17y6ZH3dJAADgLBjJmiLMTP/19y7Ul89r0r1rtujOH7ypj47yfEMAAAoVIWsKKU4V6eGvXazbLp2vV37dqdu/u07H+k7GXRYAABgFIWuKKStJ6b7fvUAP33yxtu0/pisf/Lm+88v32RkeAIACQ8iaoq48r0nPfv0yLW6u1n96bqu+8q1/0g/Xfcg2DwAAFAgrtC0B2tvbff369XGXMWW4u37+7kE9+MJ2bd1/TDMr01q9dLaubjtHnd19WjqnTvPqK+MuEwCAacnMNrh7+6jnCFnTg7vr9Z2H9b1/2aVXtneenj4sL0npknPrVVdeopryEtUGH+XplIqLTCWpIhWnTMVFRUoVWczfBQAA4alMp3TpwoZIv8bZQhZbOEwTZqZLz23Qpec2qLvvpH7ZcVi15SV6Yt2Heu9gj9490K2jJ06qu28w7lIBAJgUC5uq9LO7rojt6xOypqHqshKt/Ow5kqRLzq3/1LnBoWF19w2qb3BIg0Ouk0PDGhzO/Flgg5oAAOSltDjepeeErIQpThVpRmU67jIAAJj2eHchAABABAhZAAAAESBkAQAARICQBQAAEAFCFgAAQAQIWQAAABEgZAEAAESAkAUAABABQhYAAEAECFkAAAARMC+wB9aZ2UFJuyL+Mg2SDkX8NTA+9Elhol8KE/1SeOiTwjQZ/TLP3RtHO1FwIWsymNl6d2+Puw58gj4pTPRLYaJfCg99Upji7hemCwEAACJAyAIAAIhAUkPWo3EXgN9AnxQm+qUw0S+Fhz4pTLH2SyLXZAEAAEQtqSNZAAAAkUpUyDKzlWa23cw6zOzuuOtJEjN7zMw6zeydrLaZZvaSme0I/pwRtJuZ/X3QT5vN7OL4Kp++zGyOmb1qZlvNbIuZ/VnQTr/EyMzKzOxfzeytoF/+c9C+wMx+Ffz8f2hm6aC9NDjuCM7Pj/UbmMbMLGVmG83sJ8ExfRIzM/vAzN42s01mtj5oK5jfYYkJWWaWkvSwpGsktUm6ycza4q0qUb4jaeWItrslvezuiyS9HBxLmT5aFHzcIel/TFKNSTMo6Rvu3ibp85K+HvydoF/i1S/pKne/SNJSSSvN7POS/kbSt9x9oaSPJd0eXH+7pI+D9m8F1yEafyZpW9YxfVIYrnT3pVlbNRTM77DEhCxJyyV1uPtOdx+Q9ISk1THXlBju/pqkIyOaV0v6bvD6u5Kuy2r/nme8IanOzGZNSqEJ4u773f3N4HW3Mv94tIh+iVXw8+0JDkuCD5d0laSng/aR/XKqv56W9GUzs8mpNjnMrFXS70j6X8GxiT4pVAXzOyxJIatF0u6s4z1BG+LT7O77g9cfSWoOXtNXkyyYzlgm6VeiX2IXTEttktQp6SVJ70nqcvfB4JLsn/3pfgnOH5VUP6kFJ8O3Jf0HScPBcb3ok0Lgkn5qZhvM7I6grWB+hxVH+cmBXLm7mxlvdY2BmVVJ+kdJf+7ux7L/w02/xMPdhyQtNbM6Sc9IOi/eipLNzK6V1OnuG8zsSzGXg0+73N33mlmTpJfM7NfZJ+P+HZakkay9kuZkHbcGbYjPgVNDtcGfnUE7fTVJzKxEmYD1f9z9R0Ez/VIg3L1L0quSLlFmauPUf4yzf/an+yU4Xyvp8ORWOu1dJmmVmX2gzFKTqyT9neiT2Ln73uDPTmX+Q7JcBfQ7LEkha52kRcG7QdKSbpS0Juaakm6NpFuD17dK+nFW+x8F7wT5vKSjWUO/CEmwRuQfJG1z9/+WdYp+iZGZNQYjWDKzcklfUWa93KuSbgguG9kvp/rrBkmvOBsghsrd/9LdW919vjL/drzi7l8TfRIrM6s0s+pTryVdLekdFdDvsERtRmpmv63MvHpK0mPu/tfxVpQcZvZ/JX1JmSeiH5B0n6RnJT0paa6kXZL+wN2PBP/4P6TMuxF7Jf2xu6+Poexpzcwul/TPkt7WJ+tM/qMy67Lol5iY2RJlFuumlPmP8JPufr+ZfUaZUZSZkjZKusXd+82sTNLjyqypOyLpRnffGU/1018wXfhNd7+WPolX8PN/JjgslvQDd/9rM6tXgfwOS1TIAgAAmCxJmi4EAACYNIQsAACACBCyAAAAIkDIAgAAiAAhCwAAIAKELAAAgAgQsgAAACJAyAIAAIjA/wdrvttRcjw0KQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "criterion=nn.MSELoss()\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.01)\n",
    "losses=[]\n",
    "for i in range(500):\n",
    "    for x,y in xor_dl:\n",
    "        # 将x计算预测值\n",
    "        y_pred=model(x.to(device))\n",
    "        # 计算损失\n",
    "        loss=criterion(y_pred, y.to(device).unsqueeze(1))\n",
    "        losses.append(loss.item())\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "for p in model.parameters():\n",
    "    print(p)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "\n",
    "i=np.arange(len(losses))+1\n",
    "plt.plot(i,np.array(losses))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意由于预测出来的y_pred是一个列向量，而y本身是一个行向量，所以我们使用unsqueeze()方法将其转化为列向量，这样才能正确计算损失。否则，损失的计算将根据广播的计算法法进行计算。\n",
    "\n",
    "接下来，可以使用该模型进行预测，比如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3298],\n",
      "        [1.0000],\n",
      "        [0.3298],\n",
      "        [0.3298]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "X_pred=torch.tensor([[0.0,0],[0,1],[1,0],[1,1]]).to(device)\n",
    "\n",
    "# 改为预测模式\n",
    "model.eval()\n",
    "# 预测\n",
    "result=model(X_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意在预测前我们将模型改为了预测模式：\n",
    "```python\n",
    "model.eval()\n",
    "```\n",
    "这么做的目的是为了告诉模型我们并不是在训练模型。对于简单模型来说这一步不是必须，但是如果我们加入了Dropout层之类，就会有比较大差异了，所以一个好的习惯是做预测时都将其改成预测模式。\n",
    "\n",
    "关闭预测模式，打开训练模式需要：\n",
    "```python\n",
    "model.train()\n",
    "```\n",
    "即可。\n",
    "\n",
    "注意到以上模型效果有时不太理想。实际上现在我们的数据是确定性的：输出和输入之间是函数关系，没有误差。而且，根据我们之前对网络设计的讨论，该网络完全可以学习到这个函数。然而现实情况确是预测误差很大。\n",
    "\n",
    "这也就是我们在神经网络里面遇到的问题：网络结构是非线性的，其性质很多时候取决于网络的结构和使用的激活函数的性质，非常有可能存在大量的鞍点、局部最优点等等。这些问题的存在会使得我们多数时候找不到那个最优解。\n",
    "\n",
    "比如，在这里，虽然ReLU激活函数在理论上我们已经证明了其优异的性质，但是在使用中，由于ReLU函数对于$x<0$的部分，导数为0，这就导致如果我们不小心走进了这部分区域的话，这一块的梯度将会直接等于0，从而对参数的更新方向不带来任何的信息。所以一个好的实践是把ReLU的所有的参数设为正的，从而在初始时就能够有方向信息。\n",
    "\n",
    "或者，我们可以将ReLU激活函数稍微修改一下，即：**带泄露的线性整流函数**（**Leaky ReLU**）：$$g\\left(x\\right)=\\begin{cases}\n",
    "\\alpha x & x<0\\\\\n",
    "x & x\\geq0\n",
    "\\end{cases}$$\n",
    "函数图像为：\n",
    "![](pic/LeakyReLU.png \"Leaky ReLU函数\")\n",
    "一般$\\alpha$取比较小的值，比如默认的是0.01，从而即使跑到了小于0的区域，也不会使得梯度直接等于0。我们可以将ReLU激活函数替换成LeakyReLU函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.2877,  1.3829],\n",
      "        [-0.6622, -0.6746]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-2.0769,  0.6628], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.6810, -1.5428]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.9877], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.0664e-06],\n",
      "        [9.9952e-01],\n",
      "        [1.0001e+00],\n",
      "        [5.2452e-06]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "class XOR_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XOR_net, self).__init__()\n",
    "        # 第一层：线性组合，2个输入2个输出\n",
    "        self.layer1=nn.Sequential(nn.Linear(2,2),nn.LeakyReLU(inplace=True))\n",
    "        # 第二层：线性组合，2个输入1个输出\n",
    "        self.layer2=nn.Sequential(nn.Linear(2,1))\n",
    "    def forward(self,x):\n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "        return x\n",
    "\n",
    "model=XOR_net().to(device)\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.01)\n",
    "losses=[]\n",
    "for i in range(500):\n",
    "    for x,y in xor_dl:\n",
    "        # 将x计算预测值\n",
    "        y_pred=model(x.to(device))\n",
    "        # 计算损失\n",
    "        loss=criterion(y_pred, y.to(device).unsqueeze(1))\n",
    "        losses.append(loss.item())\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "for p in model.parameters():\n",
    "    print(p)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "\n",
    "i=np.arange(len(losses))+1\n",
    "plt.plot(i,np.array(losses))\n",
    "plt.show()\n",
    "\n",
    "X_pred=torch.tensor([[0.0,0],[0,1],[1,0],[1,1]]).to(device)\n",
    "\n",
    "# 改为预测模式\n",
    "model.eval()\n",
    "# 预测\n",
    "result=model(X_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到已经有不错的效果。\n",
    "\n",
    "当然，也可以粗暴的增加网络的宽度，也可以获得不错的效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.1554,  1.1317],\n",
      "        [-0.0269,  0.1330],\n",
      "        [-0.3700, -0.0344],\n",
      "        [ 0.8186, -0.7991],\n",
      "        [-0.1184,  0.0412],\n",
      "        [ 0.4288,  0.1084]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0094,  0.2966, -0.6076, -0.0061, -0.7065, -0.7650], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.8928, -0.0781,  0.0088,  1.2385,  0.0407, -0.0594]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0147], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAEvCAYAAAB2a9QGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAg+ElEQVR4nO3de3SV9b3n8c83OzcCAZKdBOSaC5GLN2hTAqhVwAt2PGLP2B497Rzb4xmnM7XTmfbMLM+ctdqOna7Vyzqn7Uw9Pbpaz3S6xlrb6YW2toqgtoogQRAFRJOAQEQSCPdLIMl3/tgPuomBbGDvPHvv5/1aKyv7+T2/nXzJT8OH5/nu3zZ3FwAAANKrIOwCAAAA8hEhCwAAIAMIWQAAABlAyAIAAMgAQhYAAEAGELIAAAAyoDCVSWa2RNJ3JcUk/cDdvz7g/Bck/Y2kXkldkv7a3d8KzvVJejWYusPdbzvX96qqqvLa2trz+TMAAACEYt26dXvdvXqwc0OGLDOLSXpQ0o2Sdklaa2bL3H1z0rT1kprc/ZiZ/XtJ35T0F8G54+4+O9Via2tr1dLSkup0AACA0JjZW2c7l8rtwrmSWt293d1PSnpM0tLkCe7+jLsfCw5XS5p0ocUCAADkg1RC1kRJO5OOdwVjZ3OPpN8nHZeaWYuZrTaz28+/RAAAgNyTUk9Wqszsk5KaJF2XNDzV3TvMrF7SSjN71d3bBjzvXkn3StKUKVPSWRIAAEAoUrmS1SFpctLxpGDsDGZ2g6S/l3Sbu/ecHnf3juBzu6RnJc0Z+Fx3f9jdm9y9qbp60N4xAACAnJJKyForqdHM6sysWNKdkpYlTzCzOZIeUiJgdSaNV5hZSfC4StLVkpIb5gEAAPLSkLcL3b3XzO6T9KQSWzg84u6bzOwBSS3uvkzStySNkvQzM5Pe26phpqSHzKxfiUD39QGvSgQAAMhL5u5h13CGpqYmZwsHAACQC8xsnbs3DXaOHd8BAAAygJAFAACQAZELWb19/fr1hg6te6s77FIAAEAei1zIihWYvrJskx57aefQkwEAAC5Q5EKWmemDUyu17q39YZcCAADyWORCliQ11Vaofe9R7T3SM/RkAACACxDJkPWh2gpJ4moWAADImEiGrMsnjlFxYYFe2kbzOwAAyIxIhqySwpia6yr1zNbOoScDAABcgEiGLElaPKNG7V1HtW3v0bBLAQAAeSi6IWvmOEnSii17Qq4EAADko8iGrMmVZZo+rlwrtnDLEAAApF9kQ5YkLZpZo7Xbu3Xw+KmwSwEAAHkm0iHrhpk16u13/fGNrrBLAQAAeSbSIWv25ApVjiymLwsAAKRdpENWrMC0cHqNntnapd6+/rDLAQAAeSTSIUuSFs+s0cHjp9j9HQAApFXkQ9a1jVUqiplWvM6rDAEAQPpEPmSVlxZpXn2cviwAAJBWkQ9ZUmL397auo9rO7u8AACBNCFl6b/f3p7maBQAA0oSQpcTu75eOG8Xu7wAAIG0IWYHFM8ex+zsAAEgbQlZg8Qx2fwcAAOlDyArMmcLu7wAAIH0IWYFYgen66dXs/g4AANKCkJXkhpnjdPD4Kb2840DYpQAAgBxHyEpyevd3tnIAAAAXi5CVpLy0SPMbqvT713bL3cMuBwAA5DBC1gC3XnmJdnYf18ZdB8MuBQAA5DBC1gA3zxqvopjpN6+8HXYpAAAghxGyBhhTVqRFM2r0i/UdOnGqL+xyAABAjiJkDeKv5teq++hJ/W7j7rBLAQAAOYqQNYgFDXE1VI/Uoy/tCLsUAACQowhZgzAz/fkHJmndW/u1a/+xsMsBAAA5iJB1Fn925QRJ4pYhAAC4IISss5gSL9NVk8fqt4QsAABwAQhZ53DL5eP1asdBdRw4HnYpAAAgxxCyzuHmy8ZLkp587Z2QKwEAALmGkHUOdVUjNWN8uf6wiZAFAADODyFrCDdfNl5rt3er63BP2KUAAIAcQsgawpLLx8tdenrLnrBLAQAAOSSlkGVmS8xsq5m1mtn9g5z/gpltNrONZrbCzKYmnbvbzN4MPu5OZ/HDYcb4ck2Nl+kP9GUBAIDzMGTIMrOYpAcl3SJplqS7zGzWgGnrJTW5+5WSfi7pm8FzKyV9WVKzpLmSvmxmFekrP/PMTDdfNl6r2vbq0IlTYZcDAAByRCpXsuZKanX3dnc/KekxSUuTJ7j7M+5+emv01ZImBY9vlrTc3bvdfb+k5ZKWpKf04XPTrHE61ed6dmtX2KUAAIAckUrImihpZ9LxrmDsbO6R9PsLfG5WmjOlQlWjSvQUrzIEAAApSmvju5l9UlKTpG+d5/PuNbMWM2vp6sq+q0WxAtONs2r07NYu9fT2hV0OAADIAamErA5Jk5OOJwVjZzCzGyT9vaTb3L3nfJ7r7g+7e5O7N1VXV6da+7C6adZ4Henp1aq2fWGXAgAAckAqIWutpEYzqzOzYkl3SlqWPMHM5kh6SImA1Zl06klJN5lZRdDwflMwlnMWTItrZHGMW4YAACAlQ4Ysd++VdJ8S4WiLpMfdfZOZPWBmtwXTviVplKSfmdkGM1sWPLdb0leVCGprJT0QjOWcksKYrp9Ro+Wb96iv38MuBwAAZLnCVCa5+xOSnhgw9qWkxzec47mPSHrkQgvMJjfNGqffbdyt9Tv2q6m2MuxyAABAFmPH9/OwcEaNimKmpzaz+zsAADg3QtZ5GF1apPkNVfRlAQCAIRGyztMNM2u0fd8xtXcdCbsUAACQxQhZ52nh9BpJ0srXO4eYCQAAooyQdZ4mV5apsWYUIQsAAJwTIesCLJpZo5e2deswbxgNAADOgpB1ARZNr1Fvv+v5N/eGXQoAAMhShKwL8MGpFRpdWqgV3DIEAABnQci6AIWxAl03vUbPbu1UP7u/AwCAQRCyLtDiGTXae+SkNnYcDLsUAACQhQhZF+i6S6tVYGzlAAAABkfIukAVI4s1Z0qFVr7OW+wAAID3I2RdhOsvrdZrHYfUffRk2KUAAIAsQ8i6CNc0VkmSXmhlKwcAAHAmQtZFuGLiGJWXFrJfFgAAeB9C1kUojBVoQUNcz7fulTtbOQAAgPcQsi7SNY3V6jhwXNv2Hg27FAAAkEUIWRfp2mmJvqzn6csCAABJCFkXaWq8TJMqRtCXBQAAzkDIukhmpmsbq/Ri2z719vWHXQ4AAMgShKw0uGZatQ739OqVXbzFDgAASCBkpcGChrjMxC1DAADwLkJWGlSMLNblE8bo+dausEsBAABZgpCVJtc0Vmn9jgM60tMbdikAACALELLS5NppVertd61u2xd2KQAAIAsQstLkg7UVKi0qYL8sAAAgiZCVNiWFMc2tixOyAACAJEJWWl0zLa7WziPac+hE2KUAAICQEbLSaEFD4i12XqQvCwCAyCNkpdGsS0ZrzIgirWrjliEAAFFHyEqjggLT/Pq4VnElCwCAyCNkpdmCaXHt2n9cO7uPhV0KAAAIESErzRY0xCVJL/AqQwAAIo2QlWYN1aNUU17CLUMAACKOkJVmZqYFDYm+LHcPuxwAABASQlYGLGio0t4jPWrtPBJ2KQAAICSErAyYH/RlccsQAIDoImRlwOTKMk2uHMF+WQAARBghK0MW1FfpxbZ96uunLwsAgCgiZGXIgmlxHTrRq81vHwq7FAAAEAJCVoa815fFLUMAAKKIkJUhNeWlaqwZRfM7AAARlVLIMrMlZrbVzFrN7P5Bzn/YzF42s14zu2PAuT4z2xB8LEtX4blgQUNcL23r1olTfWGXAgAAhtmQIcvMYpIelHSLpFmS7jKzWQOm7ZD0KUmPDvIljrv77ODjtousN6dcP6NGx0/16cV2rmYBABA1qVzJmiup1d3b3f2kpMckLU2e4O7b3X2jpP4M1Jiz5tfHVVYc0/LNe8IuBQAADLNUQtZESTuTjncFY6kqNbMWM1ttZrefT3G5rrQopusurdbTm/fwFjsAAETMcDS+T3X3Jkl/Kek7ZtYwcIKZ3RsEsZaurq5hKGn4LJxeo87DPXpjD2+xAwBAlKQSsjokTU46nhSMpcTdO4LP7ZKelTRnkDkPu3uTuzdVV1en+qVzwoJpbOUAAEAUpRKy1kpqNLM6MyuWdKeklF4laGYVZlYSPK6SdLWkzRdabC6aVFGmqfEyvdBK8zsAAFEyZMhy915J90l6UtIWSY+7+yYze8DMbpMkM/uQme2S9DFJD5nZpuDpMyW1mNkrkp6R9HV3j1TIkhJbOaxp36fePl4XAABAVBSmMsndn5D0xICxLyU9XqvEbcSBz1sl6YqLrDHnLWio0k9e2qnX3j6k2ZPHhl0OAAAYBuz4Pgx4ix0AAKKHkDUMqkaVaMb4cq2iLwsAgMggZA2T+Q1xrd3erZ5e3mIHAIAoIGQNk/n1cfX09uuVnQfDLgUAAAwDQtYwmVtXKTNpDe9jCABAJBCyhsnYsmJNH1euNdu6wy4FAAAMA0LWMJpXH9e6t/brFPtlAQCQ9whZw6i5rlLHT/Vp4y76sgAAyHeErGE0t65SkrSaviwAAPIeIWsYxUeV6NJxo+jLAgAgAghZw6y5Lq5127t5H0MAAPIcIWuYNddX6ujJPr329qGwSwEAABlEyBpmp/uy2C8LAID8RsgaZjXlpaqvHknzOwAAeY6QFYJ59XG1bN+vvn4PuxQAAJAhhKwQNNdV6nBPrzbTlwUAQN4iZIVgXn1ckrRmG7cMAQDIV4SsEIwbXaraeJlWt7NfFgAA+YqQFZLmurhe2raPviwAAPIUISskzfWVOnSiV6+/Q18WAAD5iJAVkubTfVncMgQAIC8RskIycewITa4cQfM7AAB5ipAVokRfVrf66csCACDvELJC1FxXqf3HTumNzsNhlwIAANKMkBWiefRlAQCQtwhZIZpUMUITxpTSlwUAQB4iZIXIzDSvPtGX5U5fFgAA+YSQFbLm+krtPXJSrZ1Hwi4FAACkESErZKf7slZvoy8LAIB8QsgK2ZTKMl0yplSr2+nLAgAgnxCyQmZmaq6r1Jp2+rIAAMgnhKwsMK8+rr1HetTWdTTsUgAAQJoQsrLAu31Z3DIEACBvELKywNR4mcaPpi8LAIB8QsjKAon9siq1mr4sAADyBiErSzQHfVnte+nLAgAgHxCysgR9WQAA5BdCVpaojZdp3OgSrebNogEAyAuErCxx+n0MV7fvoy8LAIA8QMjKIvPq4+o6TF8WAAD5gJCVRZrrKiVJa7hlCABAziNkZZG6qpGqKS+h+R0AgDxAyMoi9GUBAJA/UgpZZrbEzLaaWauZ3T/I+Q+b2ctm1mtmdww4d7eZvRl83J2uwvPVvPq4Og/3aBt9WQAA5LQhQ5aZxSQ9KOkWSbMk3WVmswZM2yHpU5IeHfDcSklfltQsaa6kL5tZxcWXnb/m1Sf6stjKAQCA3JbKlay5klrdvd3dT0p6TNLS5Anuvt3dN0rqH/DcmyUtd/dud98vabmkJWmoO2/VVY1UdXmJ1myjLwsAgFyWSsiaKGln0vGuYCwVKT3XzO41sxYza+nq6krxS+cn+rIAAMgPWdH47u4Pu3uTuzdVV1eHXU7o5tVXas+hHm3fdyzsUgAAwAVKJWR1SJqcdDwpGEvFxTw3sngfQwAAcl8qIWutpEYzqzOzYkl3SlqW4td/UtJNZlYRNLzfFIzhHOqDvixCFgAAuWvIkOXuvZLuUyIcbZH0uLtvMrMHzOw2STKzD5nZLkkfk/SQmW0Kntst6atKBLW1kh4IxnAOZqbmukqtae+mLwsAgBxVmMokd39C0hMDxr6U9HitErcCB3vuI5IeuYgaI2lefVy/3bhb2/cdU13VyLDLAQAA5ykrGt/xfvMbEn1Zq9r2hlwJAAC4EISsLFVfNVKXjCnVqlb6sgAAyEWErCxlZlrQUKUX2vaqv5++LAAAcg0hK4td0xjXgWOntHn3obBLAQAA54mQlcWubqiSJD3fSl8WAAC5hpCVxWpGl6qxZpReIGQBAJBzCFlZ7uppVVq7vVsnTvWFXQoAADgPhKwsd/W0Kp041a+Xd+wPuxQAAHAeCFlZrrm+UgUmvdjGVg4AAOQSQlaWG11apCsnjdUqQhYAADmFkJUDFjTE9crOAzrS0xt2KQAAIEWErBywoKFKvf2utdt4b20AAHIFISsHNNVWqDhWwPsYAgCQQwhZOaC0KKYPTB2rF3gfQwAAcgYhK0csaKjS5t2HtP/oybBLAQAAKSBk5Yirp8UlSS+2czULAIBcQMjKEVdOGquy4hh9WQAA5AhCVo4oihVobl0l+2UBAJAjCFk55OqGKrV3HdU7B0+EXQoAABgCISuHzG9I9GX96c2ukCsBAABDIWTlkFmXjFZ1eYmefYOQBQBAtiNk5ZCCAtPC6dX64xtdOtXXH3Y5AADgHAhZOWbh9BodPtGrl9/aH3YpAADgHAhZOeaaxioVxUzPbOWWIQAA2YyQlWPKS4v0odpKPfN6Z9ilAACAcyBk5aCF02u0dc9hdRw4HnYpAADgLAhZOWjhjBpJ4moWAABZjJCVgxqqR2pKZRkhCwCALEbIykFmpkUzavR8614dP9kXdjkAAGAQhKwctWhGjXp6+/ViO28YDQBANiJk5ajm+kqVFce0kluGAABkJUJWjiopjOmaaVVauaVT7h52OQAAYABCVg5bPLNGbx88oa17DoddCgAAGICQlcMWTk9s5bBiC7cMAQDINoSsHFYzulRXTBxDXxYAAFmIkJXjFs2o0fod+9V99GTYpQAAgCSErBy3aEaN+l167g2uZgEAkE0IWTnuioljVDWqRCtf7wq7FAAAkISQleMKCkwLp1frua2dOtXXH3Y5AAAgQMjKA4tn1ujQiV6te2t/2KUAAIAAISsPXNNYraKY8YbRAABkEUJWHhhVUqjmurhWELIAAMgaKYUsM1tiZlvNrNXM7h/kfImZ/TQ4v8bMaoPxWjM7bmYbgo9/TnP9CCyaUaPWziPase9Y2KUAAAClELLMLCbpQUm3SJol6S4zmzVg2j2S9rv7NEnflvSNpHNt7j47+PhMmurGAItmJHZ/f3rLnpArAQAAUmpXsuZKanX3dnc/KekxSUsHzFkq6UfB459LWmxmlr4yMZTaqpFqrBmlpza/E3YpAABAqYWsiZJ2Jh3vCsYGnePuvZIOSooH5+rMbL2ZPWdm1w72DczsXjNrMbOWri72e7pQSy4fr5e2dWvfkZ6wSwEAIPIy3fi+W9IUd58j6QuSHjWz0QMnufvD7t7k7k3V1dUZLil/3XzZePU7twwBAMgGqYSsDkmTk44nBWODzjGzQkljJO1z9x533ydJ7r5OUpukSy+2aAzusgmjNTVept+8sjvsUgAAiLxUQtZaSY1mVmdmxZLulLRswJxlku4OHt8haaW7u5lVB43zMrN6SY2S2tNTOgYyM9121QStaturzsMnwi4HAIBIGzJkBT1W90l6UtIWSY+7+yYze8DMbgum/VBS3MxalbgteHqbhw9L2mhmG5RoiP+Mu3en+c+AJEtnT1C/S7/byNUsAADCZO4edg1naGpq8paWlrDLyGkf+e6fVFxYoF999uqwSwEAIK+Z2Tp3bxrsHDu+56HbZk/Qhp0H9Na+o2GXAgBAZBGy8tCfXTVBkrRsw9shVwIAQHQRsvLQxLEjNLeuUr/a0KFsux0MAEBUELLy1EfnTFRb11G91nEo7FIAAIgkQlae+sjll6g4VqBfrh+4pRkAABgOhKw8NaasSItm1GjZK2+rt68/7HIAAIgcQlYeu33ORO090qMX2vaFXQoAAJFDyMpjC2dUa8yIIv2sZefQkwEAQFoRsvJYSWFMH50zUU9t2qPuoyfDLgcAgEghZOW5u+ZO0cm+fv3i5V1hlwIAQKQQsvLc9PHl+sCUsXr0pR3smQUAwDAiZEXAXXOnqL3rqNZu3x92KQAARAYhKwJuvXKCyksK9djaHWGXAgBAZBCyImBEcUy3XjVBv3/1HR0+cSrscgAAiARCVkR8vGmSjp/q0+827g67FAAAIoGQFRGzJ4/VjPHl+sHz29TXTwM8AACZRsiKCDPTfYumqbXziJ54latZAABkGiErQm65/BJNqxml/7XyTfVzNQsAgIwiZEVIrMD0uUXT9MaeI/rDpnfCLgcAgLxGyIqYW6+coLqqkfqnZ1vZnBQAgAwiZEVMrMB074fr9VrHIa1q2xd2OQAA5C1CVgR9dM5EVZeX6J+fawu7FAAA8hYhK4JKi2L69NW1+tObe/Vax8GwywEAIC8RsiLqE81TNaqkUN/nahYAABlByIqoMSOKdPeCqXri1d16/Z1DYZcDAEDeIWRF2L+9tl6jigv1zT9sDbsUAADyDiErwsaWFes/Lm7Uytc7tXzznrDLAQAgrxCyIu5TV9fq0nGj9JVlm3T8ZF/Y5QAAkDcIWRFXFCvQV5dero4Dx/W9Z94MuxwAAPIGIQtqro/rz+dM1MN/bFdb15GwywEAIC8QsiBJ+ruPzFRpUUxf+vVrvN0OAABpQMiCJKm6vET/5ebpeqF1n36zcXfY5QAAkPMIWXjXJ5qn6oqJY/SVZZvUefhE2OUAAJDTCFl4V6zA9I8fv0rHTvbq8z/ZoFN9/WGXBABAziJk4QyN48r1tduv0Ivt+/SlX2+iPwsAgAtUGHYByD7/+oOT1NZ1RP/0bJsaqkfqb66tD7skAAByDiELg/rbm6Zr+76j+toTWzQ1PlI3zhoXdkkAAOQUbhdiUAUFpn/42GxdOXGM7nv0Za1q2xt2SQAA5BRCFs5qRHFM//LpuZpSWaZP/cta/XL9rrBLAgAgZxCycE6VI4v1+L+brw9MGav//NNX9I0/vK7+fprhAQAYCiELQ6oYWawf39Osv2yeou8/26a//tFa7ew+FnZZAABkNUIWUlIUK9DXbr9cX116mda0d+vGbz+nby9/QwePnQq7NAAAslJKIcvMlpjZVjNrNbP7BzlfYmY/Dc6vMbPapHN/F4xvNbOb01g7hpmZ6d/Mr9WKL16nRTNq9N0Vb2rB11fof/x2s94+cDzs8gAAyCo21GaTZhaT9IakGyXtkrRW0l3uvjlpzn+QdKW7f8bM7pT0UXf/CzObJeknkuZKmiDpaUmXunvf2b5fU1OTt7S0XOQfC8Nhy+5Deui5Nv1m42719bvmTBmrRdNr9IGpFbpy0hiVlxaFXSIAABllZuvcvWmwc6nskzVXUqu7twdf7DFJSyVtTpqzVNJXgsc/l/Q9M7Ng/DF375G0zcxag6/34oX8QZBdZl4yWt+5c46+eNN0/Wp9h57avEf/sPwNSZKZVBcfqYkVI3TJmFLVlJdqZEmhyopjKiuOqbQoJrPE1xmY85MPz/aPgORh1+BzTPZuLe87Fwzau8eDP2fg+fdGBpszyDcCAIRmZHFMC6ZVhfb9UwlZEyXtTDreJan5bHPcvdfMDkqKB+OrBzx34sBvYGb3SrpXkqZMmZJq7cgSkyvL9LnFjfrc4kYdPHZKG3Yd0IYdB/T6O4f09sET2vpOl7qO9LwvTAEAkEnTakbp6S9cF9r3z4od3939YUkPS4nbhSGXg4swpqxI111aresurT5j3N114lS/jp3s1bGTfTpx6sw7xu+/CPT+K0Znjp555Wjg00//R5R8Jey9sTNHTh8PPO8Dzw9y9YzgCADZq6Qw3Nf3pRKyOiRNTjqeFIwNNmeXmRVKGiNpX4rPRQSYmUYUxzSiOKZ42MUAADAMUol4ayU1mlmdmRVLulPSsgFzlkm6O3h8h6SVnriEsEzSncGrD+skNUp6KT2lAwAAZK8hr2QFPVb3SXpSUkzSI+6+ycwekNTi7ssk/VDSj4PG9m4lgpiCeY8r0STfK+mz53plIQAAQL4YcguH4cYWDgAAIFecawsHdnwHAADIAEIWAABABhCyAAAAMoCQBQAAkAGELAAAgAwgZAEAAGQAIQsAACADsm6fLDPrkvRWhr9NlaS9Gf4eOD+sSXZiXbIT65J9WJPsNBzrMtXdqwc7kXUhaziYWcvZNg5DOFiT7MS6ZCfWJfuwJtkp7HXhdiEAAEAGELIAAAAyIKoh6+GwC8D7sCbZiXXJTqxL9mFNslOo6xLJniwAAIBMi+qVLAAAgIyKVMgysyVmttXMWs3s/rDriRIze8TMOs3staSxSjNbbmZvBp8rgnEzs/8ZrNNGM/tAeJXnLzObbGbPmNlmM9tkZp8PxlmXEJlZqZm9ZGavBOvy34PxOjNbE/z8f2pmxcF4SXDcGpyvDfUPkMfMLGZm683st8ExaxIyM9tuZq+a2QYzawnGsuZ3WGRClpnFJD0o6RZJsyTdZWazwq0qUv63pCUDxu6XtMLdGyWtCI6lxBo1Bh/3Svr+MNUYNb2SvujusyTNk/TZ4P8J1iVcPZIWuftVkmZLWmJm8yR9Q9K33X2apP2S7gnm3yNpfzD+7WAeMuPzkrYkHbMm2WGhu89O2qoha36HRSZkSZorqdXd2939pKTHJC0NuabIcPc/SuoeMLxU0o+Cxz+SdHvS+P/xhNWSxprZJcNSaIS4+253fzl4fFiJvzwminUJVfDzPRIcFgUfLmmRpJ8H4wPX5fR6/VzSYjOz4ak2OsxskqR/JekHwbGJNclWWfM7LEoha6KknUnHu4IxhGecu+8OHr8jaVzwmLUaZsHtjDmS1oh1CV1wW2qDpE5JyyW1STrg7r3BlOSf/bvrEpw/KCk+rAVHw3ck/VdJ/cFxXKxJNnBJT5nZOjO7NxjLmt9hhZn84kCq3N3NjJe6hsDMRkn6f5L+k7sfSv4HN+sSDnfvkzTbzMZK+qWkGeFWFG1mdqukTndfZ2bXh1wOznSNu3eYWY2k5Wb2evLJsH+HRelKVoekyUnHk4IxhGfP6Uu1wefOYJy1GiZmVqREwPq/7v6LYJh1yRLufkDSM5LmK3Fr4/Q/jJN/9u+uS3B+jKR9w1tp3rta0m1mtl2JVpNFkr4r1iR07t4RfO5U4h8kc5VFv8OiFLLWSmoMXg1SLOlOSctCrinqlkm6O3h8t6RfJ43/VfBKkHmSDiZd+kWaBD0iP5S0xd3/MekU6xIiM6sOrmDJzEZIulGJfrlnJN0RTBu4LqfX6w5JK50NENPK3f/O3Se5e60Sf3esdPdPiDUJlZmNNLPy048l3STpNWXR77BIbUZqZh9R4r56TNIj7v61cCuKDjP7iaTrlXhH9D2SvizpV5IelzRF0luSPu7u3cFf/t9T4tWIxyR92t1bQig7r5nZNZL+JOlVvddn8t+U6MtiXUJiZlcq0awbU+Ifwo+7+wNmVq/EVZRKSeslfdLde8ysVNKPleip65Z0p7u3h1N9/gtuF/6tu9/KmoQr+Pn/MjgslPSou3/NzOLKkt9hkQpZAAAAwyVKtwsBAACGDSELAAAgAwhZAAAAGUDIAgAAyABCFgAAQAYQsgAAADKAkAUAAJABhCwAAIAM+P/5kfSo5nlb6wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2014e-07],\n",
      "        [ 1.0000e+00],\n",
      "        [ 1.0000e+00],\n",
      "        [-1.2200e-07]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "class XOR_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XOR_net, self).__init__()\n",
    "        # 第一层：线性组合，2个输入6个输出\n",
    "        self.layer1=nn.Sequential(nn.Linear(2,6),nn.ReLU(inplace=True))\n",
    "        # 第二层：线性组合，6个输入1个输出\n",
    "        self.layer2=nn.Sequential(nn.Linear(6,1))\n",
    "    def forward(self,x):\n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "        return x\n",
    "\n",
    "model=XOR_net().to(device)\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.01)\n",
    "losses=[]\n",
    "for i in range(500):\n",
    "    for x,y in xor_dl:\n",
    "        # 将x计算预测值\n",
    "        y_pred=model(x.to(device))\n",
    "        # 计算损失\n",
    "        loss=criterion(y_pred, y.to(device).unsqueeze(1))\n",
    "        losses.append(loss.item())\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "for p in model.parameters():\n",
    "    print(p)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "\n",
    "i=np.arange(len(losses))+1\n",
    "plt.plot(i,np.array(losses))\n",
    "plt.show()\n",
    "\n",
    "X_pred=torch.tensor([[0.0,0],[0,1],[1,0],[1,1]]).to(device)\n",
    "\n",
    "# 改为预测模式\n",
    "model.eval()\n",
    "# 预测\n",
    "result=model(X_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 激活函数\n",
    "\n",
    "上面我们使用了ReLU函数作为激活函数，该激活函数也被应用在很多神经网络中，而且理论也证明了该激活函数的合理性。然而，该激活函数也有问题，比如对于某些样本，可能会计算得到的梯度为0，从而不提供信息，我们又引入了LeakyReLU激活函数。\n",
    "\n",
    "实际上还有很多其他的激活函数可以使用。激活函数的设计本身也是神经网络研究里面的一个热门领域。\n",
    "\n",
    "比如，从ReLU出发:\n",
    "\n",
    "* 如果把小于0的部分的斜率参数化，即通过算法学习$\\alpha$，那么就变成了**参数ReLU**（**PReLU**）\n",
    "* 如果把小于0的部分的随机化，即令$\\alpha\\sim U(0,1)$的一个随机变量，那么就变成了**随机化ReLU**（**RReLU**）\n",
    "* 取$g(x)=\\frac{1}{\\beta}\\ln\\left[1+e^{\\beta x}\\right]$，称为Softplus函数，是对ReLU的一个连续逼近，不过相比较ReLU其表现可能还更差，故一般不建议使用\n",
    "* 取$g(x)=x*\\Phi(x)$，其中$\\Phi(x)$为标准正态分布的分布函数，就变成了**正态误差线性整流函数**（Gaussian Error LU, GELU）\n",
    "* 取$g(x)=x*\\sigma(x)=\\frac{x}{1+e^{-x}}$，其中$\\sigma(x)$为标准Logistic分布的分布函数，就变成了**Sigmoid线性整流函数**（**Sigmoid LU**, **SiLU**）\n",
    "\n",
    "Softplus, GELU和SiLU都不是分段的，而是逐点的函数（element-wise），其图像如下：\n",
    "![](pic/GELU.png \"GELU函数\")\n",
    "\n",
    "此外，另一类非常常用的激活函数是Sigmoid函数，也就是Logistic分布的分布函数：$\\sigma\\left(x\\right)=\\frac{1}{1+e^{-x}}$如果将其做一个简单的变换，就变成了**双曲正切函数**（**hypobolic tangent functin**）：$$tanh\\left(x\\right)=2\\sigma\\left(2x\\right)-1=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$$其图像为：\n",
    "![](pic/sigmoid.png \"Sigmod和tanh函数\")\n",
    "\n",
    "以上两个函数都有同样的特点，即只有在$x$靠近0时导数比较大，而离0比较远的时候，导数接近于0，就没有那么敏感了。Sigmoid函数在ReLU使用之前是最常用的激活函数，然而目前在前馈网络中更多的则是使用ReLU类的激活函数，而其他的一些场合，比如在比如循环神经网络，或者建模概率时，会使用Sigmoid激活函数。\n",
    "\n",
    "此外，还有ReLU6激活函数，该函数不仅仅在$<0$的部分变成平的，在$>6$的部分也变成平的。还有Tanhshrink函数：$Tanhshrink(x)=x-tanh(x)$，与Sigmoid相反，在0附近斜率比较小，而远离0时斜率比较大，其图像为：\n",
    "![](pic/relu6.png \"ReLU6和Tanhshrink函数\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数\n",
    "\n",
    "除了激活函数外，神经网络的目标函数，也就是损失函数也非常重要。\n",
    "\n",
    "在XOR的例子中，我们最小化了误差平方和，也就是均方误差：$$MSE\\ Loss=\\sum_{i=1}^N \\left(y_i-\\hat{y}_i\\right)^2=||y_i-\\hat{y}_i||^2_2$$对于连续的输出，这是最自然也是最常用的损失函数。如果从概率论来讲，我们通过MSE的最小化可以得到条件期望的估计$\\widehat{\\mathbb{E}\\left(y_{i}|x_{i}\\right)}$。\n",
    "\n",
    "如果将以上的$L_2$范数换成$L_1$范数，即：$$L_1\\ Loss=\\sum_{i=1}^N \\left|y_i-\\hat{y}_i\\right|=||y_i-\\hat{y}_i||_1$$那么就得到了条件中位数的估计$\\widehat{Med\\left(y_{i}|x_{i}\\right)}$，也是常用的损失函数。\n",
    "\n",
    "而对于离散的输出，比如二元、多元、计数的输出，我们可以使用类似广义线性模型中的建模技巧，通过极大似然估计建模损失函数。\n",
    "\n",
    "在广义线性模型中，需要首先使用一个链接函数（link function）对条件期望$\\mathbb{E}\\left(y_{i}|x_{i}\\right)$进行建模，进而使用一个分布，给定条件期望建模$y$的分布：$y|x\\sim F\\left(\\mathbb{E}\\left(y_{i}|x_{i}\\right)\\right)$。从而可以使用极大似然估计，通过最小化：$$-\\ln {f\\left(\\mathbb{E}\\left(y_{i}|x_{i}\\right)\\right)}$$其中$f$为条件密度函数。\n",
    "\n",
    "比如，对于二元分布，我们可以在最后一层使用Sigmoid激活函数，将上一层的计算结果映射到$p_i\\in\\left(0,1\\right)$之间，假设$y_i|p_i\\sim Ber\\left(p_i\\right)$，从而最小化：$$-\\sum_i {\\left[y_i \\ln (p_i)+(1-y_i)\\ln (1-p_i)\\right]}$$在PyTorch中，可以使用BCELoss生成该损失函数。比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0310],\n",
      "        [0.9548],\n",
      "        [0.9578],\n",
      "        [0.0596]], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "class XOR_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XOR_net, self).__init__()\n",
    "        # 第一层：线性组合，2个输入6个输出\n",
    "        self.layer1=nn.Sequential(nn.Linear(2,6),nn.ReLU(inplace=True))\n",
    "        # 第二层：线性组合，6个输入1个输出，注意使用了Sigmoid激活函数映射到(0,1)\n",
    "        self.layer2=nn.Sequential(nn.Linear(6,1), nn.Sigmoid())\n",
    "    def forward(self,x):\n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "        return x\n",
    "\n",
    "model=XOR_net().to(device)\n",
    "# 这里使用BCELoss\n",
    "criterion=nn.BCELoss()\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.01)\n",
    "losses=[]\n",
    "for i in range(500):\n",
    "    for x,y in xor_dl:\n",
    "        # 将x计算预测值\n",
    "        y_pred=model(x.to(device))\n",
    "        # 计算损失\n",
    "        loss=criterion(y_pred, y.to(device).unsqueeze(1))\n",
    "        losses.append(loss.item())\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "\n",
    "i=np.arange(len(losses))+1\n",
    "plt.plot(i,np.array(losses))\n",
    "plt.show()\n",
    "\n",
    "X_pred=torch.tensor([[0.0,0],[0,1],[1,0],[1,1]]).to(device)\n",
    "\n",
    "# 改为预测模式\n",
    "model.eval()\n",
    "# 预测\n",
    "result=model(X_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BCELoss只能解决二元分类，如果需要解决多元分类的问题，可以首先使用Softmax激活函数：$$softmax\\left(x\\right)_i=\\frac{e^{x_i}}{\\sum_j{e^{x_j}}}$$其中$x=[x_1,x_2,...,x_J]$，$J$为分类的个数。其实以上设定就是Multinomial Logit的设定，有$$\\sum_j{softmax\\left(x\\right)_j}=1$$\n",
    "\n",
    "那么，对于$y\\in\\left\\{1,2,...,J\\right\\}$，极大似然最小化：$$\\sum_{i}\\sum_{j}1\\left\\{ y_{i}=j\\right\\} \\cdot softmax\\left(x\\right)_{j}$$PyTorch中**交叉熵损失函数**——**CrossEntropy**实现了以上损失函数。\n",
    "\n",
    "或者，注意到：$$\\ln \\left[softmax\\left(x\\right)_i\\right]=x_i-\\ln\\left(\\sum_j{e^{x_j}}\\right)$$我们称以上函数为对数Softmax函数，在PyTorch中有激活函数**LogSoftmax**计算以上函数。\n",
    "\n",
    "接着，使用**负对数似然损失**（**negative log likelihood loss**）——**NLLLOSS**：$$NLLLoss=-\\sum_i l_i$$其中$l_i$为计算出的对数似然函数值，就可以计算出CrossEntropy损失函数。所以CrossEntropy损失函数可以看作是LogSoftmax激活函数和NLLLoss损失函数的组合。\n",
    "\n",
    "比如，在XOR的例子中，有两个分类，如果我们使用CrossEntropy损失函数，必须有两个输出，所以我们首先修改DataLoader："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xor_data2(Dataset):\n",
    "    def __len__(self):\n",
    "        return X.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        x=X[i,:]\n",
    "        y=Y[i]\n",
    "        data=torch.from_numpy(x)\n",
    "        # 为了使用CrossEntropyLoss，这里的y需要是{0,1,...,J-1}的取值，且必须是Long类型\n",
    "        label=torch.tensor(y).long()\n",
    "        return data, label\n",
    "data2=xor_data2()\n",
    "xor_dl2=DataLoader(data2, batch_size=data.__len__(), pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意CrossEntropyLoss要求标签为long类型，所以我们使用long()将其转化为一个long类型。然后才能使用交叉熵损失函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9857e-01, 1.4341e-03],\n",
      "        [2.5176e-04, 9.9975e-01],\n",
      "        [1.7206e-03, 9.9828e-01],\n",
      "        [9.9960e-01, 3.9506e-04]], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class XOR_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XOR_net, self).__init__()\n",
    "        # 第一层：线性组合，2个输入6个输出\n",
    "        self.layer1=nn.Sequential(nn.Linear(2,6),nn.ReLU(inplace=True))\n",
    "        # 第二层：线性组合，6个输入2个输出，注意使用了Softmax激活函数\n",
    "        # dim=0代表在哪个维度上加起来为1，这里只有一个维度，所以是0\n",
    "        self.layer2=nn.Sequential(nn.Linear(6,2))\n",
    "    def forward(self,x):\n",
    "        x=self.layer1(x)\n",
    "        x=self.layer2(x)\n",
    "        return x\n",
    "\n",
    "model=XOR_net().to(device)\n",
    "# 这里使用CrossEntropyLoss\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.01)\n",
    "losses=[]\n",
    "for i in range(500):\n",
    "    for x,y in xor_dl2:\n",
    "        # 将x计算预测值\n",
    "        y_pred=model(x.to(device))\n",
    "        # 计算损失\n",
    "        loss=criterion(y_pred, y.to(device))\n",
    "        losses.append(loss.item())\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "\n",
    "i=np.arange(len(losses))+1\n",
    "plt.plot(i,np.array(losses))\n",
    "plt.show()\n",
    "\n",
    "X_pred=torch.tensor([[0.0,0],[0,1],[1,0],[1,1]]).to(device)\n",
    "\n",
    "# 改为预测模式\n",
    "model.eval()\n",
    "# 预测\n",
    "result=model(X_pred)\n",
    "result=torch.exp(result)\n",
    "result=result/torch.sum(result, axis=1).unsqueeze(1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化和标准化\n",
    "\n",
    "在神经网络中，参数的初始化是非常重要的。由于在网络中同一层的所有节点都是地位对等的，可以想象如果初始时所有的参数都初始化为0，那么梯度下降就不可能知道下降的方向。一个简单的网络可以写成：$$\\hat{y}=g_2\\left[g_1\\left(x'\\beta_1\\right)'\\beta_2\\right]$$如果对$\\beta_1$求导，可以得到导数为：$g_2'\\left[g_1\\left(x'\\beta_1\\right)'\\beta_2\\right]\\beta_2g_1\\left(x'\\beta_1\\right)x$，如果$beta_1$初始化为0，那么对$\\beta_1$求导的导数为0，从而不可能知道梯度更新的方向。\n",
    "\n",
    "为了解决这个问题，最简单的方法是对参数进行随机的初始化。随机初始化的方法有很多，也有文献讨论初始化的方法，不过由于PyTorch内置的默认初始化函数已经足够好用，我们几乎不需要担心初始化的问题，在这里我们就不再详细介绍了。\n",
    "\n",
    "此外，很多时候我们不仅仅需要对原始数据进行标准化（normalize），不同的层之间也可以进行标准化。特别是，如果网络的深度比较深，数据的初始分布会随着神经元的计算而出现偏移，此时数据的分布很可能并不集中在激活函数反应比较大的“甜区”，而是集中在激活函数导数比较小的地方，所以在训练中进行规范化也是非常有必要的。\n",
    "\n",
    "规范化的第一种常见的方法是**批标准化**（**Batch Normalization**）。由于我们在训练模型时，数据是一个批次一个批次输入的（可能不是全样本同时输入），所以对每个神经元，都可以在这个批次进行标准化。比如，如果每个批次数据的数据是100个，那么可以通过计算该神经元的所有100个数据输入进来以后计算得到的值，并计算均值$m$和标准差$s$，使用$$\\frac{h_i-m}{\\sigma}$$进行标准化。\n",
    "\n",
    "不过由于输入的不是全样本，所以在每次输入样本时，都会对$m$和$s$进行更新：$m=0.9\\times m_{-1} +0.1\\times m_0$其中$m_{-1}$为上一次计算的$m$，$m_0$位本次计算的$m$，0.1为momentum，可以自由设定。\n",
    "\n",
    "而将上面的方法进一步推广一下，加入一个漂移和放缩项，就有：$$\\frac{h_i-m}{\\sigma}\\times \\gamma+\\beta$$其中$\\gamma,\\beta$是待学习的参数。这样，就给本来的“标准化”过程加入了一定的可学习的自由度。\n",
    "\n",
    "然而批标准化不是没有问题的，特别是在序列数据，包括文本、时间序列等数据中，跨样本的标准化就没有对同一序列进行标准化更符合直觉。为此，另一种新的标准化方法是**层标准化**（**Layer Normalization**， https://arxiv.org/abs/1607.06450 ）。\n",
    "\n",
    "层标准化的公式与批标准化是一样的，区别仅仅在于：\n",
    "\n",
    "* 批标准化是对样本$i=1,...,N$进行加总求和然后标准化，层标准化是每个样本计算时，对同一层的所有神经元进行加总求和然后标准化\n",
    "* 由于层标准化不是针对样本的，所以不需要用动量跨迭代的更新均值和标准差\n",
    "\n",
    "形象的理解，批标准化是对所有人的语文成绩、数学成绩、英语成绩分别做标准化，而层标准化则是对同一个人的语文、数据、英语成绩做标准化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结构设计：深度还是宽度？\n",
    "\n",
    "在设计网络结构时，显然更深、更宽的网络具有更好的表达能力。比如在刚刚的XOR例子中，虽然我们知道了一个简单单层、两个隐藏节点的网络就可以表示该函数，然而问题是算法不一定能够精确找到这样的结构，而我们把这个网络加宽就可以轻易解决这个问题。\n",
    "\n",
    "根据万能近似定理，足够大的单层网络就可以很好的表达几乎所有函数了，然而问题是可能需要的节点或者参数可数是指数级增长的，而指数级增长的参数会使得模型难以求解。\n",
    "\n",
    "一个简单的方法是增加网络的深度而适当较少网络的宽度。这也就是“深度学习”中深度的含义：通过一个比较深的神经网络使用一个尽量小的模型对函数形式进行逼近。\n",
    "\n",
    "前馈网络可以看成是一个计算“潜在变量”、并根据“潜在变量”重新组合输出的过程，而更“深”的网络可以看成是为了找到这些“潜在变量”所需要的步骤。\n",
    "\n",
    "实际上，也有研究发现更“深”的模型有更好的泛化能力，所以一般而言相对于宽度，我们会选择一个稍微“深”一点的模型结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化\n",
    "\n",
    "不管是更深的还是更宽的模型，神经网络通常参数的数量都非常的大，对于有限的样本，如果不想办法规范模型的大小，很容易就会出现过拟合的情况。\n",
    "\n",
    "## 收缩\n",
    "\n",
    "就像在回归分析中那样，最简单的正则化方法是加入正则化项，比如岭回归所使用的$L_2$范数$$||\\theta||_2^2=\\theta'\\theta=\\sum_k \\theta_k^2$$以及Lasso所使用的$L_1$范数：$$||\\theta||_1=\\sum_k \\left|\\theta_k\\right|$$\n",
    "\n",
    "在神经网络中，如果使用正则化项，会对每个神经元的权重向量进行正则化，或者收缩（即$g\\left(Wx+b\\right)$中的$W$，$b$一般不需要收缩）。就像回归中那样，$L_2$正则化项使得权重向0衰减，所以也叫做权重衰减（weight decay），而$L_1$正则化则使得一些权重变为$0$，实现了特征选择。\n",
    "\n",
    "此外，正则化不一定使用统一的惩罚系数，也可以对每一层使用不同的惩罚系数：$$Loss+\\sum_l \\lambda_l\\times ||\\theta_l||_1$$其中$Loss$为损失函数，$l$代表层，而$\\lambda_l$为$l$层的惩罚系数，$\\theta_l$为$l$层的参数。当然，这么做就需要选择更多的超参数$\\lambda_l$。\n",
    "\n",
    "PyTorch中如果需要做$L_2$或者$L_1$正则化需要手动将其加上，比如：\n",
    "```python\n",
    "lamb=0.9\n",
    "loss=criterion(y_pred, y.to(device))\n",
    "for p in model.parameters():\n",
    "    loss += ( lamb * torch.sum(torch.abs(p)) )\n",
    "\n",
    "```\n",
    "\n",
    "## 早停\n",
    "\n",
    "我们知道，过拟合会导致样本外预测的误差变大，那么一个很自然的想法就是，我们选取一个使得样本外预测最小的模型就好了。\n",
    "\n",
    "一般而言，随着梯度下降算法的迭代次数的增加，样本外预测误差会出现一个“U”形曲线的关系。利用这一特征，**早停**（**eraly stopping**）算法让迭代在样本外预测误差达到最小的时候停下来，从而避免过拟合。\n",
    "\n",
    "为了实现早停，必须首先预留出一个验证集，模型首先使用训练集进行训练，然后每隔固定的一段时间对验证集的预测误差进行测试，并记录每一次的参数和预测误差。比如，可以在10000次迭代中，每隔50次就记录一次样本外预测误差，然后保留样本外预测误差最小的那一份参数就好了。\n",
    "\n",
    "实际上早停方法与收缩方法具有类似的功能：通过早停，参数的迭代就被限制在一个范围之内，这与收缩估计量是异曲同工的。\n",
    "\n",
    "这个算法的好处是其计算效率：我们接下来会介绍，其他方法都涉及到超参数的选择，一般需要使用交叉验证等方法，非常耗时。而这个算法只需要进行一次模型训练就可以了，不需要重复训练模型，所以是非常快速的正则化方法。\n",
    "\n",
    "然而，该算法同样也有缺点，即损失了样本。为了实现该方法，我们必须预留出一个验证集，从而没有使用全样本。也有避免这个缺点的方法，比如我们可以先使用该算法，并记录最优模型的迭代次数，然后使用全样本进行这么多次数的迭代，然而由于随机梯度下降和样本的随机性，没有任何理论可以保证部分样本和全样本的最优迭代次数是一样的。\n",
    "\n",
    "## 随机失活\n",
    "\n",
    "另外一种更加常用的方法是在样本中加入一定的随机性。实际上在很多模型中，在样本中加入随机性与收缩估计量是等价的。比如，对于线性回归模型，如果令$$\\epsilon_{i}\\sim N\\left(0,\\sigma^{2}I_{K}\\right)$$那么如果我们最小化目标函数：$$\\sum_{i=1}^{N}\\left[y_{i}-\\left(x_{i}+\\epsilon_{i}\\right)'\\beta\\right]^{2}$$将其展开，得到：$$\\sum_{i=1}^{N}\\left[\\left(y_{i}-x_{i}'\\beta\\right)+\\epsilon_{i}'\\beta\\right]^{2}=\\sum_{i=1}^{N}\\left(y_{i}-x_{i}'\\beta\\right)^{2}+2\\sum_{i=1}^{N}\\epsilon_{i}'\\beta\\left(y_{i}-x_{i}'\\beta\\right)+\\sum_{i=1}^{N}\\beta'\\epsilon_{i}\\epsilon_{i}'\\beta$$以上等式右边第二项期望为0，第三项期望为$\\sigma^2\\beta'\\beta$，所以样本量足够大时，以上的目标函数等价于最小化：$$\\sum_{i=1}^{N}\\left(y_{i}-x_{i}'\\beta\\right)^{2}+\\sigma^{2}\\beta'\\beta$$这实际上就是一个$L_2$正则化，而在$x$中加入的随机性的方差扮演了惩罚系数的角色。\n",
    "\n",
    "而在神经网络中，不仅仅可以在样本中加入一定的随机性，还可以在隐藏神经元中加入随机性，而且往往效果更好。\n",
    "\n",
    "在隐藏神经元中加入随机性，除了可以直接加如一个随机扰动之外，还有一种更常用的方法是**随机失活**（**dropout**）。\n",
    "\n",
    "Dropout即在模型的训练过程中，随机的让一些神经元失效，每次迭代时都随机选择一些神经元将其去掉，剩下的节点组成的网络进行模型的训练，使用反向传播算法计算梯度，更新参数。\n",
    "\n",
    "去掉神经元的方法很简单，只需要将随机选择出的神经元乘以0即可，而没有被选择去掉的则乘以1。\n",
    "\n",
    "Dropout有效的原理是这个算法模仿了bagging。回忆在bagging中，我们通过有放回抽样的方法选取样本，训练很多模型，最终将这些模型进行平均。而Dropout方法的每次随机选择神经元都可以看作是构建了一个新的神经网络。\n",
    "\n",
    "而与bagging不同的是，Dropout具有“参数共享”（parameters sharing）的特性，即不像baggering，通过随机化获得的每一棵树并不是独立的，相同节点的参数仍然是一样的。参数共享有助于模型的降维并提高模型的性能。\n",
    "\n",
    "而且更重要的是，不像bagging需要独立的训练很多模型，由于参数共享，Dropout只需要训练一次模型即可，仅仅需要在每次迭代的时候临时改变网络的结构，每一次迭代都是不同的模型，最终通过多次迭代，把所有的神经元的参数一起训练出来。所以Dropout在计算上是非常有效率的。\n",
    "\n",
    "当然，由于随机失活会随机扔掉一些神经元，所以如果使用Dropout，需要在模型的宽度上稍微加宽一些。\n",
    "\n",
    "一般而言，一个有用的经验是，对于输入层，以0.8的概率丢掉输入层的特征；而对于中间的隐含层，可以以0.5的概率丢掉一些神经元。\n",
    "\n",
    "在PyTorch中，Dropout可以使用专门的Dropout层进行：\n",
    "```python\n",
    "# 第一层：线性组合，10个输入20个输出\n",
    "self.layer1=nn.Sequential(nn.Linear(2,20),nn.ReLU(inplace=True))\n",
    "# 第二层：线性组合，20个输入20个输出，加入Dropout层，以0.5的概率dropout（默认就是0.5，所以也可以不写p=0.5）\n",
    "self.layer2=nn.Sequential(nn.Linear(20,20),nn.ReLU(inplace=True),nn.Dropout(p=0.5))\n",
    "# 第三层：线性组合，20个输入1个输出\n",
    "self.layer2=nn.Sequential(nn.Linear(20,1))\n",
    "```\n",
    "\n",
    "其中在第二层，我们使用了ReLU激活函数，在激活后使用了一个Dropout层，以0.5的概率丢掉ReLU输出的结果。\n",
    "\n",
    "需要额外注意的是，Dropout层在训练和在预测时的作用是不一样的：在训练的时候，Dropout层会以一定的概率用0乘以某个神经元，而在预测的时候，则是用某个神经元被丢掉的概率乘以这个神经元的值。所以如果使用了Dropout层，必须使用：\n",
    "```python\n",
    "model.eval()\n",
    "model.train()\n",
    "```\n",
    "对神经网络的模式进行切换。\n",
    "\n",
    "\n",
    "## 超参数的选择\n",
    "\n",
    "以上的正则化方法通常涉及到一些**超参数**（**hyperparameters**），比如收缩方法中的$\\lambda$，以及Dropout中的dropout的概率等，以及神经网络的一些参数，比如神经网络的深度、宽度、学习率、momentum等等，都是为了训练神经网络的参数而引入的参数，所以称为超参数。\n",
    "\n",
    "一般而言超参数是不能训练得到的，常见的方法比如交叉验证等都可以用于超参数的选取。然而在使用交叉验证时，计算量通常都非常大，比如为了从10个参数里面选取最优的，对于每个参数计算10折的交叉验证，那么就需要训练100次模型，对于大型数据而言通常需要很长的时间才能计算好。\n",
    "\n",
    "当然，如果有不止一台机器，或者比如机器中有不止一张显卡，我们完全可以把不同的模型放在不同的机器或者显卡上运行，这可以大大的降低训练的时间。\n",
    "\n",
    "而对于超参数的取值，一般可以使用网格点法或者随机搜索法。比如，对于$\\lambda$，如果使用网格点法，可以取$\\lambda=0.01,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5...$等挨个尝试，而如果使用随机搜索法，则可以在某个区间内生成均匀分布的随机数进行搜索。\n",
    "\n",
    "此外，目前还有自动机器学习（AutoML）的包，比如NNI等，我们在这里不再详细介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实例：贸易运输方式选择\n",
    "\n",
    "作为一个综合实例，接下来我们使用PyTorch构建一个用于预测海关数据中运输方式的神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_税号编码</th>\n",
       "      <th>_金额</th>\n",
       "      <th>_数量</th>\n",
       "      <th>_价格</th>\n",
       "      <th>_企业编码</th>\n",
       "      <th>_消费地进口or生产地出口</th>\n",
       "      <th>_企业性质</th>\n",
       "      <th>_起运国或目的国</th>\n",
       "      <th>_海关口岸</th>\n",
       "      <th>_运输方式</th>\n",
       "      <th>_中转国</th>\n",
       "      <th>_数量单位编码</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90308910.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>3302961512</td>\n",
       "      <td>宁波其他</td>\n",
       "      <td>173</td>\n",
       "      <td>博茨瓦那</td>\n",
       "      <td>外港海关</td>\n",
       "      <td>江海运输</td>\n",
       "      <td>博茨瓦那</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73181590.0</td>\n",
       "      <td>13386.0</td>\n",
       "      <td>1383.0</td>\n",
       "      <td>9.678959</td>\n",
       "      <td>3414960440</td>\n",
       "      <td>宣城</td>\n",
       "      <td>173</td>\n",
       "      <td>墨西哥</td>\n",
       "      <td>外港海关</td>\n",
       "      <td>江海运输</td>\n",
       "      <td>墨西哥</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84818040.0</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>106.812500</td>\n",
       "      <td>3116961711</td>\n",
       "      <td>南汇</td>\n",
       "      <td>NaN</td>\n",
       "      <td>埃及</td>\n",
       "      <td>外港海关</td>\n",
       "      <td>汽车运输</td>\n",
       "      <td>埃及</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73072900.0</td>\n",
       "      <td>1279.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>85.266670</td>\n",
       "      <td>3116961711</td>\n",
       "      <td>南汇</td>\n",
       "      <td>NaN</td>\n",
       "      <td>埃及</td>\n",
       "      <td>外港海关</td>\n",
       "      <td>江海运输</td>\n",
       "      <td>埃及</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84824000.0</td>\n",
       "      <td>3363.0</td>\n",
       "      <td>4400.0</td>\n",
       "      <td>0.764318</td>\n",
       "      <td>3217961029</td>\n",
       "      <td>宿迁</td>\n",
       "      <td>NaN</td>\n",
       "      <td>秘鲁</td>\n",
       "      <td>外港海关</td>\n",
       "      <td>江海运输</td>\n",
       "      <td>秘鲁</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        _税号编码      _金额     _数量         _价格       _企业编码 _消费地进口or生产地出口 _企业性质  \\\n",
       "0  90308910.0    410.0     2.0  205.000000  3302961512          宁波其他   173   \n",
       "1  73181590.0  13386.0  1383.0    9.678959  3414960440            宣城   173   \n",
       "2  84818040.0   1709.0    16.0  106.812500  3116961711            南汇   NaN   \n",
       "3  73072900.0   1279.0    15.0   85.266670  3116961711            南汇   NaN   \n",
       "4  84824000.0   3363.0  4400.0    0.764318  3217961029            宿迁   NaN   \n",
       "\n",
       "  _起运国或目的国 _海关口岸 _运输方式  _中转国  _数量单位编码  \n",
       "0     博茨瓦那  外港海关  江海运输  博茨瓦那      1.0  \n",
       "1      墨西哥  外港海关  江海运输   墨西哥      9.0  \n",
       "2       埃及  外港海关  汽车运输    埃及      1.0  \n",
       "3       埃及  外港海关  江海运输    埃及      9.0  \n",
       "4       秘鲁  外港海关  江海运输    秘鲁      1.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DATA=pd.read_csv(\"csv/custom.csv\")\n",
    "DATA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来针对属性变量进行编码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['江海运输', '汽车运输', '航空运输', '空运', '铁路运输', '邮件运输', '其它运输', '邮运', '其它'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA['_运输方式'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用map()函数对以上数据进行编码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_税号编码</th>\n",
       "      <th>_金额</th>\n",
       "      <th>_数量</th>\n",
       "      <th>_价格</th>\n",
       "      <th>_企业编码</th>\n",
       "      <th>_消费地进口or生产地出口</th>\n",
       "      <th>_企业性质</th>\n",
       "      <th>_起运国或目的国</th>\n",
       "      <th>_海关口岸</th>\n",
       "      <th>_运输方式</th>\n",
       "      <th>_中转国</th>\n",
       "      <th>_数量单位编码</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90308910.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>3302961512</td>\n",
       "      <td>宁波其他</td>\n",
       "      <td>173</td>\n",
       "      <td>博茨瓦那</td>\n",
       "      <td>外港海关</td>\n",
       "      <td>江海运输</td>\n",
       "      <td>博茨瓦那</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73181590.0</td>\n",
       "      <td>13386.0</td>\n",
       "      <td>1383.0</td>\n",
       "      <td>9.678959</td>\n",
       "      <td>3414960440</td>\n",
       "      <td>宣城</td>\n",
       "      <td>173</td>\n",
       "      <td>墨西哥</td>\n",
       "      <td>外港海关</td>\n",
       "      <td>江海运输</td>\n",
       "      <td>墨西哥</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84818040.0</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>106.812500</td>\n",
       "      <td>3116961711</td>\n",
       "      <td>南汇</td>\n",
       "      <td>NaN</td>\n",
       "      <td>埃及</td>\n",
       "      <td>外港海关</td>\n",
       "      <td>汽车运输</td>\n",
       "      <td>埃及</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73072900.0</td>\n",
       "      <td>1279.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>85.266670</td>\n",
       "      <td>3116961711</td>\n",
       "      <td>南汇</td>\n",
       "      <td>NaN</td>\n",
       "      <td>埃及</td>\n",
       "      <td>外港海关</td>\n",
       "      <td>江海运输</td>\n",
       "      <td>埃及</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84824000.0</td>\n",
       "      <td>3363.0</td>\n",
       "      <td>4400.0</td>\n",
       "      <td>0.764318</td>\n",
       "      <td>3217961029</td>\n",
       "      <td>宿迁</td>\n",
       "      <td>NaN</td>\n",
       "      <td>秘鲁</td>\n",
       "      <td>外港海关</td>\n",
       "      <td>江海运输</td>\n",
       "      <td>秘鲁</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        _税号编码      _金额     _数量         _价格       _企业编码 _消费地进口or生产地出口 _企业性质  \\\n",
       "0  90308910.0    410.0     2.0  205.000000  3302961512          宁波其他   173   \n",
       "1  73181590.0  13386.0  1383.0    9.678959  3414960440            宣城   173   \n",
       "2  84818040.0   1709.0    16.0  106.812500  3116961711            南汇   NaN   \n",
       "3  73072900.0   1279.0    15.0   85.266670  3116961711            南汇   NaN   \n",
       "4  84824000.0   3363.0  4400.0    0.764318  3217961029            宿迁   NaN   \n",
       "\n",
       "  _起运国或目的国 _海关口岸 _运输方式  _中转国  _数量单位编码  label  \n",
       "0     博茨瓦那  外港海关  江海运输  博茨瓦那      1.0      0  \n",
       "1      墨西哥  外港海关  江海运输   墨西哥      9.0      0  \n",
       "2       埃及  外港海关  汽车运输    埃及      1.0      1  \n",
       "3       埃及  外港海关  江海运输    埃及      9.0      0  \n",
       "4       秘鲁  外港海关  江海运输    秘鲁      1.0      0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dict={'江海运输':0,\n",
    "       '汽车运输':1,\n",
    "       '航空运输':2,\n",
    "       '空运':2,\n",
    "       '铁路运输':3,\n",
    "       '邮件运输':4,\n",
    "       '邮运':4,\n",
    "       '其它运输':5,\n",
    "       '其它':5}\n",
    "DATA['label']=DATA['_运输方式'].map(y_dict)\n",
    "DATA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来对其他变量进行编码，其中分类变量我们将其编码为整数，税号我们使用前两位编码，用企业编码的前两位识别城市："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_税号编码</th>\n",
       "      <th>_金额</th>\n",
       "      <th>_数量</th>\n",
       "      <th>_价格</th>\n",
       "      <th>_企业编码</th>\n",
       "      <th>_消费地进口or生产地出口</th>\n",
       "      <th>_企业性质</th>\n",
       "      <th>_起运国或目的国</th>\n",
       "      <th>_海关口岸</th>\n",
       "      <th>_运输方式</th>\n",
       "      <th>_中转国</th>\n",
       "      <th>_数量单位编码</th>\n",
       "      <th>label</th>\n",
       "      <th>city_label</th>\n",
       "      <th>trans_label</th>\n",
       "      <th>target_label</th>\n",
       "      <th>custom_label</th>\n",
       "      <th>prod_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90308910.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>3302961512</td>\n",
       "      <td>宁波其他</td>\n",
       "      <td>173</td>\n",
       "      <td>博茨瓦那</td>\n",
       "      <td>外港海关</td>\n",
       "      <td>江海运输</td>\n",
       "      <td>博茨瓦那</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62625</th>\n",
       "      <td>90041000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3301968800</td>\n",
       "      <td>杭州其他</td>\n",
       "      <td>NaN</td>\n",
       "      <td>博茨瓦那</td>\n",
       "      <td>外港海关</td>\n",
       "      <td>江海运输</td>\n",
       "      <td>博茨瓦那</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106860</th>\n",
       "      <td>90318010.0</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3100.000000</td>\n",
       "      <td>3301965475</td>\n",
       "      <td>杭州其他</td>\n",
       "      <td>159</td>\n",
       "      <td>博茨瓦那</td>\n",
       "      <td>外港海关</td>\n",
       "      <td>江海运输</td>\n",
       "      <td>博茨瓦那</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128283</th>\n",
       "      <td>90318090.0</td>\n",
       "      <td>1380.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>690.000000</td>\n",
       "      <td>3302961512</td>\n",
       "      <td>宁波其他</td>\n",
       "      <td>173</td>\n",
       "      <td>博茨瓦那</td>\n",
       "      <td>外港海关</td>\n",
       "      <td>江海运输</td>\n",
       "      <td>博茨瓦那</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>90251990.0</td>\n",
       "      <td>520634.0</td>\n",
       "      <td>37400.0</td>\n",
       "      <td>13.920695</td>\n",
       "      <td>3301968132</td>\n",
       "      <td>杭州其他</td>\n",
       "      <td>173</td>\n",
       "      <td>美国</td>\n",
       "      <td>外港海关</td>\n",
       "      <td>江海运输</td>\n",
       "      <td>美国</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             _税号编码       _金额      _数量          _价格       _企业编码 _消费地进口or生产地出口  \\\n",
       "0       90308910.0     410.0      2.0   205.000000  3302961512          宁波其他   \n",
       "62625   90041000.0       5.0      2.0     2.500000  3301968800          杭州其他   \n",
       "106860  90318010.0    3100.0      1.0  3100.000000  3301965475          杭州其他   \n",
       "128283  90318090.0    1380.0      2.0   690.000000  3302961512          宁波其他   \n",
       "1518    90251990.0  520634.0  37400.0    13.920695  3301968132          杭州其他   \n",
       "\n",
       "       _企业性质 _起运国或目的国 _海关口岸 _运输方式  _中转国  _数量单位编码  label  city_label  \\\n",
       "0        173     博茨瓦那  外港海关  江海运输  博茨瓦那      1.0      0           0   \n",
       "62625    NaN     博茨瓦那  外港海关  江海运输  博茨瓦那      1.0      0           0   \n",
       "106860   159     博茨瓦那  外港海关  江海运输  博茨瓦那      1.0      0           0   \n",
       "128283   173     博茨瓦那  外港海关  江海运输  博茨瓦那      1.0      0           0   \n",
       "1518     173       美国  外港海关  江海运输    美国      1.0      0           0   \n",
       "\n",
       "        trans_label  target_label  custom_label  prod_label  \n",
       "0                 0             0             0           0  \n",
       "62625             0             0             0           0  \n",
       "106860            0             0             0           0  \n",
       "128283            0             0             0           0  \n",
       "1518              1             1             0           0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_label(df, col, new_name=None):\n",
    "    if new_name==None:\n",
    "        new_name=col\n",
    "    label=df[col].unique()\n",
    "    label=pd.Series(np.arange(len(label)), index=label)\n",
    "    label=pd.DataFrame({new_name+'_label': label})\n",
    "    df=pd.merge(df, label, left_on=col, right_index=True)\n",
    "    return df\n",
    "\n",
    "DATA['city']=DATA['_企业编码'].str[0:2]\n",
    "DATA=gen_label(DATA, 'city')\n",
    "del DATA['city']\n",
    "DATA=gen_label(DATA, '_中转国', 'trans')\n",
    "DATA=gen_label(DATA, '_起运国或目的国', 'target')\n",
    "DATA=gen_label(DATA, '_海关口岸', 'custom')\n",
    "DATA['prod']=DATA['_税号编码'].astype('str').str[0:2]\n",
    "DATA=gen_label(DATA, 'prod')\n",
    "del DATA['prod']\n",
    "DATA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后得到了用于训练的数据集，其中第一个变量为被预测对象，后面的变量为预测变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>city_label</th>\n",
       "      <th>trans_label</th>\n",
       "      <th>target_label</th>\n",
       "      <th>custom_label</th>\n",
       "      <th>prod_label</th>\n",
       "      <th>log_quantity</th>\n",
       "      <th>log_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168789</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.367418</td>\n",
       "      <td>-0.303586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288453</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.955962</td>\n",
       "      <td>-0.603737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524763</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.630860</td>\n",
       "      <td>-0.843629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125272</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>-0.738658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32142</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.988635</td>\n",
       "      <td>1.558253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239882</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.209978</td>\n",
       "      <td>-0.337810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47550</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>-1.015667</td>\n",
       "      <td>-0.350759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70579</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.350552</td>\n",
       "      <td>-0.009497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240683</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.606960</td>\n",
       "      <td>-0.183300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621630</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>-0.904548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>387593 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label  city_label  trans_label  target_label  custom_label  \\\n",
       "168789      0           7           65            65             0   \n",
       "288453      1           1           26            26             0   \n",
       "524763      2          16           28            28             3   \n",
       "125272      0           0           97            97             0   \n",
       "32142       2           2            1             1             0   \n",
       "...       ...         ...          ...           ...           ...   \n",
       "239882      2           3           18            18             0   \n",
       "47550       0           0            2             2             0   \n",
       "70579       0           1           22            22             0   \n",
       "240683      0           0          111           111             0   \n",
       "621630      0           1           46            46             3   \n",
       "\n",
       "        prod_label  log_quantity  log_price  \n",
       "168789           8      0.367418  -0.303586  \n",
       "288453           3      0.955962  -0.603737  \n",
       "524763           7     -0.630860  -0.843629  \n",
       "125272           1      0.001281  -0.738658  \n",
       "32142           18     -0.988635   1.558253  \n",
       "...            ...           ...        ...  \n",
       "239882          10     -1.209978  -0.337810  \n",
       "47550           33     -1.015667  -0.350759  \n",
       "70579           16     -0.350552  -0.009497  \n",
       "240683          20     -0.606960  -0.183300  \n",
       "621630          18      0.999982  -0.904548  \n",
       "\n",
       "[387593 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打乱顺序\n",
    "DATA['random_order']=np.random.random(DATA.shape[0])\n",
    "DATA=DATA.sort_values(['random_order'])\n",
    "DATA=DATA.drop('random_order', axis=1)\n",
    "# 数值型数据标准化\n",
    "DATA['log_quantity']=np.log(DATA['_数量'])\n",
    "DATA['log_price']=np.log(DATA['_价格'])\n",
    "DATA=DATA.dropna()\n",
    "DATA['log_quantity']=(DATA['log_quantity']-DATA['log_quantity'].mean())/DATA['log_quantity'].std()\n",
    "DATA['log_price']=(DATA['log_price']-DATA['log_price'].mean())/DATA['log_price'].std()\n",
    "# 挑选变量\n",
    "data=DATA[['label', 'city_label', 'trans_label', 'target_label', 'custom_label', 'prod_label', 'log_quantity', 'log_price']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们构建神经网络模型。注意到这里面city_label、trans_label、target_label、custom_label、prod_label这些变量都是分类变量，不能直接放到模型里面，在计量经济学中，我们通常使用虚拟变量的方法，然而虚拟变量的问题是会导致输入的维数太高，在这里我们使用嵌入的方法解决高维的问题。\n",
    "\n",
    "在PyTorch中，提供了一个嵌入层，其用法为：\n",
    "```python\n",
    "nn.Embedding(num_embeddings, embedding_dim)\n",
    "```\n",
    "其中num_embeddings为输入的维数，embedding_dim为输出的维数，比如我们可以使用如下代码进行降维："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 5])\n",
      "tensor([[-0.9579, -0.3355, -0.0845,  1.5591, -0.7833],\n",
      "        [ 0.5289, -0.4694,  0.0626,  0.5925, -1.0836],\n",
      "        [ 0.4640,  0.8360,  1.2166,  1.5552, -0.9560]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "num_city=len(data['city_label'].unique())\n",
    "embedding_layer=nn.Embedding(num_city, 5)\n",
    "embedded_vec=embedding_layer(torch.tensor([[1],[0],[15]]))\n",
    "print(embedded_vec.shape)\n",
    "print(embedded_vec.squeeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意上面的输出实际上是随机的，实际的嵌入结果会随着神经网络的学习而自动学习。实际上嵌入层就是一个使用前馈网络学习嵌入结果的过程。\n",
    "\n",
    "接下来我们可以进行神经网络的构建了。首先准备数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=data.iloc[:300000,0]\n",
    "X_train=data.iloc[:300000,1:]\n",
    "Y_test=data.iloc[300000:,0]\n",
    "X_test=data.iloc[300000:,1:]\n",
    "class trade_data(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X=X\n",
    "        self.Y=Y\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        d1=torch.tensor(self.X.iloc[i,0]).long()\n",
    "        d2=torch.tensor(self.X.iloc[i,1]).long()\n",
    "        d3=torch.tensor(self.X.iloc[i,2]).long()\n",
    "        d4=torch.tensor(self.X.iloc[i,3]).long()\n",
    "        d5=torch.tensor(self.X.iloc[i,4]).long()\n",
    "        x=torch.tensor(self.X.iloc[i,-2:]).float()\n",
    "        label=torch.tensor(self.Y.iloc[i]).long()\n",
    "        return d1, d2, d3, d4, d5, x, label\n",
    "tdata=trade_data(X_train, Y_train)\n",
    "dl=DataLoader(tdata, shuffle=True, batch_size=300, pin_memory=True, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次epoch，Loss=tensor(0.9741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第20次epoch，Loss=tensor(0.9896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第40次epoch，Loss=tensor(1.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第60次epoch，Loss=tensor(0.9825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第80次epoch，Loss=tensor(0.9726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第100次epoch，Loss=tensor(0.9645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第120次epoch，Loss=tensor(0.9894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第140次epoch，Loss=tensor(0.9603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第160次epoch，Loss=tensor(0.9959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第180次epoch，Loss=tensor(1.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第200次epoch，Loss=tensor(0.9572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第220次epoch，Loss=tensor(1.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第240次epoch，Loss=tensor(0.9777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第260次epoch，Loss=tensor(0.9315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第280次epoch，Loss=tensor(0.9852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第300次epoch，Loss=tensor(0.9263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第320次epoch，Loss=tensor(1.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第340次epoch，Loss=tensor(0.8902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第360次epoch，Loss=tensor(0.9609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第380次epoch，Loss=tensor(1.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第400次epoch，Loss=tensor(0.9067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第420次epoch，Loss=tensor(0.9487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第440次epoch，Loss=tensor(0.9705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第460次epoch，Loss=tensor(0.8846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第480次epoch，Loss=tensor(0.9823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第500次epoch，Loss=tensor(0.9284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第520次epoch，Loss=tensor(0.9706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第540次epoch，Loss=tensor(1.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第560次epoch，Loss=tensor(1.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第580次epoch，Loss=tensor(0.9747, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "num_city=data['city_label'].max()+1\n",
    "num_trans=data['trans_label'].max()+1\n",
    "num_target=data['target_label'].max()+1\n",
    "num_custom=data['custom_label'].max()+1\n",
    "num_prod=data['prod_label'].max()+1\n",
    "num_label=data['label'].max()+1\n",
    "\n",
    "class NET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NET, self).__init__()\n",
    "        # 第一层，embedding层\n",
    "        self.embedding1=nn.Embedding(num_city,5)\n",
    "        self.embedding2=nn.Embedding(num_trans,5)\n",
    "        self.embedding3=nn.Embedding(num_target,5)\n",
    "        self.embedding4=nn.Embedding(num_custom,5)\n",
    "        self.embedding5=nn.Embedding(num_prod,5)\n",
    "        # 第二层：线性组合，27个输入50个输出，带Dropout和BatchNormalization\n",
    "        self.layer2=nn.Sequential(nn.Linear(27, 50),nn.LeakyReLU(inplace=True),\n",
    "                                  torch.nn.BatchNorm1d(50),nn.Dropout(p=0.5))\n",
    "        # 第三层：50个输入40个输出\n",
    "        self.layer3=nn.Sequential(nn.Linear(50, 40),nn.LeakyReLU(inplace=True))\n",
    "        # 第四层：线性组合，40个输入5个输出，注意使用了Softmax激活函数\n",
    "        self.layer4=nn.Sequential(nn.Linear(40,num_label))\n",
    "    def forward(self,d1, d2, d3, d4, d5, x):\n",
    "        d1=self.embedding1(d1)\n",
    "        d2=self.embedding2(d2)\n",
    "        d3=self.embedding3(d3)\n",
    "        d4=self.embedding4(d4)\n",
    "        d5=self.embedding5(d5)\n",
    "        x=torch.cat([d1,d2,d3,d4,d5,x[:,-2:]], dim=1)\n",
    "        x=self.layer2(x)\n",
    "        x=self.layer3(x)\n",
    "        x=self.layer4(x)\n",
    "        return x\n",
    "\n",
    "model=NET().to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "# optimizer=optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer=optim.RMSprop(model.parameters(), lr=0.01)\n",
    "losses=[]\n",
    "for i in range(800):\n",
    "    for d1, d2, d3, d4, d5, x, y in dl:\n",
    "        # 将x计算预测值\n",
    "        y_pred=model(d1.to(device), d2.to(device), d2.to(device), d4.to(device), d5.to(device), x.to(device))\n",
    "        # 计算损失\n",
    "        loss=criterion(y_pred, y.to(device))\n",
    "        losses.append(loss.item())\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if i%20==0:\n",
    "        print(\"第%s次epoch，Loss=%s\" % (i, loss))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "\n",
    "i=np.arange(len(losses))+1\n",
    "plt.plot(i,np.array(losses))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
