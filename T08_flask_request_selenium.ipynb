{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫简介\n",
    "\n",
    "爬虫一般指从网站上获得需要的数据，这个过程其实是建造一个网站的逆向过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Flask建立简单网站\n",
    "\n",
    "所谓知己知彼，则百战不殆。既然爬虫主要是从网站上爬取数据，那么只有对建造网站的步骤一清二楚，才能在变幻多端的网站环境下，获取想要的数据。\n",
    "\n",
    "然而实际建立网站的过程是非常繁琐而复杂的，写HTML、CSS仅仅是一个网站的冰山一角，这其中至少涉及到网站架构设计、数据库设计、后台服务设计、前端设计等等，此外根据不同的架构设计，还有很多中间件、缓存数据库等等复杂的设计。\n",
    "\n",
    "不过，对于比较简单的网站，Python本身就有很多成熟的框架可以方便我们快速搭建一个简单的网站。这其中，Flask由于其比较精美的设计架构以及简单的模板等应用，是非常受欢迎的轻量级Web框架。在这里，我们不妨使用Flask搭建一个最简单的网站（因为在Jupyter里面，所以我把run()给注释掉了，如果需要执行，请直接执行html/web.py）：\n",
    "```python\n",
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "import random\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    with open(\"example4.html\") as f:\n",
    "        html=f.read()\n",
    "    return html\n",
    "\n",
    "@app.route('/dynamic')\n",
    "def dynamic():\n",
    "    with open(\"dynamic.html\") as f:\n",
    "        html=f.read()\n",
    "    return html\n",
    "\n",
    "@app.route('/dynamic_response')\n",
    "def dynamic_response():\n",
    "    return str(random.random())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #app.run()\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在执行以上命令之前，需要首先使用pip install flask安装Flask。\n",
    "\n",
    "在上面的程序中，@代表修饰器（decorator），是Python编程的一个高级特性，我们暂且不管。我们需要知道的仅仅是，通过@app.route函数，声明了一个路径，该路径即访问接下来定义的页面的路径。\n",
    "\n",
    "在@app.route下方，我们定义了几个函数，这几个函数的作用是返回一个字符串，这些字符串会通过网络传递给访问的浏览器。\n",
    "\n",
    "如果运行html/web.py，会提示：\n",
    ">  Running on http://127.0.0.1:5000/\n",
    "\n",
    "此时，如果在浏览器中输入以上网址，服务器就会执行index()函数，该函数会读取example4.html，并将其返回，从而我们在浏览器上就看到了example4.html。\n",
    "\n",
    "同理，如果访问 http://127.0.0.1:5000/dynamic ，服务器就会执行dynamic()函数，将dynamic.html的内容返回。\n",
    "\n",
    "而如果访问 http://127.0.0.1:5000/dynamic_response ，服务器会执行dynamic_response()函数，该函数生成一个随机数并返回给浏览器。\n",
    "\n",
    "以上就是一个简单的网站。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 发送请求\n",
    "\n",
    "知道了服务器端如何处理页面，那么现在的问题是，客户端怎样与服务器端通讯呢？\n",
    "\n",
    "这里就要介绍以下HTTP协议的概念了。\n",
    "\n",
    "HTTP协议是**Hyper Text Transfer Protocol**（**超文本传输协议**）的缩写，一种服务器/客户端范式的传输协议，我们访问网址（URL）时，都是以http:// 开头的，或者https:// 开头，代表的就是使用http协议。\n",
    "\n",
    "一个最简单的传输模型是，客户端通过URL地址向服务器端发送**请求**（**request**），服务器根据所请求的地址、头部信息（headers）做出**响应**（**response**）。\n",
    "\n",
    "这里有几个概念要特别注意：\n",
    "\n",
    "## 网址URL\n",
    "\n",
    "一个常见的网址通常具有如下形式：\n",
    "\n",
    "> http://econpaper.cn:8080/article/article.jsp?id=56987&userid=3455\n",
    "\n",
    "其中：\n",
    "\n",
    "* http:// 代表协议\n",
    "* econpaper.cn 代表域名\n",
    "* :8080 代表端口号，默认为80端口\n",
    "* /article/article.jsp 部分为请求的页面路径\n",
    "* ?id=56987&userid=3455 为需要传递给这个页面的参数\n",
    "\n",
    "## 请求（request）\n",
    "\n",
    "客户端向服务器端发送请求，要按照一定的格式，请求消息由以下三部分组成：\n",
    "\n",
    "* 请求行（request line）\n",
    "* 头部（header）\n",
    "* 请求数据\n",
    "\n",
    "![](pic/request.png)\n",
    "\n",
    "比如，以下是一个典型的请求头部：\n",
    "\n",
    "![](pic/request_headers.png)\n",
    "\n",
    "这些信息传递给服务器后，服务器根据这些信息进行处理。\n",
    "\n",
    "实际上，在我们刚刚运行的Flask中，在客户端可以看到每一次的请求。\n",
    "\n",
    "此外，上面从URL地址中已经看到，?id=56987&userid=3455 为需要传递给这个页面的参数，实际上，这是一个GET的请求方法。为了从客户端向服务器端传数据，以下两种方法是最常用的：\n",
    "\n",
    "* GET：像上面一样，请求的数据明文写在URL上\n",
    "* POST：数据包含在请求体中\n",
    "\n",
    "当然，两种方法也可以结合起来使用。不过最为常用的仍然是GET方法。\n",
    "\n",
    "## 响应（response）\n",
    "\n",
    "有了请求，就会有服务器的响应。同样，响应也有响应头和数据体。在响应头中，最重要的是响应的状态码以及内容类型（html文档或者图片、pdf等），常见的状态码比如：\n",
    "\n",
    "* 200 OK                        客户端请求成功\n",
    "* 400 Bad Request               客户端请求有语法错误，不能被服务器所理解\n",
    "* 401 Unauthorized              请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用 \n",
    "* 403 Forbidden                 服务器收到请求，但是拒绝提供服务\n",
    "* 404 Not Found                 请求资源不存在，eg：输入了错误的URL\n",
    "* 500 Internal Server Error     服务器发生不可预期的错误\n",
    "* 503 Server Unavailable        服务器当前不能处理客户端的请求，一段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python中发送请求\n",
    "\n",
    "在Python中，有很多工具可以帮助我们向服务器发送请求，包括但不限于：urllib、urllib2、urllib3、httplib2、requests等等等等。出于个人偏好原因，在这里我们以requests为例，介绍如何发送请求。\n",
    "\n",
    "为了使用requests，需要先进行安装：pip install requests\n",
    "\n",
    "使用时直接导入：\n",
    "```python\n",
    "import requests\n",
    "```\n",
    "\n",
    "接着，最简单的请求即GET请求：\n",
    "```python\n",
    "r=requests.get(url)\n",
    "```\n",
    "\n",
    "其中url为需要请求的地址。\n",
    "\n",
    "有时我们在发送请求时可能需要控制发送请求的头部，头部可以使用一个字典表示，比如：\n",
    "```python\n",
    "headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36 OPR/60.0.3255.70\",\n",
    "        \"Accept-Encoding\":\"gzip, deflate, lzma, sdch\",\n",
    "        \"Accept-Language\":\"en-US,en;q=0.8\",\n",
    "        \"Connection\":\"keep-alive\",\n",
    "        \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"}\n",
    "```\n",
    "\n",
    "以上字典定义了头部的内容，接下来只要在get()函数中加入该头部就可以了：\n",
    "```python\n",
    "r=requests.get(url, headers=hearders)\n",
    "```\n",
    "\n",
    "返回结果：\n",
    "\n",
    "* r.text 可以读取返回的HTML\n",
    "* r.json() 获得Json数据转换后的字典数据\n",
    "* r.encoding 记录了返回数据的字符编码\n",
    "* r.status_code 记录了响应的状态码\n",
    "\n",
    "如果需要发送POST请求，需要首先将请求的数据写成字典形式，在使用urllib包中的urlencode函数将其编码，比如：\n",
    "```python\n",
    "data=dict(name=\"Joe\", comment=\"A test comment\")\n",
    "r=requests.post(url, data=data)\n",
    "```\n",
    "\n",
    "如果需要设置自定义的cookie到服务器，可以使用：\n",
    "```python\n",
    "cookies = dict(cookies_are='working')\n",
    "r=requests.get(url, cookies=cookies)\n",
    "```\n",
    "\n",
    "如果我们希望设置最长的等待时间，可以使用超时选项：\n",
    "```python\n",
    "r=requests.get(url, timeout=10)\n",
    "```\n",
    "\n",
    "即设定等待时间超过10s则放弃连接。\n",
    "\n",
    "最后，如果需要使用代理，可以使用：\n",
    "```python\n",
    "proxies = {\n",
    "  'http': 'http://user@password@10.10.1.10:3128',\n",
    "  'https': 'http://10.10.1.10:1080',\n",
    "}\n",
    "r=requests.get(url, proxies=proxies)\n",
    "```\n",
    "\n",
    "比如，使用一个获取微博热搜的API："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 200,\n",
       " 'msg': 'success',\n",
       " 'data': {'hot_word': '这是今日中国的声音赵英明回家丈夫儿子捧花迎接全球新冠肺炎超过110万例美国疫情川藏线突发雪崩武警紧急救援王智的后盾新闻联播法国人远程视频葬礼30多地推出消费券平如美棠作者饶平如去世罗永浩回应收到限制消费令外卖小哥下车默哀流泪河南民众列出哀悼字样队形全球天空一个月航班变化图山火中牺牲的18名扑火队员的最后一刻武汉外卖骑手老计没活了特朗普解雇美国国家情报督察长西昌森林大火的三天两夜一觉醒来发现屋门被1米深积雪挡住91岁抗美援朝老兵为战友立碑法国600名军人感染新冠病毒小麦稻谷库存可够全国吃一年维和战士与国内同步悼念久久不愿离开的武汉市民无名烈士墓终于刻上姓名返京留学生两次出现症状未报告默哀护士援鄂47天归来儿子已认不出她伊朗大使馆引用左传发文中国抗疫图卷南京路步行街市民停下脚步哀悼悼念援华医生白求恩与柯棣华西安1.4万辆出租车同时鸣笛钟南山在办公室静立默哀特朗普集团已在加美两国裁员1000多人这是今天10点的武汉鸣笛崔娃连线比尔盖茨宁南万人迎英雄遗体回乡苹果向黑客发7.5万美元奖励刘强东卸任京东法定代表人多国驻华使馆发微博哀悼小学生隔着墙向国旗敬礼部队官兵鸣枪悼念逝者范佩西和儿子带球神同步有五星红旗的地方就有对同胞的悼念湖北50万吨积压蔬菜清零俄罗斯驻华大使中文悼念逝者甘肃天水四月飞雪东京奥运会资格赛截止日确定',\n",
       "  'hot_word_num': '84302146540833182233177633112533064033014232984628447827617126517324685321628019868917395117359317307417182517158517095916923816047015384114848514346913974911817811182111144810958210710399179840758277881785804677815377404736777094763461601415651355515550165486152598523524684046121'},\n",
       " 'author': {'name': 'Alone88',\n",
       "  'desc': '由Alone88提供的免费API 服务，官方文档：www.alapi.cn'}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=\"https://v1.alapi.cn/api/new/wbtop?num=10\"\n",
    "import requests\n",
    "r=requests.get(url)\n",
    "data=r.json()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大多数接口都需要用一个token进行身份验证，一般会根据token进行一定的数量限制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码实现了从链家上爬去房价数据。值得注意的是，如果没有设定headers，可能会爬不下来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['107102307012', '2室1厅', '53.35平米', '松江老城', '163万', '松江老城/2室1厅/53.35平米/南/毛坯']\n",
      "['107102121073', '2室2厅', '89.31平米', '莘闵别墅', '285万', '莘闵别墅/2室2厅/89.31平米/南/精装']\n",
      "['107102290984', '1室2厅', '59.7平米', '松江大学城', '252万', '松江大学城/1室2厅/59.7平米/南/精装']\n",
      "['107102307737', '1室1厅', '60.25平米', '泗泾', '156万', '泗泾/1室1厅/60.25平米/南/简装']\n",
      "['107102099196', '2室2厅', '86.2平米', '莘闵别墅', '268万', '莘闵别墅/2室2厅/86.2平米/南/精装']\n",
      "['107102052326', '2室2厅', '83.13平米', '新桥', '245万', '新桥/2室2厅/83.13平米/南/精装']\n",
      "['107102252934', '2室2厅', '89.5平米', '松江大学城', '330万', '松江大学城/2室2厅/89.5平米/东/精装']\n",
      "['107102304054', '3室2厅', '92.13平米', '九亭', '355万', '九亭/3室2厅/92.13平米/南/毛坯']\n",
      "['107102234290', '2室2厅', '71.94平米', '松江大学城', '322万', '松江大学城/2室2厅/71.94平米/南/精装']\n",
      "['107101778575', '2室1厅', '70.8平米', '泗泾', '202万', '泗泾/2室1厅/70.8平米/南/简装']\n",
      "['107101714497', '2室1厅', '72.81平米', '泗泾', '218.8万', '泗泾/2室1厅/72.81平米/南 北/简装']\n",
      "['107101974503', '2室2厅', '91.52平米', '九亭', '359万', '九亭/2室2厅/91.52平米/南/精装']\n",
      "['107102314909', '2室2厅', '94.44平米', '九亭', '388万', '九亭/2室2厅/94.44平米/南/精装']\n",
      "['107102248549', '1室1厅', '56.59平米', '泗泾', '168万', '泗泾/1室1厅/56.59平米/南/精装']\n",
      "['107102248087', '1室1厅', '51.29平米', '泗泾', '126万', '泗泾/1室1厅/51.29平米/南/毛坯']\n",
      "['107102266723', '3室2厅', '115.98平米', '九亭', '550万', '九亭/3室2厅/115.98平米/南/精装']\n",
      "['107101215962', '1室1厅', '52.92平米', '泗泾', '158万', '泗泾/1室1厅/52.92平米/南/毛坯']\n",
      "['107101995350', '2室1厅', '45.36平米', '松江大学城', '235万', '松江大学城/2室1厅/45.36平米/南/精装']\n",
      "['107101355197', '3室2厅', '175.02平米', '莘闵别墅', '685万', '莘闵别墅/3室2厅/175.02平米/南/精装']\n",
      "['107102315818', '3室1厅', '72.11平米', '松江老城', '228万', '松江老城/3室1厅/72.11平米/南/精装']\n",
      "['107102320848', '3室2厅', '89.48平米', '九亭', '405万', '九亭/3室2厅/89.48平米/南 北/精装']\n",
      "['107102250821', '3室2厅', '92.08平米', '松江大学城', '360万', '松江大学城/3室2厅/92.08平米/南/精装']\n",
      "['107102283498', '3室2厅', '100.28平米', '九亭', '445万', '九亭/3室2厅/100.28平米/南/简装']\n",
      "['107102284011', '2室2厅', '93.3平米', '九亭', '389万', '九亭/2室2厅/93.3平米/南 北/简装']\n",
      "['107101170742', '2室2厅', '90.91平米', '九亭', '353万', '九亭/2室2厅/90.91平米/南/精装']\n",
      "['107102239879', '3室2厅', '115.28平米', '松江老城', '325万', '松江老城/3室2厅/115.28平米/南/精装']\n",
      "['107102272419', '2室2厅', '79.34平米', '松江老城', '265万', '松江老城/2室2厅/79.34平米/南/精装']\n",
      "['107102309535', '3室1厅', '93.49平米', '泗泾', '312万', '泗泾/3室1厅/93.49平米/南/精装']\n",
      "['107100424961', '2室1厅', '69.27平米', '泗泾', '203万', '泗泾/2室1厅/69.27平米/南/毛坯']\n",
      "['107102243246', '3室1厅', '90.51平米', '新桥', '275万', '新桥/3室1厅/90.51平米/南/毛坯']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36 OPR/60.0.3255.70\",\n",
    "        \"Accept-Encoding\":\"gzip, deflate, lzma, sdch\",\n",
    "        \"Accept-Language\":\"en-US,en;q=0.8\",\n",
    "        \"Connection\":\"keep-alive\",\n",
    "        \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"}\n",
    "url='https://sh.lianjia.com/ershoufang/songjiang/'\n",
    "r=requests.get(url, headers=headers)\n",
    "html=r.text\n",
    "bs=BeautifulSoup(html,\"html.parser\")\n",
    "iterms=bs.find_all(name='div',attrs={\"class\":\"item\"})\n",
    "info=[]\n",
    "for it in iterms:\n",
    "    house_id=it.get(\"data-houseid\")\n",
    "    prop=it.find(name='div',attrs={\"class\":\"info\"}).text\n",
    "    shi_ting=re.search(r\"\\d室\\d厅\",prop).group()\n",
    "    area=re.search(r\"\\d*(\\.\\d*)?平米\",prop).group()\n",
    "    location=prop[:prop.find('/')]\n",
    "    price=it.find(name='div',attrs={\"class\":\"price\"}).text\n",
    "    print([house_id,shi_ting,area,location,price,prop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们使用一个更加综合的例子：百度新闻的爬取："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "### funcs.py\n",
    "#!/usr/bin/python3\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "## write log file.\n",
    "def writeLog(content):\n",
    "    try:\n",
    "        logfile=open('News.log','a')\n",
    "        ltime=time.localtime()\n",
    "        trstr=lambda s:('0'+str(s))[-2:]\n",
    "        logfile.write(str(ltime.tm_year)+'-'+trstr(ltime.tm_mon)+'-'+trstr(ltime.tm_mday)+' '\n",
    "                      +trstr(ltime.tm_hour)+':'+trstr(ltime.tm_min)+':'+trstr(ltime.tm_sec)+'-->'+\n",
    "                      content+'\\n')\n",
    "        logfile.close()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "## get htmltext\n",
    "def getPage(url,trytimes=10,tot=10):\n",
    "    trytime=1\n",
    "    headers={\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36 OPR/60.0.3255.70\",\n",
    "        \"Accept-Encoding\":\"gzip, deflate, lzma, sdch\",\n",
    "        \"Accept-Language\":\"en-US,en;q=0.8\",\n",
    "        \"Connection\":\"keep-alive\",\n",
    "        \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"\n",
    "        }\n",
    "    while True:\n",
    "        try:\n",
    "            r=requests.get(url, headers=headers, timeout=tot, stream=False)\n",
    "            htmlText=r.text\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if trytime>=trytimes:\n",
    "                print(e)\n",
    "                writeLog('Error happened in reading '+url+':'+str(e)+'.')\n",
    "                return ''\n",
    "            else:\n",
    "                trytime=trytime+1\n",
    "                time.sleep(10)\n",
    "    return htmlText\n",
    "# get current date\n",
    "def getDate():\n",
    "    ts=time.localtime()\n",
    "    trstr=lambda s:('0'+str(s))[-2:]\n",
    "    return '-'.join((str(ts.tm_year),trstr(ts.tm_mon),trstr(ts.tm_mday)))+' '+':'.join((trstr(ts.tm_hour),trstr(ts.tm_min)))\n",
    "# get the longest content\n",
    "def getLongText(url):\n",
    "    html=getPage(url,trytimes=2)\n",
    "    bs=BeautifulSoup(html,\"html.parser\")\n",
    "    content=\"\"\n",
    "    ## delete all scripts\n",
    "    while bs.script!=None:\n",
    "        bs.script.extract()\n",
    "    while bs.style!=None:\n",
    "        bs.style.extract()\n",
    "    ## get content, method 1\n",
    "    divs=bs.find_all('div')\n",
    "    text=\"\"\n",
    "    minlen=5\n",
    "    for d in divs:\n",
    "        # if d.find_all(\"div\")==[]:\n",
    "        text=d.get_text().strip('\\n\\r ')\n",
    "        if text is not None and text not in content:\n",
    "            if text.count('。')>=1 and len(text)>minlen:\n",
    "                content=content+text\n",
    "    ## get content, method 2\n",
    "    ps=bs.find_all('p')\n",
    "    content2=\"\"\n",
    "    if ps!=[]:\n",
    "        for p in ps:\n",
    "            text=p.get_text()\n",
    "            if text is not None and text not in content2:\n",
    "                if text.count('。')>=1 and len(text)>minlen:\n",
    "                    content2=content2+text\n",
    "    ## compare\n",
    "    if len(content)<=len(content2):\n",
    "        content=content2\n",
    "    ## filtering\n",
    "    content=content.split(\"\\n\")\n",
    "    longtext=\"\"\n",
    "    for c in content:\n",
    "        if len(c)>=10 and (\"。\" in c or \"！\" in c or \"？\" in c):\n",
    "            longtext=longtext+\"\\n\"+c\n",
    "    return longtext.replace(\"百度首页登录个人中心帐号设置意见反馈退出\",'')\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "### News.py\n",
    "#!/usr/bin/python3\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "from funs import writeLog\n",
    "from funs import getPage\n",
    "from funs import getDate\n",
    "from funs import getLongText\n",
    "import time\n",
    "import sys\n",
    "## get page\n",
    "def getSubPage(a,longtxt=1):\n",
    "    ## string handler\n",
    "    treatStr=lambda s: s.replace('\\'','\\'\\'')\n",
    "    ## nega words\n",
    "    negaurlwords=('letv','youku','tudou','iqiyi','jfinfo','gmw.cn')\n",
    "    negatitlewords=(\"(图)\",\"（图）\",\"车展\",\"重磅\",\"健康问答\",\"养生\",\"(组图)\",\"（组图）\",\"专治\")\n",
    "    ## url\n",
    "    url=a.get(\"href\")\n",
    "    title=a.get_text().strip(\"\\r\\n \")\n",
    "    if title!=None and url!=None:\n",
    "        if url.find('http://')>=0 and len(title)>4:\n",
    "            ## exist?\n",
    "            sql=\"SELECT count(*) FROM news WHERE url=\\'\"+treatStr(url)+\"\\' LIMIT 1\"\n",
    "            cur.execute(sql)\n",
    "            exist=cur.fetchall()[0][0]\n",
    "            if exist==0:\n",
    "                skip=0\n",
    "                for w in negaurlwords:\n",
    "                    if url.find(w)>=0:\n",
    "                        skip=1\n",
    "                        break\n",
    "                if skip==0:\n",
    "                    for w in negatitlewords:\n",
    "                        if title.find(w)>=0:\n",
    "                            skip=1\n",
    "                            break\n",
    "                if skip==0:\n",
    "                    content=getLongText(url)\n",
    "                    if content!=None and len(content)>=80:\n",
    "                        return (getDate(),title,url,content)\n",
    "    return None\n",
    "## init database\n",
    "conn=sqlite3.connect('News.db')\n",
    "cur=conn.cursor()\n",
    "createTable=(\"CREATE TABLE IF NOT EXISTS news (\"\n",
    "             \"id INTEGER PRIMARY KEY AUTOINCREMENT, \"\n",
    "             \"date DATE, \"\n",
    "             \"title VARCHAR(500),\"\n",
    "             \"url VARCHAR(800),\"\n",
    "             \"content LONGTEXT);\")\n",
    "conn.execute(createTable)\n",
    "## homepage\n",
    "html=getPage(\"http://news.baidu.com\")\n",
    "bs=BeautifulSoup(html,\"html.parser\")\n",
    "Divs=bs.find_all('div')\n",
    "for d in Divs:\n",
    "    if d.get(\"id\")==\"pane-news\":\n",
    "        links=d.find_all(\"a\")\n",
    "        for a in links:\n",
    "            print(a)\n",
    "            content=getSubPage(a,longtxt=2)\n",
    "            if content!=None:\n",
    "                ## write into database\n",
    "                column=('date','title','url','content')\n",
    "                sql=\"INSERT INTO news (\"+','.join(column)+\") VALUES(\"+','.join('?'*len(column))+\");\"\n",
    "                cur.execute(sql,content)\n",
    "                conn.commit()\n",
    "## sub page\n",
    "Times=int(sys.argv[1])\n",
    "HomePages=((\"http://finance.baidu.com\",2),\n",
    "    (\"http://guonei.news.baidu.com\",3),\n",
    "    (\"http://guoji.news.baidu.com\",3),\n",
    "    (\"http://shehui.news.baidu.com\",5),\n",
    "    (\"http://mil.news.baidu.com\",6),\n",
    "    (\"http://tech.baidu.com\",7),\n",
    "    (\"http://internet.baidu.com\",7))\n",
    "for page in HomePages:\n",
    "    url=page[0]\n",
    "    times=page[1]\n",
    "    if Times%times==0:\n",
    "        html=getPage(url)\n",
    "        bs=BeautifulSoup(html,\"html.parser\")\n",
    "        links=bs.find_all(\"a\")\n",
    "        for a in links:\n",
    "            content=getSubPage(a)\n",
    "            if content!=None:\n",
    "                ## write into database\n",
    "                column=('date','title','url','content')\n",
    "                sql=\"INSERT INTO news (\"+','.join(column)+\") VALUES(\"+','.join('?'*len(column))+\");\"\n",
    "                cur.execute(sql,content)\n",
    "                conn.commit()\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用selenium爬取动态页面\n",
    "\n",
    "现在，很多网页通过使用JavaScript的Ajax技术实现了网页内容的动态改变。\n",
    "\n",
    "比如，一个很简单的例子，如果访问我们之前建立的web.py中的网址： http://127.0.0.1:5000/dynamic 上面会有一个按钮，每按一次，该页面就会向服务器再发一个请求，服务器收到请求后会随机产生一个数字，并返回。所以每一次点击按钮，网页内容都会动态改变。\n",
    "\n",
    "在碰到动态页面时，一种方法是精通JavaScript，并使用浏览器跟踪浏览器行为，分析JavaScript脚本，进而使用以上的方法模拟浏览器请求。但是这种方法非常复杂，很多时候JavaScript可能会复杂到一定程度，使得分析异常困难。\n",
    "\n",
    "而另外一种方法，即使用selenium直接调用浏览器，浏览器自动执行JavaScript，然后调用浏览器的HTML即可。这种方法非常方便，但是速度异常之慢。\n",
    "\n",
    "为了实现这一方法，我们首先要安装selenium：pip install selenium\n",
    "\n",
    "除此之外，还要安装浏览器的支持。几种常见的浏览器支持插件的下载地址：\n",
    "\n",
    "* firefox：https://github.com/mozilla/geckodriver/releases （对于Windows，下载后放到C:\\Windows下；对于Linux/Mac，放到/usr/local/bin下。）\n",
    "* chrome： https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "* safari： https://webkit.org/blog/6900/webdriver-support-in-safari-10/\n",
    "* edge：   https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/\n",
    "\n",
    "安装完成之后，使用：\n",
    "```python\n",
    "from selenium import webdriver\n",
    "driver=webdriver.Firefox()\n",
    "## 做一些事情\n",
    "driver.close()\n",
    "```\n",
    "\n",
    "就可以打开网络驱动器，此时我们可以看到一个Firefox窗口被打开。同样注意的是，用完之后记得关掉。\n",
    "\n",
    "比如，淘宝的网页上，价格数据是动态加载的，我们可以使用如下代码找到价格："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: use options instead of firefox_options\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "及木家具 北欧简约  榉木 白橡 黑胡桃 长方形抽屉实木茶几CJ030 5598.00 - 8336.00\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "url=\"https://detail.tmall.com/item.htm?id=524649065000&sku_properties=29112:97926\"\n",
    "### 如果不需要Firefox窗口打开：\n",
    "from selenium.webdriver import FirefoxOptions\n",
    "opts = FirefoxOptions()\n",
    "opts.add_argument(\"--headless\")\n",
    "driver = webdriver.Firefox(firefox_options=opts)\n",
    "###\n",
    "## 如果需要Firefox窗口打开，直接：driver=webdriver.Firefox()\n",
    "driver.get(url)\n",
    "html=driver.page_source\n",
    "driver.close()\n",
    "## 产品名\n",
    "bs=BeautifulSoup(html,\"html.parser\")\n",
    "meta=bs.find_all(\"meta\")\n",
    "for m in meta:\n",
    "    if m.get(\"name\")==\"keywords\":\n",
    "        title=m.get(\"content\")\n",
    "## 价格\n",
    "pattern=re.compile(\"TShop.Setup\\(.+?\\);\")\n",
    "res=pattern.finditer(html.replace('\\r', '').replace('\\n', ''))\n",
    "for r in res:\n",
    "    json_text=r.group()\n",
    "    json_text=json_text[json_text.find('{'):-2]\n",
    "\n",
    "info=json.loads(json_text)\n",
    "print(title,info['detail']['defaultItemPrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selenium另外一个更常用的功能是自动填充、点击。为了实现这一目的，首先需要能够找到相应的按钮、输入框等，有如下方法可以使用：\n",
    "\n",
    "* find_element_by_name\n",
    "* find_element_by_id\n",
    "* find_element_by_xpath\n",
    "* find_element_by_link_text\n",
    "* find_element_by_partial_link_text\n",
    "* find_element_by_tag_name\n",
    "* find_element_by_class_name\n",
    "* find_element_by_css_selector\n",
    "\n",
    "理解以上函数需要一些HTML、CSS、JavaScript的背景知识，我们在此不再详细讨论。在此，展示一个简单的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: use options instead of firefox_options\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python爬虫开发教程,一看就懂! /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd9TZMvi2W3uAoYXaSIL_XJxkmEjFJuzFhOMqFf5rWu9KiKiMYewYZFTXuKBoZPNv8HiUdDE87OdQR3taBxVRbMNap09NcnDqL_ZqTWtN8_3bEX4Mnou1_58DiUZ4Zs0oLdwKEQ-5IiIeFBPCsvYpxvReqe7NC02gBgBpyHwrqR9DX0OK9DLfgmPA..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n",
      "python爬虫入门:什么是爬虫,怎么玩爬虫? /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd9tmfVEOQRmQM8w59c5ta3Z-dnBbN0YBlnJTVziiaXiFQp9y1TIlT9jueJ_ZTqmNl2HWkGux-zNmj5_7-JnTIGGOZSOx6FqshaCRPyK0ARfwx1KYNcZCymjelWhbTlu1EV2hgNWXW4j1gRS3p9Wv9czbWXknrN8M8Ni3V3QwCfarA5eBgmN3LoYQ..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n",
      "python爬虫从入门,10分钟就够了 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd9ev3iuVDTLwRFCI70JCdjk75kx3X4KW4msAtUZDWQmHReeJghD-GRWQTIamCIoE2XuuHaYH0KPyKRV0dszyoD1l1aJoJ_5BO2fZ9SY9HZ33FzheLmjJGV-6twxw5I0-WX3_QdQ10y3OYLXXxGnLzzq3LmdnhqgV-MvY6-KvH0bify08OLTBhW0A..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n",
      "Python爬虫入门并不难,甚至入门也很简单 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd9KOzIpsWiw3TCY2wZQcmEagtJGvecktwbhbVNUi65k06dpfDTQBzl__BoMGflJ_A7e-aBWBE-LqQzGVcxzpt-jobnK1HwUrwSLbdHZlFHBMDw2EklJnJHIupmLYbMF-Lzrp5uQN3177lX4tCh7tZ4JScbmVgj2jMFFLESiyhUIZ_yPfCoem7FzA..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n",
      "Python爬虫 | 一条高效的学习路径 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd97IQjFXFScRkTFnPCBh7P7ykrc-la1jYfSnjfJekC-I7V9yzJD7PeVA35uJbMOmqS8UnkM-exwLmQHQcF7ETLu-ii7oFq3oqeN3oQGxGMUt6xVUNQ7HCbWqAwl6Y5Z-WNOwgOjFqfCoq1jfVtkNUy_-Ccf3KjZIxCaqRWd45qAXtOJBjQH7pCxQ..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n",
      "Python 爬虫“学前班”!学会免踩坑! /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd9eo4z5bQUlsyn6LOuG5zxNmWh9xd-xmGUkPaWZ2iKoKLyL9s6eiEqZMnYiDU30OmevuyLSb0d89ut0PWmF_LZnxtVYDD_W39vta8BSDZaU-tcnGcULHYNNQAQ1BbBL0jFn71nJFJOgo6Slt_FlC72IQTvQoW1D3pDEAjyf0lR_f15Nu7CnoSqqg..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n",
      "Python 爬虫介绍 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd9FUyVtdTcKnViH0q9cRyB5iSsy1sAcotWO_-iYEpQF_1FRrOEO7GApFyHbKFLtCmLNbWuTuWDD2CoKnrN62kWNggnD3c3PCHWCLCSUm3zYB-uhzKISaoPVNC_RvbFgxlVqLj8mAOS1uJPXiRGgt8p2Ct59T2LIwzjAdXmz54h0iECYioxHkzTmA..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n",
      "Python爬虫能做什么 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd9UI7A7_DGvXgYm57CqymxwA4j3lfUN0_D3axVvR2xKLWV4DIe_UBn4aPy1U21Dt0LOs4yVvY9sbQ0v8snW8HGUDsYiKktpj5oWodf52YI7r3dOK2taaf5Vt19YskLlwtS7ELWZ32AfYk2lnFYpU5ZUWDNVFL8AgvasYaNrdgt5A0SYTqT4eyLmA..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n",
      "在知乎上学 Python - 爬虫篇 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd9_StP4p0bgbTSWEM0yrT4Jby9SA3lfzjXW72ljS_FwuEXsCbwS_mEIEDQAstjwcHxInAsYnSo83ZJyGrKrcYLFJJuTBfxGIWcKM3CRMttoK7QNApvYhCKp2hFm9WtMiI2dARPTcq-gE4GMrg54vT1KfwiQ4H6hCd1pIezF4pp66MC2Qmj7RgGwQ..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n",
      "Python爬虫知识点梳理 /link?url=dn9a_-gY295K0Rci_xozVXfdMkSQTLW6cwJThYulHEtVjXrGTiVgS9YUkBtB9BdW1tUk0l8LpqAdBAuqjeojM1qXa8Fplpd9cLEbGMnQqtR7vK7xzkQzvmf3XltvCWTBJv0bnwnLu2KM-kVHn_QEJiOpx4eszC-mU0zvmC_fXPhNIG4UaNWVtq5AaJLcdQ2HNLmpr32wqRN0m23-cFh82wY1ywMn3xUN7lkDgjjjjfWn_LNTcdMCy0fWUZLubeWXT3ZQHn-_iPyE8RsmObDbtQ..&type=2&query=Python%E7%88%AC%E8%99%AB&token=3CAC3BA5CE2B17EBD2D770A8DEC36A0DD2038FB05E88BEA6\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "url=\"http://weixin.sogou.com\"\n",
    "### 如果不需要Firefox窗口打开：\n",
    "from selenium.webdriver import FirefoxOptions\n",
    "opts = FirefoxOptions()\n",
    "opts.add_argument(\"--headless\")\n",
    "driver = webdriver.Firefox(firefox_options=opts)\n",
    "###\n",
    "## 如果需要Firefox窗口打开，直接：driver=webdriver.Firefox()\n",
    "driver.get(url)\n",
    "driver.find_element_by_id('query').send_keys(\"Python爬虫\") #模仿填写搜索内容\n",
    "driver.find_element_by_class_name(\"swz\").click() #模仿点击搜索按钮\n",
    "html=driver.page_source\n",
    "driver.close()\n",
    "bs=BeautifulSoup(html,\"html.parser\")\n",
    "iterms=bs.find_all(name='h3')\n",
    "for it in iterms:\n",
    "    print(it.a.text,it.a['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 后记：关于爬虫的一些补充\n",
    "\n",
    "关于爬虫，这里有一些需要补充的话。\n",
    "\n",
    "首先是关于爬虫的道德问题。实际上爬虫一直处在一个灰色地带：违法或者不违法，道德或者不道德。在此一些原则希望与大家分享：\n",
    "\n",
    "* 按照我国法律的相关规定，如果网页内容、API接口使用了加密，破解加密是违法行为。\n",
    "* 如果将爬取的数据商用，法律风险非常大\n",
    "* 如果网站提供了接口（比如豆瓣API），不使用接口而直接爬取网页也是不道德的。\n",
    "* 尽量爬取的速率不要太高，不要给他人的服务器造成太大负担。\n",
    "\n",
    "当然，还有一些技术出于时间所限、个人能力所限，我是没有讲到的，这其中很多都是与反爬虫有关。比如：\n",
    "\n",
    "* 如何破解验证码\n",
    "* 如何破解短信验证码\n",
    "* 消息队列\n",
    "* 并行爬虫\n",
    "\n",
    "等等。如果有需要，可以自行学习。\n",
    "\n",
    "最后，爬虫和反爬一直是相互伴生的两个技术，我们这里提供一些常用的反爬思路：\n",
    "\n",
    "* 如果request到的内容与自己使用浏览器看到的内容不符：\n",
    "    - 首先查看请求头部，最极端情况下，完全复制浏览器的请求头部，看看能不能得到相同的反应。\n",
    "    - 是否需要cookies登录？\n",
    "    - 是否有重定向？\n",
    "    - 仔细查看request得到的html，里面可能有玄机\n",
    "    - 实在不行，selenium\n",
    "* 如果request中的内容没有自己想要的信息：\n",
    "    - 是否是动态加载的？\n",
    "        * 对比request得到的HTML和浏览器的HTML是否不同，如果是不同的，可能时动态加载的\n",
    "        * 仔细分析JavaScript，或者浏览器发送的每一次请求\n",
    "        * 尝试直接发送动态请求\n",
    "    - 内容是否是加密的？\n",
    "        * 仔细分析加密方法\n",
    "* 爬取几个页面后被禁止访问：\n",
    "    - 可能爬取的频率太频繁\n",
    "    - 间隔时间小一点\n",
    "    - 使用代理池（比如该项目： https://github.com/SpiderClub/haipproxy ）\n",
    "    - 使用ADSL\n",
    "* 需要登录：\n",
    "    - 不频繁的登录通常使用cookies，模拟一次登录之后获得cookies即可\n",
    "    - 频繁的登录或者验证码，可能需要结合图像识别之类的方法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
