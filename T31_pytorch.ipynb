{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch入门\n",
    "\n",
    "随着神经网络，特别是深度神经网络的出现，如何使用计算机高效的完成参数量巨大的神经网络的训练就成了一个难题。为此，出现了诸如TensorFlow、PyTorch等深度学习框架。在工业界中，TensorFlow出现的早而且应用较多，而PyTorch虽然出现的晚，不过由于其与Python结合更加紧密，且具有动态图等一系列优良特性，我们在这里以PyTorch为例讲解深度学习框架的使用。\n",
    "\n",
    "在这一节中，我们首先学习PyTorch的一些基础操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor的使用\n",
    "\n",
    "在机器学习中，Tensor是向量、矩阵的一个自然推广，即一个任意维的数组，比如：标量、向量（1维数组）、矩阵（2维数组）以及更高维的数组都可以看作是Tensor。\n",
    "\n",
    "## Tensor的创建\n",
    "\n",
    "在PyTorch中提供了Tensor数据类型，其使用方法与NumPy类似，但是也有一些差别。比如，为了创建一个Tensor，使用torch中的 **tensor()** 函数就可以了，这与NumPy里面的numpy.array()函数用法是一样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.tensor([1.,2,3,4])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以通过NumPy的array创建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0117, 0.1346, 0.4583, 0.1797, 0.0012], dtype=torch.float64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "b=np.random.random(5)\n",
    "b_torch=torch.from_numpy(b)\n",
    "b_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者使用torch.tensor()函数，两者的区别是使用b=torch.from_numpy(a)函数引入时，a和b是共享内存的；而如果使用b=torch.tensor(a)函数时，如果a的类型不是Float32，则会新建，此时可能不共享内存。\n",
    "\n",
    "反过来，也可以使用Tensor的numpy()函数转化为NumPy的array，此时也是共享内存的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b= [ 1.  2.  3.  4.]\n",
      "b= [ 10.   2.   3.   4.]\n",
      "a= tensor([10.,  2.,  3.,  4.])\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([1.,2,3,4])\n",
    "b=a.numpy()\n",
    "print(\"b=\",b)\n",
    "b[0]=10\n",
    "print(\"b=\",b)\n",
    "print(\"a=\",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者，使用 **Tensor()** 函数（*注意大小写！*）需要创建的Tensor的维数，从而创建一个未初始化的tensor："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.5122e-37, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 3.6431e-37, 0.0000e+00],\n",
       "        [3.6431e-37, 0.0000e+00, 3.6431e-37],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [1.8395e+25, 6.1963e-04, 1.3119e-08],\n",
       "        [2.0922e+23, 2.1344e-07, 8.2640e+20],\n",
       "        [1.7298e-04, 1.3542e-05, 3.6431e-37],\n",
       "        [0.0000e+00, 3.6431e-37, 0.0000e+00],\n",
       "        [3.6431e-37, 0.0000e+00, 1.4013e-45],\n",
       "        [0.0000e+00, 1.8788e+31, 7.9303e+34]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=torch.Tensor(10,3)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意以上新创建的Tensor的值是凌乱的，因为没有对其进行初始化，所以里面的值是完全无意义的。\n",
    "\n",
    "几乎所有的神经网络工具箱都需要支持GPU计算，而GPU计算最常用的工具是NVIDIA公司的CUDA工具箱。如果你有支持CUDA的显卡，并且配置好了CUDA，可以在创建时通过 **cuda()** 函数将Tensor放在显卡的内存（显存）中，而非内存中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac=torch.tensor([1.,2,3,4]).cuda()\n",
    "ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的结果除了展示了Tensor的取值之外，还有一个 *device='cuda:0'* ，显示了这个Tensor所出的CUDA设备编号为0。如果有不止一块GPU，可以在括号中加入数字（从0开始计数），指定将其放在哪一块GPU上。\n",
    "\n",
    "我们可以使用cup()函数和cuda()函数在两者之间转换："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac_cpu=ac.cpu()\n",
    "ac_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里值得一提的是Tensor的数据类型。在PyTorch中，Tensor有如下数据类型可以使用：\n",
    "* 浮点类型\n",
    "  * torch.FloatTensor（torch.cuda.FloatTensor）: 32位浮点型\n",
    "  * torch.DoubleTensor（torch.cuda.DoubleTensor）: 64位浮点型\n",
    "  * （torch.cuda.HalfTensor）: 16位浮点型\n",
    "* 整型\n",
    "  * torch.IntTensor（torch.cuda.IntTensor）: 32位整型\n",
    "  * torch.LongTensor（torch.cuda.LongTensor）: 64位整型\n",
    "  * torch.ShortTensor（torch.cuda.ShortTensor）: 16位整型\n",
    "  * torch.ByteTensor（torch.cuda.ByteTensor）：8位无符号整型\n",
    "  * torch.CharTensor（torch.cuda.CharTensor）：8位有符号整型\n",
    "  \n",
    "其中括号里面表示的是在CUDA中的类型。\n",
    "\n",
    "不管是在CPU上还是GPU上，位数更小的类型总是需要更少的内存（显存），并且具有更高的计算速度。然而位数更小的类型可能具有更差的精度或者更高的误差（特别是对于浮点类型）、更小的表示范围（特别是对于整数类型），所以在创建Tensor时可以按照需求指定类型。\n",
    "\n",
    "我们可以使用Tensor.type()函数展示其类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.FloatTensor\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "print(ac.type())\n",
    "print(c.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据上面的输出可以知道，默认的类型是32位浮点型，我们可以使用double()（以及float(), int(), long(), short()等）将其换为64位浮点型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.DoubleTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac_64=ac.double()\n",
    "print(ac_64.type())\n",
    "ac_64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以使用type_as()方法产生相同类型的Tensor："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e的类型： torch.LongTensor\n",
      "e_64的类型： torch.DoubleTensor\n",
      "tensor([1, 2, 3])\n",
      "tensor([1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "d=c.double()\n",
    "e=torch.tensor([1,2,3])\n",
    "print(\"e的类型：\", e.type())\n",
    "e_64=e.type_as(d)\n",
    "print(\"e_64的类型：\", e_64.type())\n",
    "print(e)\n",
    "print(e_64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果需要改变所有的默认类型，可以使用torch.set_default_tensor_type()函数，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.DoubleTensor\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([1.,2,3,4])\n",
    "print(a.type())\n",
    "torch.set_default_tensor_type(\"torch.DoubleTensor\")\n",
    "a=torch.tensor([1.,2,3,4])\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般而言，32位的浮点数已经足够用了，用Double会比较慢，特别是涉及到神经网络中大规模的计算，使用Double的计算开销比较大，所以不妨换回去："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(\"torch.FloatTensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与NumPy类似，PyTorch也有很多创建特殊Tensor的方法，比如：\n",
    "\n",
    "* 全为0的Tensor：torch.zeors()\n",
    "* 全为1的Tensor：torch.ones()\n",
    "* 单位阵：torch.eye()\n",
    "* U(0,1)的随机数：torch.rand()\n",
    "* 标准正态分布的随机数：torch.randn()\n",
    "* 从start到end，步长为step：torch.arange(start,end,step)\n",
    "* 从start到end，切成m份：torch.linspace(start,end,m)\n",
    "* 从10^start到10^end，切成m份：torch.linspace(start,end,m)\n",
    "* 随机排列：torch.randperm(m)\n",
    "\n",
    "比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "维数为5-by-4的全为0的矩阵：\n",
      " tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "维数为5-by-4的全为1的矩阵：\n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "维数为5-by-4的单位阵：\n",
      " tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0.]])\n",
      "维数为5-by-4的(0,1)上均匀分布随机数矩阵：\n",
      " tensor([[0.3455, 0.8366, 0.7481, 0.1533],\n",
      "        [0.5082, 0.4689, 0.2202, 0.0285],\n",
      "        [0.3153, 0.6423, 0.8443, 0.6514],\n",
      "        [0.1680, 0.1136, 0.0015, 0.4410],\n",
      "        [0.7553, 0.4446, 0.2803, 0.1984]])\n",
      "维数为5-by-4的标准正态分布矩阵：\n",
      " tensor([[-0.0331, -1.9236, -1.3173, -0.6466],\n",
      "        [-0.8283, -0.1833,  0.6490, -1.8499],\n",
      "        [ 1.0081, -1.4476,  0.4020, -1.1233],\n",
      "        [ 0.4895,  0.3461,  0.2943,  1.4924],\n",
      "        [-0.1123,  1.1365, -1.0954, -0.0594]])\n",
      "从1到10，步长为2：\n",
      " tensor([1, 3, 5, 7, 9])\n",
      "从1到10，4等分（5个数字）：：\n",
      " tensor([ 1.0000,  3.2500,  5.5000,  7.7500, 10.0000])\n",
      "从10^1到10^3，4等分（5个数字）：：\n",
      " tensor([   1.0000,    5.6234,   31.6228,  177.8279, 1000.0000])\n",
      "0-9共10个数字的随机排列：\n",
      " tensor([0, 6, 3, 5, 7, 8, 2, 4, 9, 1])\n"
     ]
    }
   ],
   "source": [
    "a=torch.zeros(5,4)\n",
    "print(\"维数为5-by-4的全为0的矩阵：\\n\",a)\n",
    "b=torch.ones(5,4)\n",
    "print(\"维数为5-by-4的全为1的矩阵：\\n\",b)\n",
    "c=torch.eye(5,4)\n",
    "print(\"维数为5-by-4的单位阵：\\n\",c)\n",
    "d=torch.rand(5,4)\n",
    "print(\"维数为5-by-4的(0,1)上均匀分布随机数矩阵：\\n\",d)\n",
    "e=torch.randn(5,4)\n",
    "print(\"维数为5-by-4的标准正态分布矩阵：\\n\",e)\n",
    "f=torch.arange(1,10,2)\n",
    "print(\"从1到10，步长为2：\\n\",f)\n",
    "g=torch.linspace(1,10,5)\n",
    "print(\"从1到10，4等分（5个数字）：：\\n\",g)\n",
    "h=torch.logspace(0,3,5)\n",
    "print(\"从10^1到10^3，4等分（5个数字）：：\\n\",h)\n",
    "i=torch.randperm(10)\n",
    "print(\"0-9共10个数字的随机排列：\\n\",i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，还可以使用如下命令产生形状相同的Tensor：\n",
    "\n",
    "* 创建与T形状相同的全为1的Tensor：torch.ones_like(T)\n",
    "* 创建与T形状相同的全为0的Tensor：torch.zeros_like(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与i形状相同的全为1的Tensor：\n",
      " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "与g形状相同的全为0的Tensor：\n",
      " tensor([0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "j=torch.ones_like(i)\n",
    "print(\"与i形状相同的全为1的Tensor：\\n\",j)\n",
    "k=torch.zeros_like(g)\n",
    "print(\"与g形状相同的全为0的Tensor：\\n\",k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与NumPy类似，对于一个Tensor，可以通过shape、size等属性和方法查看Tensor的形状，但是值得注意的是PyTorch的方法、属性与NumPy略有区别："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np_a=\n",
      " [[ 0.63123599  0.45494402  0.40702602  0.8229605   0.67766256  0.02695471]\n",
      " [ 0.4828611   0.2698627   0.3370565   0.48324526  0.91748013  0.05095007]\n",
      " [ 0.80420429  0.16541403  0.78957757  0.02940164  0.4046946   0.96254584]\n",
      " [ 0.66934309  0.02629138  0.62714104  0.20828769  0.71963974  0.87280719]\n",
      " [ 0.37634699  0.49697003  0.15583049  0.4941665   0.23214339  0.45092006]]\n",
      "np_a的维度： 2\n",
      "np_a的形状： (5, 6)\n",
      "np_a的长度： 30\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np_a=np.random.random((5,6))\n",
    "print(\"np_a=\\n\",np_a)\n",
    "print(\"np_a的维度：\",np_a.ndim)\n",
    "print(\"np_a的形状：\",np_a.shape)\n",
    "print(\"np_a的长度：\",np_a.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      " [[ 0.63123599  0.45494402  0.40702602  0.8229605   0.67766256  0.02695471]\n",
      " [ 0.4828611   0.2698627   0.3370565   0.48324526  0.91748013  0.05095007]\n",
      " [ 0.80420429  0.16541403  0.78957757  0.02940164  0.4046946   0.96254584]\n",
      " [ 0.66934309  0.02629138  0.62714104  0.20828769  0.71963974  0.87280719]\n",
      " [ 0.37634699  0.49697003  0.15583049  0.4941665   0.23214339  0.45092006]]\n",
      "a的维度： 2\n",
      "a的形状： torch.Size([5, 4])\n",
      "a的长度： torch.Size([5, 4])\n",
      "a的元素的个数： 20\n"
     ]
    }
   ],
   "source": [
    "print(\"a=\\n\",np_a)\n",
    "print(\"a的维度：\",a.dim())\n",
    "print(\"a的形状：\",a.shape)\n",
    "print(\"a的长度：\",a.size())\n",
    "print(\"a的元素的个数：\",a.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现：\n",
    "\n",
    "* NumPy中array的ndim, shape, size都是属性\n",
    "* PyTorch中Tensor的dim(), size()都是函数，shape为属性\n",
    "* NumPy中size代表元素的个数，而PyTorch中Tensor元素的个数是使用numel()方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 索引、视图与形状调整与\n",
    "\n",
    "与NumPy类似，Tensor也可以通过切片等操作产生Tensor的视图（view），与NumPy一样，视图并不会对Tensor进行复制，仅仅是一个引用。与NumPy类似，可以使用切片操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor([1, 2, 3, 4])\n",
      "b= tensor([1, 2])\n",
      "b= tensor([3, 2])\n",
      "a= tensor([3, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([1,2,3,4])\n",
    "print(\"a=\",a)\n",
    "b=a[0:2]\n",
    "print(\"b=\",b)\n",
    "b[0]=3\n",
    "print(\"b=\",b)\n",
    "print(\"a=\",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟NumPy类似的是，也可以使用掩码操作，不过掩码操作所创建的是一个拷贝，而非视图："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor([1, 2, 3, 4])\n",
      "c= tensor([3, 4])\n",
      "c= tensor([3, 5])\n",
      "a= tensor([1, 2, 3, 4])\n",
      "b= tensor([3, 2])\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([1,2,3,4])\n",
    "print(\"a=\",a)\n",
    "c=a[a>=3]\n",
    "print(\"c=\",c)\n",
    "c[-1]=5\n",
    "print(\"c=\",c)\n",
    "print(\"a=\",a)\n",
    "print(\"b=\",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果需要调整Tensor的形状，可以使用Tensor的view()函数以及reshape()方法，其中view()和reshape()接受的参数为修改后的形状，区别在于view()一定会创建视图，而reshape()则不一定，也可能创建原Tensor的拷贝。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor([1, 2, 3, 4])\n",
      "b=\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "c=\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "现在对b重新赋值\n",
      "a= tensor([1, 2, 3, 4])\n",
      "b=\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "c=\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([1,2,3,4])\n",
    "print(\"a=\",a)\n",
    "b=a.view(2,2)\n",
    "print(\"b=\\n\",b)\n",
    "c=a.reshape(2,2)\n",
    "print(\"c=\\n\",c)\n",
    "print(\"现在对b重新赋值\")\n",
    "b[0,0]=1\n",
    "print(\"a=\",a)\n",
    "print(\"b=\\n\",b)\n",
    "print(\"c=\\n\",c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以使用torch.reshape()函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor([5, 2, 3, 4])\n",
      "d=\n",
      " tensor([[5, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "d=torch.reshape(a,(2,2))\n",
    "d[0,0]=5\n",
    "print(\"a=\",a)\n",
    "print(\"d=\\n\",d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "形状也可以使用-1来描述，当使用-1时，自动计算该维度的大小："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor([1, 2, 3, 4, 5, 6])\n",
      "b=\n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([1,2,3,4,5,6])\n",
    "print(\"a=\",a)\n",
    "b=a.view(-1,2)\n",
    "print(\"b=\\n\",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用该特性将Tensor转化为向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "b=\n",
      " tensor([1, 2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(\"a=\\n\",a)\n",
    "b=a.view(-1)\n",
    "print(\"b=\\n\",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外还可以使用squeeze()和unsqueeze()方法对某一个维度压缩一个1或者增加一个1，比如如下的Tensor中，第0个维度的个数为1，可以使用squeeze()方法删掉这个维度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "torch.Size([1, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([[1,2,3,4,5,6]])\n",
    "print(a)\n",
    "print(a.shape)\n",
    "b=a.squeeze(0)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反过来也可以在指定维度上增加一个维度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "torch.Size([6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([1,2,3,4,5,6])\n",
    "print(a)\n",
    "print(a.shape)\n",
    "b=a.unsqueeze(1)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的计算\n",
    "\n",
    "与NumPy类似，Tensor也有很多计算功能，这与我们之前所学的NumPy几乎是相同的。\n",
    "\n",
    "一个与NumPy有点区别的地方是，在Tensor中，很多运算函数都有带下划线的版本，比如Tensor的add()方法同样有add_()方法，两者的区别在于：\n",
    "\n",
    "* 使用a.add(b)会返回计算结果a+b，而不改变a\n",
    "* 使用a.add_()会改变a，计算结束后a=a+b\n",
    "\n",
    "比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c= tensor([5, 7, 9])\n",
      "a= tensor([1, 2, 3])\n",
      "c= tensor([5, 7, 9])\n",
      "a= tensor([1, 2, 3])\n",
      "a= tensor([5, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([1,2,3])\n",
    "b=torch.tensor([4,5,6])\n",
    "c=a+b\n",
    "print(\"c=\",c)\n",
    "print(\"a=\",a)\n",
    "c=a.add(b)\n",
    "print(\"c=\",c)\n",
    "print(\"a=\",a)\n",
    "a.add_(b)\n",
    "print(\"a=\",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch中支持NumPy中支持的很多运算，比如：\n",
    "\n",
    "* 逐元素的算数运算\n",
    "  * 加（+,add()）\n",
    "  * 减（-,abs()）\n",
    "  * 乘（*,mul()）\n",
    "  * 除（/,div()）\n",
    "  * 取反（-，neg()）\n",
    "* 比较运算\n",
    "  * 大于（>,gt()）、大于等于（>=,ge()）、小于（<,lt()）、小于等于（>,le()）\n",
    "  * 等于（==, eq()）、不等于（!=,ne()）\n",
    "  * 排序：sort()\n",
    "  * 最大值（max()）、最小值（min()）\n",
    "  * 最大的k个数：topk()\n",
    "* 函数运算\n",
    "  * 绝对值（abs()）、平方根（sqrt()）\n",
    "  * 三角函数：sin(), cos(), asin(), tan(), atan(), cosh(), tanh().....\n",
    "  * 向上取整（ceil()）、向下取整（floor()）、四舍五入（round()）、取整数部分（trunc()）、取余（fmod()）、符号函数（sign()）\n",
    "  * 指数（exp()）、对数（log()）\n",
    "  * 激活函数\n",
    "    * sigmoid()\n",
    "    * tanh()\n",
    "    * softmax()\n",
    "* 归并函数\n",
    "  * 均值（mean()）、求和（sum()）、中位数（median()）、众数（mode()）\n",
    "  * 标准差（std()）、方差（var()）\n",
    "  * 范数（norm()）、距离（dist()）\n",
    "  * 累加（cumsum()）、累乘（cumprod()）\n",
    "* 矩阵运算\n",
    "  * 内积（dot()）、外积（cross()）\n",
    "  * 转置（t()）\n",
    "  * 矩阵乘法（mm()）\n",
    "  * 矩阵与向量乘法（mv()）\n",
    "  * 迹（trace()）\n",
    "  * 求逆（inverse()）\n",
    "  * 对角线元素（diag()）\n",
    "  * 奇异值分解（svd()）\n",
    "  \n",
    "等等非常多的运算。\n",
    "\n",
    "比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "a+b=\n",
      " tensor([ 3., 11., 18.])\n",
      "---\n",
      "a/b=\n",
      " tensor([0.5000, 0.2222, 1.2500])\n",
      "---\n",
      "a>=b\n",
      " tensor([False, False,  True])\n",
      "---\n",
      "b.sort()=\n",
      " torch.return_types.sort(\n",
      "values=tensor([2., 8., 9.]),\n",
      "indices=tensor([0, 2, 1]))\n",
      "---\n",
      "torch.max(a,b)=\n",
      " tensor([ 2.,  9., 10.])\n",
      "---\n",
      "b.max()=\n",
      " tensor(9.)\n",
      "---\n",
      "b.topk(2)=\n",
      " torch.return_types.topk(\n",
      "values=tensor([9., 8.]),\n",
      "indices=tensor([1, 2]))\n",
      "---\n",
      "torch.sqrt(b)=\n",
      " tensor([1.4142, 3.0000, 2.8284])\n",
      "---\n",
      "torch.sigmoid(b)=\n",
      " tensor([0.8808, 0.9999, 0.9997])\n",
      "---\n",
      "a.sum()=\n",
      " tensor(13.)\n",
      "---\n",
      "a.std()=\n",
      " tensor(4.9329)\n",
      "---\n",
      "a.norm(2)=\n",
      " tensor(10.2470)\n",
      "---\n",
      "torch.dist(a,b,2)=\n",
      " tensor(7.3485)\n",
      "---\n",
      "a.cumsum()=\n",
      " tensor([ 1.,  3., 13.])\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([1.,2,10])\n",
    "b=torch.tensor([2.,9,8])\n",
    "print(\"---\\na+b=\\n\",a+b)\n",
    "print(\"---\\na/b=\\n\",a/b)\n",
    "print(\"---\\na>=b\\n\",a>=b)\n",
    "print(\"---\\nb.sort()=\\n\",b.sort())\n",
    "print(\"---\\ntorch.max(a,b)=\\n\",torch.max(a,b))\n",
    "print(\"---\\nb.max()=\\n\",b.max())\n",
    "print(\"---\\nb.topk(2)=\\n\",b.topk(2))\n",
    "print(\"---\\ntorch.sqrt(b)=\\n\",torch.sqrt(b))\n",
    "print(\"---\\ntorch.sigmoid(b)=\\n\",torch.sigmoid(b))\n",
    "print(\"---\\na.sum()=\\n\",a.sum())\n",
    "print(\"---\\na.std()=\\n\",a.std())\n",
    "print(\"---\\na.norm(2)=\\n\",a.norm(2))\n",
    "print(\"---\\ntorch.dist(a,b,2)=\\n\",torch.dist(a,b,2))\n",
    "print(\"---\\na.cumsum()=\\n\",a.cumsum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      " tensor([[1., 2., 3.],\n",
      "        [5., 6., 4.],\n",
      "        [8., 1., 0.]])\n",
      "b=\n",
      " tensor([[ 2.,  4.,  3.],\n",
      "        [ 5.,  7.,  1.],\n",
      "        [ 8., 10.,  2.]])\n",
      "c=\n",
      " tensor([1., 2., 3.])\n",
      "---\n",
      "a+b=\n",
      " tensor([[ 3.,  6.,  6.],\n",
      "        [10., 13.,  5.],\n",
      "        [16., 11.,  2.]])\n",
      "---\n",
      "torch.exp(a)=\n",
      " tensor([[2.7183e+00, 7.3891e+00, 2.0086e+01],\n",
      "        [1.4841e+02, 4.0343e+02, 5.4598e+01],\n",
      "        [2.9810e+03, 2.7183e+00, 1.0000e+00]])\n",
      "---\n",
      "a.diag()=\n",
      " tensor([1., 6., 0.])\n",
      "---\n",
      "a.t()=\n",
      " tensor([[1., 5., 8.],\n",
      "        [2., 6., 1.],\n",
      "        [3., 4., 0.]])\n",
      "---\n",
      "torch.dot(c,d)=\n",
      " tensor(10.)\n",
      "---\n",
      "torch.mm(a,b)=\n",
      " tensor([[ 36.,  48.,  11.],\n",
      "        [ 72., 102.,  29.],\n",
      "        [ 21.,  39.,  25.]])\n",
      "---\n",
      "a.mm(b)=\n",
      " tensor([[ 36.,  48.,  11.],\n",
      "        [ 72., 102.,  29.],\n",
      "        [ 21.,  39.,  25.]])\n",
      "---\n",
      "a.trace()= tensor(7.)\n",
      "---\n",
      "a.inverse()= tensor([[ 0.0580, -0.0435,  0.1449],\n",
      "        [-0.4638,  0.3478, -0.1594],\n",
      "        [ 0.6232, -0.2174,  0.0580]])\n",
      "---\n",
      "a.mm(a.inverse())= tensor([[1.0000e+00, 2.9802e-08, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0000e+00]])\n",
      "---\n",
      "torch.mv(a,c)=\n",
      " tensor([14., 29., 10.])\n",
      "---\n",
      "torch.mv(a.t(),c)=\n",
      " tensor([35., 17., 11.])\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([[1.,2,3],[5,6,4],[8,1,0]])\n",
    "b=torch.tensor([[2.,4,3],[5,7,1],[8,10,2]])\n",
    "c=torch.tensor([1,2,3.])\n",
    "d=torch.tensor([3,2,1.])\n",
    "print(\"a=\\n\",a)\n",
    "print(\"b=\\n\",b)\n",
    "print(\"c=\\n\",c)\n",
    "print(\"---\\na+b=\\n\",a+b)\n",
    "print(\"---\\ntorch.exp(a)=\\n\",torch.exp(a))\n",
    "print(\"---\\na.diag()=\\n\",a.diag())\n",
    "print(\"---\\na.t()=\\n\",a.t())\n",
    "print(\"---\\ntorch.dot(c,d)=\\n\",torch.dot(c,d))\n",
    "print(\"---\\ntorch.mm(a,b)=\\n\",torch.mm(a,b))\n",
    "print(\"---\\na.mm(b)=\\n\",a.mm(b))\n",
    "print(\"---\\na.trace()=\",a.trace())\n",
    "print(\"---\\na.inverse()=\",a.inverse())\n",
    "print(\"---\\na.mm(a.inverse())=\",a.mm(a.inverse()))\n",
    "print(\"---\\ntorch.mv(a,c)=\\n\",torch.mv(a,c))\n",
    "print(\"---\\ntorch.mv(a.t(),c)=\\n\",torch.mv(a.t(),c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，还记得如果我们使用torch.from_numpy()函数和Tensor的numpy()函数在NumPy的array和PyTorch的Tensor之间转换，内存是共享的，转换速度很快，可以直接进行操作而付出较小的性能代价："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      " tensor([[1., 2., 3.],\n",
      "        [5., 6., 4.],\n",
      "        [8., 1., 0.]])\n",
      "b=\n",
      " tensor([[ 2.,  4.,  3.],\n",
      "        [ 5.,  7.,  1.],\n",
      "        [ 8., 10.,  2.]])\n",
      "c=\n",
      " tensor([[ 36.,  48.,  11.],\n",
      "        [ 72., 102.,  29.],\n",
      "        [ 21.,  39.,  25.]])\n",
      "c_np=\n",
      " [[  36.   48.   11.]\n",
      " [  72.  102.   29.]\n",
      " [  21.   39.   25.]]\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([[1.,2,3],[5,6,4],[8,1,0]])\n",
    "b=torch.tensor([[2.,4,3],[5,7,1],[8,10,2]])\n",
    "print(\"a=\\n\",a)\n",
    "print(\"b=\\n\",b)\n",
    "a_np=a.numpy()\n",
    "b_np=b.numpy()\n",
    "c_np=a_np@b_np\n",
    "c=torch.tensor(c_np)\n",
    "print(\"c=\\n\",c)\n",
    "print(\"c_np=\\n\",c_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，Tensor也支持广播操作，在这里不再赘述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动求导\n",
    "\n",
    "在神经网络中，优化算法需要计算损失函数对于每个参数的导数，然而在动辄成千上万甚至数十万参数的情况下，靠人工取计算这些导数显然是不现实的。为此，在现代化的深度学习框架中，都包含了自动求导的模块。\n",
    "\n",
    "在PyTorch中，torch.autograd模块就是用来实现自动求导的。\n",
    "\n",
    "对于一个Tensor，在创建的时候可以通过加入\"requires_grad=True\"选项，表示需要对这个Tensor进行求导。比如，我们对一个最简单的线性函数：$$y=x'b$$进行求导，易得：$$\\frac{dy}{db}=x$$一般而言，如果$(x,y)$为数据，$b$为参数，我们需要对参数求导（而非数据），从而我们只需要要求Tensor b在创建时加入requires_grad=True选项就可以了，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=\n",
      " tensor([0.7134, 0.1564, 0.2105])\n",
      "y= tensor(1.6577, grad_fn=<DotBackward>)\n",
      "dy/dx=\n",
      " None\n",
      "dy/db=\n",
      " tensor([0.7134, 0.1564, 0.2105])\n"
     ]
    }
   ],
   "source": [
    "x=torch.rand(3)\n",
    "b=torch.tensor([1.,2,3], requires_grad=True)\n",
    "y=torch.dot(x,b)\n",
    "print(\"x=\\n\",x)\n",
    "print(\"y=\",y)\n",
    "y.backward()\n",
    "print(\"dy/dx=\\n\",x.grad)\n",
    "print(\"dy/db=\\n\",b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到由于b加入了requires_grad=True选项，我们可以使用y.backward()选项，实现对b的求导；而x由于没有这个选项，从而没有对x进行求导。\n",
    "\n",
    "我们还可以使用更加复杂的函数，比如：$$y=x'b \\\\ z=e^{y}$$其对$b$的导数应该按照链式法则进行计算：$$\\frac{dz}{db}=\\frac{dz}{dy}\\cdot\\frac{dy}{db}=e^{y}\\cdot x=zx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=\n",
      " tensor([0.7400, 0.6846, 0.7948])\n",
      "y= tensor(4.4936, grad_fn=<DotBackward>)\n",
      "z= tensor(89.4405, grad_fn=<ExpBackward>)\n",
      "dz/db=\n",
      " tensor([66.1897, 61.2272, 71.0877])\n"
     ]
    }
   ],
   "source": [
    "x=torch.rand(3)\n",
    "b=torch.tensor([1.,2,3], requires_grad=True)\n",
    "y=torch.dot(x,b)\n",
    "z=torch.exp(y)\n",
    "print(\"x=\\n\",x)\n",
    "print(\"y=\",y)\n",
    "print(\"z=\",z)\n",
    "z.backward()\n",
    "print(\"dz/db=\\n\",b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上复合函数求导在PyTorch中是以 **计算图** 的方式进行的，比如上面的例子我们可以使用如下的计算图来表示：\n",
    "![](pic/compute_graph.gv.png \"计算图\")\n",
    "其中菱形代表计算。在计算导数时，使用一种“反向传播”算法，即使用链式法则，按照计算方向的反方向计算导数，如下图所示：\n",
    "![](pic/compute_graph_back.gv.png \"反向传播图\")\n",
    "上图中虚线代表计算导数的方向，每条虚线计算导数，然后在节点上从最终的节点$z$到该节点$b$的导数累乘起来，就得到了$\\frac{dz}{db}$。正因如此，计算节点导数的函数才被成为backward()。\n",
    "\n",
    "PyTorch的计算图有个非常重要的优势是使用了动态的计算图，意味着每次传播时，计算图可以动态重新构建。\n",
    "\n",
    "在中间节点加入参数也是完全可行的，比如：$$y=x'b \\\\ z=e^{cy}$$那么导数为：$$\\frac{dz}{dc}=ye^{cy}=yz \\\\ \\frac{dz}{db}=\\frac{dz}{dy}\\cdot\\frac{dy}{db}=ce^{cy}\\cdot x=czx$$使用计算图表示为：\n",
    "![](pic/compute_graph_back2.gv.png \"反向传播图\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=\n",
      " tensor([0.1427, 0.4501, 0.1079])\n",
      "c= tensor([2.], requires_grad=True)\n",
      "y= tensor(1.3666, grad_fn=<DotBackward>)\n",
      "z= tensor([15.3830], grad_fn=<ExpBackward>)\n",
      "dz/db=\n",
      " tensor([ 4.3908, 13.8477,  3.3199])\n",
      "dz/dc=\n",
      " tensor([21.0229])\n"
     ]
    }
   ],
   "source": [
    "x=torch.rand(3)\n",
    "b=torch.tensor([1.,2,3], requires_grad=True)\n",
    "y=torch.dot(x,b)\n",
    "c=torch.tensor([2.], requires_grad=True)\n",
    "z=torch.exp(c*y)\n",
    "print(\"x=\\n\",x)\n",
    "print(\"c=\",c)\n",
    "print(\"y=\",y)\n",
    "print(\"z=\",z)\n",
    "z.backward()\n",
    "print(\"dz/db=\\n\",b.grad)\n",
    "print(\"dz/dc=\\n\",c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在统计和机器学习中，计算的目标函数（损失函数）通常是一个求和的形式：$$Q(x,b)=\\sum_{i=1}^N q(x_i,b)$$此时我们对$b$求导：$$\\frac{dQ}{db}=\\sum_{i=1}^N \\frac{dq(x_i,b)}{db}$$在默认的情况下，每次调用backward()后，导数都会自动进行累加："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/db=\n",
      " tensor([1., 2., 3.])\n",
      "dy/db=\n",
      " tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "x1=torch.tensor([1,2,3.])\n",
    "x2=torch.tensor([4,5,6.])\n",
    "b=torch.tensor([1.,2,3], requires_grad=True)\n",
    "y=torch.dot(x1,b)\n",
    "y.backward()\n",
    "print(\"dy/db=\\n\",b.grad)\n",
    "y=torch.dot(x2,b)\n",
    "y.backward()\n",
    "print(\"dy/db=\\n\",b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果不需要累加，需要对梯度清零："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/db=\n",
      " tensor([1., 2., 3.])\n",
      "dy/db=\n",
      " tensor([4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "x1=torch.tensor([1,2,3.])\n",
    "x2=torch.tensor([4,5,6.])\n",
    "b=torch.tensor([1.,2,3], requires_grad=True)\n",
    "y=torch.dot(x1,b)\n",
    "y.backward()\n",
    "print(\"dy/db=\\n\",b.grad)\n",
    "b.grad.zero_()\n",
    "y=torch.dot(x2,b)\n",
    "y.backward()\n",
    "print(\"dy/db=\\n\",b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在PyTorch中，只允许一个标量对张量求导，而不允许张量对张量求导。如果需要张量对张量求导，需要手动计算。\n",
    "比如，对于函数：$$y_1=b_1 \\times x_1 + b_2 \\times x_2 +b_3 \\times x_1\\times x_2 \\\\ y_2= b_2\\times x_1^{b_1}+x_2^{b_3}$$参数为$\\left[b_1,b_2,b_3\\right]$，而输出为$\\left[y_1,y_2\\right]$，其导数为：$$\\frac{dy}{db}=\\left[\\begin{array}{ccc}\n",
    "x_{1} & x_{2} & x_{1}x_{2}\\\\\n",
    "b_{2}x_{1}^{b_{1}}\\ln\\left(x_{1}\\right) & x_{1}^{b_{1}} & x_{2}^{b_{3}}\\ln\\left(x_{2}\\right)\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "为了计算该Jacobian矩阵，可以分开计算第一行和第二行。在backward()函数中，可以加入一个参数向量v，比如如果使用：\n",
    "```python\n",
    "y.backward(v)\n",
    "```\n",
    "\n",
    "那么该函数就将计算：$\\frac{d(vy')}{db}$，我们只需要分别令$v_1=\\left[1,0\\right],v_2=\\left[0,1\\right]$就可以分别计算出Jaccobian的两行，然后放在一起即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= tensor([1., 2.])\n",
      "y= tensor([11., 10.], grad_fn=<CopySlices>)\n",
      "J1= tensor([0.0000, 1.0000, 5.5452])\n",
      "J2= tensor([0.0000, 1.0000, 5.5452])\n",
      "J=\n",
      " tensor([[0.0000, 1.0000, 5.5452],\n",
      "        [0.0000, 1.0000, 5.5452]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([1.,2])\n",
    "b=torch.tensor([1,2,3.], requires_grad=True)\n",
    "y=torch.zeros(2)\n",
    "y[0]=b[0]*x[0]+b[1]*x[1]+b[2]*x[0]*x[1]\n",
    "y[1]=b[1]*(x[0]**b[0])+x[1]**b[2]\n",
    "print(\"x=\",x)\n",
    "print(\"y=\",y)\n",
    "y.backward(torch.tensor([1.,0]), retain_graph=True)\n",
    "J1=b.grad\n",
    "b.grad.zero_()\n",
    "J2=y.backward(torch.tensor([0,1.]))\n",
    "J2=b.grad\n",
    "print(\"J1=\",J1)\n",
    "print(\"J2=\",J2)\n",
    "J=torch.zeros(2,3)\n",
    "J[0]=J1\n",
    "J[1]=J2\n",
    "print(\"J=\\n\",J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意以上代码中的两个细节：\n",
    "\n",
    "* 第一次调用backward时，使用了retain_graph=True选项，该选项代表在多次传播时，强制保留反向传播中的中间计算结果\n",
    "* 由于每一次传播时grad会累加，为了不让其累加，我们对b.grad进行了清零操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据读取\n",
    "\n",
    "在torch.utils.data模块中包含了读取数据的类，比如Dataset、IterableDataset和DataLoader等。其中：\n",
    "\n",
    "* Dataset提供了一个抽象类，用于用户自定义数据集，用户需要重载两个函数完成这个类：\n",
    "    * \\_\\_len\\_\\_()：数据量大小\n",
    "    * \\_\\_getitem\\_\\_()：获得一条数据\n",
    "* IterableDataset同样提供了一个抽象类，与Dataset不同的是该抽象类使用迭代器的方式返回数据，通常用来比较大型的数据或者从数据库、远程服务器中读取的数据等等，用户需要重载一个函数完成这个类：\n",
    "    * \\_\\_iter\\_\\_()：返回一个数据条目（可迭代）\n",
    "* DataLoader在Dataset的基础上定义一个迭代器，实现批量（batch）读取、随机读取等操作。\n",
    "\n",
    "我们首先以一个伪数据看一下Dataset的使用方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=\n",
      " [[ 0.58602222  0.80495029  0.19734014]\n",
      " [-1.57468075  0.81411451  1.9698675 ]\n",
      " [-0.93712474  1.02467448 -1.11080625]\n",
      " [-2.02342489 -0.32069881 -0.12281846]\n",
      " [ 0.32559092 -0.74437519 -1.49184672]\n",
      " [-1.19720758  0.34140826 -0.44582146]\n",
      " [ 0.02193199  0.75287457  0.1785099 ]\n",
      " [ 1.2688634   0.26122625  3.94300985]\n",
      " [-1.45330455 -0.20342525  0.12963423]\n",
      " [ 0.65416681 -0.47964503  0.65412619]] \n",
      "Y=\n",
      " [  2.78794322   5.96315078  -2.22019451  -3.03327789  -5.6386996\n",
      "  -1.85185543   2.06321083  13.62034545  -1.47125234   1.65725532]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 这里假设一个伪数据集，使用NumPy生成\n",
    "X=np.random.normal(0,1,(10,3))\n",
    "b=np.array([1,2,3.])\n",
    "Y=X@b.T\n",
    "print(\"X=\\n\",X,\"\\nY=\\n\",Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上使用NumPy产生了10个观测，其中有3个特征，以及一个标签（Y），接下来我们定义Dataset："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class fake_data(Dataset):\n",
    "    def __len__(self):\n",
    "        return X.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        x=X[i,:]\n",
    "        y=Y[i]\n",
    "        data=torch.from_numpy(x)\n",
    "        label=torch.tensor(y)\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上我们就定义了一个Dataset，我们可以使用如下方法获得给定下标的数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有10个观测\n",
      "第1个观测为：\n",
      " (tensor([0.5860, 0.8050, 0.1973], dtype=torch.float64), tensor(2.7879))\n"
     ]
    }
   ],
   "source": [
    "fd=fake_data()\n",
    "print(\"共有{}个观测\".format(fd.__len__()))\n",
    "print(\"第1个观测为：\\n\",fd[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上的Dataset只提供了一个简单的数据抽取对象，而深度学习所需要的更多的操作可以由DataLoader来完成。\n",
    "\n",
    "为了创建一个DataLoader，需要提供以下信息：\n",
    "* Dataset：像上面定义的Dataset，数据源\n",
    "* shuffle：是否将数据打乱\n",
    "* batch_size: 批量处理每批的大小\n",
    "* sampler/batch_sampler：一个用于将数据打乱的类，可以使用默认\n",
    "* num_workers：载入数据使用的进程数，默认为0，即使用主进程\n",
    "* pin_memory：是否将数据保存在CUDA的pinned memory区，从而放入GPU中会快一些\n",
    "* drop_last：是否扔掉最后一个不完整的batch（通常由于数据量/批大小不能整除）\n",
    "\n",
    "比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0批：\n",
      "tensor([[ 0.0219,  0.7529,  0.1785],\n",
      "        [ 0.5860,  0.8050,  0.1973],\n",
      "        [-1.5747,  0.8141,  1.9699]], dtype=torch.float64)\n",
      "tensor([2.0632, 2.7879, 5.9632])\n",
      "第1批：\n",
      "tensor([[-2.0234, -0.3207, -0.1228],\n",
      "        [ 0.6542, -0.4796,  0.6541],\n",
      "        [ 0.3256, -0.7444, -1.4918]], dtype=torch.float64)\n",
      "tensor([-3.0333,  1.6573, -5.6387])\n",
      "第2批：\n",
      "tensor([[-0.9371,  1.0247, -1.1108],\n",
      "        [-1.1972,  0.3414, -0.4458],\n",
      "        [ 1.2689,  0.2612,  3.9430]], dtype=torch.float64)\n",
      "tensor([-2.2202, -1.8519, 13.6203])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "fdl=DataLoader(fd, shuffle=True, batch_size=3, drop_last=True)\n",
    "for d,l in enumerate(fdl):\n",
    "    print(\"第{}批：\".format(d))\n",
    "    print(l[0])\n",
    "    print(l[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果需要全部返回，设定batch_size为数据量即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6542, -0.4796,  0.6541],\n",
      "        [ 0.0219,  0.7529,  0.1785],\n",
      "        [-2.0234, -0.3207, -0.1228],\n",
      "        [ 0.5860,  0.8050,  0.1973],\n",
      "        [-1.4533, -0.2034,  0.1296],\n",
      "        [-0.9371,  1.0247, -1.1108],\n",
      "        [ 1.2689,  0.2612,  3.9430],\n",
      "        [ 0.3256, -0.7444, -1.4918],\n",
      "        [-1.5747,  0.8141,  1.9699],\n",
      "        [-1.1972,  0.3414, -0.4458]], dtype=torch.float64)\n",
      "tensor([ 1.6573,  2.0632, -3.0333,  2.7879, -1.4713, -2.2202, 13.6203, -5.6387,\n",
      "         5.9632, -1.8519])\n"
     ]
    }
   ],
   "source": [
    "fdl=DataLoader(fd, shuffle=True, batch_size=fd.__len__(), drop_last=True)\n",
    "for d in fdl:\n",
    "    print(d[0])\n",
    "    print(d[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应用实例：使用PyTorch计算Logistic回归\n",
    "\n",
    "应用PyTorch可以自动计算导数的特性，我们可以轻松的计算线性回归、Logistic回归等多数简单的统计模型。接下来我们就使用PyTorch计算Logistic回归，并与真实概率进行比较。\n",
    "\n",
    "首先，我们在一个fake数据集上进行计算，并验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntelPy",
   "language": "python",
   "name": "intelpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
