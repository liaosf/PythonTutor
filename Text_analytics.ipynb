{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分析的基本概念\n",
    "\n",
    "跟我们之前介绍的分析不同，文本数据是典型的非结构化数据，而且本身具有非常复杂的结构，相对之前的数据分析相比有其特有的分析技巧。这里我们将简单介绍文本分析的基本流程以及一些基础的工具。\n",
    "\n",
    "虽然差别很大，但是基本流程与之前的分析还是想通的，一般都需要如下步骤：\n",
    "\n",
    "1. 准备数据，在这一步除了要准备需要进行分析的数据之外，可能还需要准备额外的语料库（corpus）。\n",
    "2. 文本规范化处理，也就是我们之前清洗数据的步骤，比如分词、去除停用词、去除特殊符号等无意义字符、同义词转换、缩写转换等等。\n",
    "3. 特征工程，从已经清洗好的数据中提取特征。由于计算机只能处理数值型的变量，因而在这一步有一个比较关键的步骤是将文字转换为计算机可以理解的向量等数值型变量。\n",
    "4. 训练模型，经过这些步骤后，针对不同的目的，模型训练可能与之前的算法比较类似，但是也有针对文本数据特有的模型。\n",
    "5. 模型评价，评价模型的性能，重复以上步骤，改进模型。\n",
    "\n",
    "针对文本数据，除了其他数据同样可以进行的相关性计算、聚类、分类等模型之外，还有一些任务是文本数据特有的，比如：\n",
    "\n",
    "* 分词\n",
    "* 词性标注\n",
    "* 词嵌入\n",
    "* 摘要和主题建模\n",
    "* 实体识别\n",
    "* 知识图谱\n",
    "* 语义分析\n",
    "* ......\n",
    "\n",
    "其中有的模型结果是其他模型的基础，比如分词、词性标注等是很多其他模型的基础。\n",
    "\n",
    "在本节，我们将主要使用Python中的**NLTK**（http://www.nltk.org ）、**Scikit-Learn**、**Jieba**（https://github.com/fxsjy/jieba ）、**Gensim**（https://github.com/RaRe-Technologies/gensim ）等工具介绍文本分析的基本原理。不过与此同时，自然语言处理，包括中文的自然语言处理正在蓬勃发展，很多新的工具可以使用，比如对标NLTK并且号称有更好性能的**spaCy**（ https://spacy.io ），以及已经经过预训练可以直接拿来用的模型比如最近如火如荼的**BERT**、**HanLP**（https://github.com/hankcs/HanLP ）、**Stanford CoreNLP**（https://github.com/stanfordnlp/CoreNLP ）等等，学习基本原理后可以直接使用这些包进行自己的研究。\n",
    "\n",
    "我们首先从文本的规范化处理开始，介绍文本分析的基本原理和方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本规范化\n",
    "\n",
    "文本是典型的非结构化数据，我们需要将文本转换为高度结构化的数据，首先要对文本进行有意义的划分，一般涉及到句子的**切分**（**tokenization**）以及其他清洗步骤。\n",
    "\n",
    "**标识**（**token**）是文本的有意义的最小成分，文本处理的最简单操作即将文本切成一个个的token，通常包括句子切分和词语切分。接下来我们很少有研究句子的成分和语义，更多时候是针对词语的分析，因而接下来主要介绍句子的切分方法。\n",
    "\n",
    "## 英文切分\n",
    "\n",
    "英文的词语切分一般比较简单，主要原因是因为英文的单词之间都有空格进行分割，而中文的切分就复杂很多。常见的自然语言处理包比如NKTL以及spaCy都肯定包含了切分的函数，比如在NLTK中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am working very hard to help New York City & State. Dealing with both Mayor & Governor and producing tremendously for them, including four new medical centers and four new hospitals. Fake News that I won’t help them because I don’t like Cuomo (I do). Just sent 4000 ventilators!'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=\"I am working very hard to help New York City & State. Dealing with both Mayor & Governor and producing tremendously for them, including four new medical centers and four new hospitals. Fake News that I won’t help them because I don’t like Cuomo (I do). Just sent 4000 ventilators!\"\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'working', 'very', 'hard', 'to', 'help', 'New', 'York', 'City', '&', 'State', '.', 'Dealing', 'with', 'both', 'Mayor', '&', 'Governor', 'and', 'producing', 'tremendously', 'for', 'them', ',', 'including', 'four', 'new', 'medical', 'centers', 'and', 'four', 'new', 'hospitals', '.', 'Fake', 'News', 'that', 'I', 'won', '’', 't', 'help', 'them', 'because', 'I', 'don', '’', 't', 'like', 'Cuomo', '(', 'I', 'do', ')', '.', 'Just', 'sent', '4000', 'ventilators', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "words=word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意运行以上命令可能先要下载相应的包：在Python解释器中运行：nltk.download('punkt') ，如果提示错误，可以参考：https://www.cnblogs.com/sddai/p/10543359.html\n",
    "\n",
    "当然NLTK不止支持这一种切分方法，比如我们可以使用正则表达式切分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'working', 'very', 'hard', 'to', 'help', 'New', 'York', 'City', 'State', 'Dealing', 'with', 'both', 'Mayor', 'Governor', 'and', 'producing', 'tremendously', 'for', 'them', 'including', 'four', 'new', 'medical', 'centers', 'and', 'four', 'new', 'hospitals', 'Fake', 'News', 'that', 'I', 'won’t', 'help', 'them', 'because', 'I', 'don’t', 'like', 'Cuomo', 'I', 'do', 'Just', 'sent', '4000', 'ventilators']\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "\n",
    "Tokenizer=RegexpTokenizer(pattern=r\"[\\w\\-’']+\")\n",
    "words=Tokenizer.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外还可以使用空白字符（空格、缩进、换行）等进行切分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'working', 'very', 'hard', 'to', 'help', 'New', 'York', 'City', '&', 'State.', 'Dealing', 'with', 'both', 'Mayor', '&', 'Governor', 'and', 'producing', 'tremendously', 'for', 'them,', 'including', 'four', 'new', 'medical', 'centers', 'and', 'four', 'new', 'hospitals.', 'Fake', 'News', 'that', 'I', 'won’t', 'help', 'them', 'because', 'I', 'don’t', 'like', 'Cuomo', '(I', 'do).', 'Just', 'sent', '4000', 'ventilators!']\n"
     ]
    }
   ],
   "source": [
    "from nltk import WhitespaceTokenizer\n",
    "\n",
    "Tokenizer=WhitespaceTokenizer()\n",
    "words=Tokenizer.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "英文分词虽然原理简单，但是还是有很多细节的坑，比如上面Trump先生的「don’t」和「don't」，如果在切分或者其他清洗步骤中予以重视，计算机会认为这是两个词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文分词\n",
    "\n",
    "中文文本分析与英文的文本分析一个最重要的区别在于，英文使用空格分割每个单词，但是中文没有分割单词的概念。\n",
    "\n",
    "为了克服这个问题，分词就应运而生了。结合字典和算法，分词软件可以帮助我们将中文的文章、句子分解为一个个的中文单词。\n",
    "\n",
    "目前已经有很多成熟的分词工具，比如中科院的NLPIR汉语分词系统、结巴分词以及腾讯、阿里、百度的分词系统等等。在这里我们以开源的结巴分词为例，介绍分词工具的用法。\n",
    "\n",
    "为了使用结巴分词，首先需要安装。在terminal中输入：\n",
    "```shell\n",
    "pip install jieba\n",
    "```\n",
    "\n",
    "就可以进行安装了。安装好之后，可以将jieba模块导入到Python程序中，就可以正常使用了：\n",
    "```shell\n",
    "import jieba\n",
    "```\n",
    "\n",
    "比如，最简单的用法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.591 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年以来', '，', '我国', '持续', '推进', '减税', '降费', '、', '提高', '最低工资', '标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人所得税', '改革', '以及', '专项', '附加', '扣除', '方案', '，', '有效', '增加', '了', '居民', '可', '支配', '收入', '。', '与此同时', '，', '不断', '消除', '居民消费', '的', '后顾之忧', '。', '消费', '需求', '进一步', '释放', '，', '消费市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "line=\"今年以来，我国持续推进减税降费、提高最低工资标准、促进就业，特别是年初开始实施的个人所得税改革以及专项附加扣除方案，有效增加了居民可支配收入。与此同时，不断消除居民消费的后顾之忧。消费需求进一步释放，消费市场亮点纷呈\"\n",
    "wlist=jieba.cut(line)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cut()函数有三个参数：必须要提供的是需要进行分词的字符串；此外，cut_all参数控制是否采用全模式；HMM参数用来控制是否使用HMM模型。其区别是：\n",
    "\n",
    "* cut_all=True， 代表使用全模式，全模式可以切出混合不同粒度的词\n",
    "* HMM=True，代表使用HMM模型，用于推断字典中没有的词\n",
    "\n",
    "比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年', '今年以来', '以来', '，', '我国', '持续', '推进', '减税', '降', '费', '、', '提高', '最低', '最低工资', '低工资', '工资', '工资标准', '标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人', '个人所得', '个人所得税', '所得', '所得税', '税改', '改革', '以及', '专项', '附加', '扣除', '方案', '，', '有效', '增加', '了', '居民', '可支配', '支配', '收入', '。', '与此', '与此同时', '同时', '，', '不断', '消除', '居民', '居民消费', '消费', '的', '后顾之忧', '。', '消费', '需求', '求进', '进一步', '一步', '释放', '，', '消费', '消费市场', '市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "wlist=jieba.cut(line, cut_all=True)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年以来', '，', '我国', '持续', '推进', '减税', '降费', '、', '提高', '最低工资', '标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人所得税', '改革', '以及', '专项', '附加', '扣除', '方案', '，', '有效', '增加', '了', '居民', '可', '支配', '收入', '。', '与此同时', '，', '不断', '消除', '居民消费', '的', '后顾之忧', '。', '消费', '需求', '进一步', '释放', '，', '消费市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "wlist=jieba.cut(line, HMM=True)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年', '今年以来', '以来', '，', '我国', '持续', '推进', '减税', '降', '费', '、', '提高', '最低', '最低工资', '低工资', '工资', '工资标准', '标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人', '个人所得', '个人所得税', '所得', '所得税', '税改', '改革', '以及', '专项', '附加', '扣除', '方案', '，', '有效', '增加', '了', '居民', '可支配', '支配', '收入', '。', '与此', '与此同时', '同时', '，', '不断', '消除', '居民', '居民消费', '消费', '的', '后顾之忧', '。', '消费', '需求', '求进', '进一步', '一步', '释放', '，', '消费', '消费市场', '市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "wlist=jieba.cut(line, HMM=True, cut_all=True)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，上面的分词结果......一言难尽。实际上，任何分词算法都不可避免的不能跟上时代的潮流，特别是网络时代，新的词语层出不穷，而在一些专业领域中，一些专有名词往往普通词典无法完全覆盖。比如“减费降税”、“可支配收入”这些专有名词，都没有被正确分出来。\n",
    "\n",
    "为了克服这个问题，往往需要用户自己添加字典。比如，我们可以把“减费降税”、“可支配收入”这些名词放在一个文本文件中，每个新词写成一行，然后使用load_userdict()函数给定这个文件，就可以添加自己的新词列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年以来', '，', '我国', '持续', '推进', '减税降费', '、', '提高', '最低工资标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人所得税', '改革', '以及', '专项附加扣除', '方案', '，', '有效', '增加', '了', '居民', '可支配收入', '。', '与此同时', '，', '不断', '消除', '居民消费', '的', '后顾之忧', '。', '消费需求', '进一步', '释放', '，', '消费市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "with open('user_dict.txt','wt') as f:\n",
    "    f.write(\"减税降费\\n可支配收入\\n最低工资\\n最低工资标准\\n专项附加扣除\\n消费需求\")\n",
    "jieba.load_userdict('user_dict.txt')\n",
    "wlist=jieba.cut(line)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入了用户字典后，新的分词更加准确了。在实际应用的时候，无论使用什么分词工具，用户字典的构建往往是非常关键的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大小写转换\n",
    "\n",
    "主要针对英文等字母文字，一般的做法是统一转换为小写字母，避免出现「FAKE NEWS$\\neq$fake news」的情况出现。可以使用字符串的.lower()方法很容易的完成大小写转换："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'working', 'very', 'hard', 'to', 'help', 'new', 'york', 'city', '&', 'state.', 'dealing', 'with', 'both', 'mayor', '&', 'governor', 'and', 'producing', 'tremendously', 'for', 'them,', 'including', 'four', 'new', 'medical', 'centers', 'and', 'four', 'new', 'hospitals.', 'fake', 'news', 'that', 'i', 'won’t', 'help', 'them', 'because', 'i', 'don’t', 'like', 'cuomo', '(i', 'do).', 'just', 'sent', '4000', 'ventilators!']\n"
     ]
    }
   ],
   "source": [
    "words=[w.lower() for w in words]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 删除停用词和特殊字符\n",
    "\n",
    "分词之后，在进一步进行处理之前，消除停用词和特殊符号往往是非常关键的。比如在上面的分词结果中，“的”、“是”、“与此同时”等，含义并不是非常明显，对其分析的意义不大，留着这些词只会空占内存，并且可能对分析结果产生巨大影响。一个常用的做法是，使用一个停用词列表，分词结束之后，把停用词列表中的词全都剔除出去。\n",
    "\n",
    "比如，在文本文档“Chinese/stopword.txt”中，我们列举出了一些常用的停用词以及特殊字符，我们可以使用如下方法消除停用词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年以来', '，', '我国', '持续', '推进', '减税降费', '、', '提高', '最低工资标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人所得税', '改革', '以及', '专项附加扣除', '方案', '，', '有效', '增加', '了', '居民', '可支配收入', '。', '与此同时', '，', '不断', '消除', '居民消费', '的', '后顾之忧', '。', '消费需求', '进一步', '释放', '，', '消费市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "with open('Chinese/stopword.txt','rt') as f:\n",
    "    stoplist=f.readlines()\n",
    "    stoplist=set([w.lower() for w in stoplist])\n",
    "wlist=jieba.cut(line)\n",
    "wlist=[w.lower() for w in wlist if w not in stoplist]\n",
    "print(wlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意上面的函数里面我们还同时对停用词和词语转换为了小写。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 扩展缩写词、同义词转换\n",
    "\n",
    "缩写词即诸如：「isn't==is not」之类的缩写，一般而言也需要特殊处理。一般来说我们可以通过定义一个映射关系来处理这种情况，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年以来', '，', '我国', '持续', '推进', '减税降费', '、', '提高', '最低工资标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个税', '改革', '以及', '专项附加扣除', '方案', '，', '有效', '增加', '了', '居民', '可支配收入', '。', '与此同时', '，', '不断', '消除', '居民消费', '的', '后顾之忧', '。', '消费需求', '进一步', '释放', '，', '消费市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "syno={\"isn't\": \"is not\",\n",
    "     \"aren't\": \"are not\",\n",
    "     \"i'll\": \"i will\",\n",
    "     \"个人所得税\":\"个税\"}\n",
    "wlist=[syno[w]  if w in syno.keys() else w for w in wlist]\n",
    "print(wlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，还有一些更加细致的工作，比如检查拼写错误、矫正重复字符等等，此外英文可能还涉及到时态、单复数的问题，这些都需要大量细致的工作，特别是针对不同的应用场景进行特定的优化是非常有必要的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个简单的例子：中文词频统计\n",
    "\n",
    "以下实现了对《越女剑》的词频统计："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>范蠡</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>道</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>剑士</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>青衣</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>阿青</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>勾践</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>锦衫</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>长剑</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>说道</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>吴国</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word  frequency\n",
       "0   范蠡        118\n",
       "1    道        113\n",
       "2   剑士        105\n",
       "3   青衣         47\n",
       "4   阿青         47\n",
       "5   勾践         44\n",
       "6   锦衫         35\n",
       "7   长剑         34\n",
       "8   说道         31\n",
       "9   吴国         30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "## 停用词\n",
    "with open('Chinese/stopword.txt','rt') as f:\n",
    "    stoplist=f.readlines()\n",
    "    stoplist=[w.lower().strip() for w in stoplist]\n",
    "## 读入小说\n",
    "wordlist=[]\n",
    "with open('Chinese/越女剑.txt','rt') as f:\n",
    "    for l in f:\n",
    "        line_cut=jieba.cut(l)\n",
    "        line=[w.strip().lower() for w in line_cut]\n",
    "        wordlist.extend(line)\n",
    "wordlist=[w for w in wordlist if w not in stoplist]\n",
    "## 统计\n",
    "text_dict=dict()\n",
    "for l in wordlist:\n",
    "    if l not in text_dict:\n",
    "        text_dict[l]=1\n",
    "    else:\n",
    "        text_dict[l]+=1\n",
    "text_freq=[]\n",
    "for k in text_dict:\n",
    "    text_freq.append((k,text_dict[k]))\n",
    "## 排序\n",
    "text_freq.sort(key=lambda x: x[1],reverse=True)\n",
    "## 用pandas显示，更好看\n",
    "import pandas as pd\n",
    "freq=pd.DataFrame(text_freq, columns=['word','frequency'])\n",
    "freq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取特征\n",
    "\n",
    "由于计算机只能处理数值变量进行运算，在文本数据清洗完毕后，接下来通常需要将其转换为计算机可以识别的向量。目前有多重方法可以完成这项任务，我们在这里主要介绍其中常见的三种：词袋、TF-IDF以及词嵌入三种模型。\n",
    "\n",
    "## 词袋模型\n",
    "\n",
    "**词袋**（**bag of words**）模型是最基础的一种将文本数据结构化为向量的一种方法。实际上词袋可以简单理解为每个文本的词频统计。比如，我们接下来使用一些上市公司的标题数据，并使用上面介绍的方法将每个标题转换为向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59021767</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>沪争取增值税扩围改革试点</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59021769</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>周小川：外部施压不会影响人民币升值步伐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59021771</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>准备金率再上调 达到19.5％创新高</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59021772</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>浙江亚太药业股份有限公司股票2011年02月14日2011年02月18日二级市场表现周简报</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59021774</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>定基价格指数若涨20% 政府或出手调控</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59021776</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>政策倾斜和加大投入 西藏将做大做强藏药产业</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59021778</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>新疆13个地州探矿权年 590个项目涉嫌“圈而不探”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>59021779</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>沪士电子股份有限公司股票2011年02月14日2011年02月18日二级市场表现周简报</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>59021783</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>北京：楼市限购首日成交环比降9成</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>59021787</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>上海：房管局发布“沪九条”限购执行细则</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       date                                          title\n",
       "0  59021767 2011-02-19                                   沪争取增值税扩围改革试点\n",
       "1  59021769 2011-02-19                            周小川：外部施压不会影响人民币升值步伐\n",
       "2  59021771 2011-02-19                             准备金率再上调 达到19.5％创新高\n",
       "3  59021772 2011-02-19  浙江亚太药业股份有限公司股票2011年02月14日2011年02月18日二级市场表现周简报\n",
       "4  59021774 2011-02-19                            定基价格指数若涨20% 政府或出手调控\n",
       "5  59021776 2011-02-19                          政策倾斜和加大投入 西藏将做大做强藏药产业\n",
       "6  59021778 2011-02-19                     新疆13个地州探矿权年 590个项目涉嫌“圈而不探”\n",
       "7  59021779 2011-02-19    沪士电子股份有限公司股票2011年02月14日2011年02月18日二级市场表现周简报\n",
       "8  59021783 2011-02-19                               北京：楼市限购首日成交环比降9成\n",
       "9  59021787 2011-02-19                            上海：房管局发布“沪九条”限购执行细则"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "RAW=pd.read_csv(\"csv/stocknews1.csv\")\n",
    "RAW['date']=pd.to_datetime(RAW['date'])\n",
    "RAW.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们额外进行一些处理，注意到其中的类似「2011年02月14日2011年02月18日二级市场表现周简报」之类的title很没有营养，我们打算去除他，可以使用正则表达式方便的达到目的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59021767</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>沪争取增值税扩围改革试点</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59021769</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>周小川：外部施压不会影响人民币升值步伐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59021771</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>准备金率再上调 达到19.5％创新高</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59021774</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>定基价格指数若涨20% 政府或出手调控</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59021776</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>政策倾斜和加大投入 西藏将做大做强藏药产业</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134769</th>\n",
       "      <td>19839999</td>\n",
       "      <td>2006-12-27</td>\n",
       "      <td>定价基准不同 新利率基准将冲击旧浮动利率债</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134770</th>\n",
       "      <td>19840000</td>\n",
       "      <td>2006-12-27</td>\n",
       "      <td>债市延续调整格局 投资者宜缩短投资久期</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134771</th>\n",
       "      <td>19840023</td>\n",
       "      <td>2006-12-27</td>\n",
       "      <td>曾培炎:利用外汇储备优势</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134772</th>\n",
       "      <td>19840026</td>\n",
       "      <td>2006-12-27</td>\n",
       "      <td>广州小时最低工资标准7.5元</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134773</th>\n",
       "      <td>19840032</td>\n",
       "      <td>2006-12-27</td>\n",
       "      <td>左小蕾:警惕股市系统性风险</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>905013 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id       date                  title\n",
       "0        59021767 2011-02-19           沪争取增值税扩围改革试点\n",
       "1        59021769 2011-02-19    周小川：外部施压不会影响人民币升值步伐\n",
       "2        59021771 2011-02-19     准备金率再上调 达到19.5％创新高\n",
       "4        59021774 2011-02-19    定基价格指数若涨20% 政府或出手调控\n",
       "5        59021776 2011-02-19  政策倾斜和加大投入 西藏将做大做强藏药产业\n",
       "...           ...        ...                    ...\n",
       "1134769  19839999 2006-12-27  定价基准不同 新利率基准将冲击旧浮动利率债\n",
       "1134770  19840000 2006-12-27    债市延续调整格局 投资者宜缩短投资久期\n",
       "1134771  19840023 2006-12-27           曾培炎:利用外汇储备优势\n",
       "1134772  19840026 2006-12-27         广州小时最低工资标准7.5元\n",
       "1134773  19840032 2006-12-27         左小蕾:警惕股市系统性风险 \n",
       "\n",
       "[905013 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jianbao=RAW['title'].str.match(r'.+\\d{4}年\\d{2}月\\d{2}日.+简报')\n",
    "RAW1=RAW.iloc[list(~jianbao),:]\n",
    "RAW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便起见，我们选取其中的前十条先进行分析："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59021767</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>沪争取增值税扩围改革试点</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59021769</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>周小川：外部施压不会影响人民币升值步伐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59021771</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>准备金率再上调 达到19.5％创新高</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59021774</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>定基价格指数若涨20% 政府或出手调控</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59021776</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>政策倾斜和加大投入 西藏将做大做强藏药产业</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59021778</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>新疆13个地州探矿权年 590个项目涉嫌“圈而不探”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>59021783</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>北京：楼市限购首日成交环比降9成</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>59021787</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>上海：房管局发布“沪九条”限购执行细则</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>59021789</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>6.5781！人民币升值容忍度继续提高？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>59021791</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>2010年上海商品住宅销售降四成</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id       date                       title\n",
       "0   59021767 2011-02-19                沪争取增值税扩围改革试点\n",
       "1   59021769 2011-02-19         周小川：外部施压不会影响人民币升值步伐\n",
       "2   59021771 2011-02-19          准备金率再上调 达到19.5％创新高\n",
       "4   59021774 2011-02-19         定基价格指数若涨20% 政府或出手调控\n",
       "5   59021776 2011-02-19       政策倾斜和加大投入 西藏将做大做强藏药产业\n",
       "6   59021778 2011-02-19  新疆13个地州探矿权年 590个项目涉嫌“圈而不探”\n",
       "8   59021783 2011-02-19            北京：楼市限购首日成交环比降9成\n",
       "9   59021787 2011-02-19         上海：房管局发布“沪九条”限购执行细则\n",
       "10  59021789 2011-02-19        6.5781！人民币升值容忍度继续提高？\n",
       "11  59021791 2011-02-19            2010年上海商品住宅销售降四成"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=RAW1.iloc[:10,:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['沪', '增值税', '扩围', '改革', '试点'],\n",
       " ['周小川', '外部', '施压', '影响', '人民币', '升值', '步伐'],\n",
       " ['准备金率', '上调', '19.5', '创新', '高'],\n",
       " ['定基', '价格指数', '若涨', '20%', '政府', '出手', '调控'],\n",
       " ['政策', '倾斜', '加大', '投入', '西藏', '做', '做', '强', '藏药', '产业'],\n",
       " ['新疆', '地州', '探矿权', '590', '项目', '涉嫌', '圈', '不探'],\n",
       " ['北京', '楼市', '限购', '首日', '成交', '环', '比降', '成'],\n",
       " ['上海', '房管局', '发布', '沪', '九条', '限购', '执行', '细则'],\n",
       " ['6.5781', '人民币', '升值', '容忍度', '提高'],\n",
       " ['2010', '上海', '商品住宅', '销售', '降', '四成']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 首先进行分词、去除停用词等\n",
    "import jieba\n",
    "\n",
    "with open('Chinese/stopword.txt','rt') as f:\n",
    "    stoplist=f.readlines()\n",
    "    stoplist=[w.replace('\\n','') for w in stoplist]\n",
    "\n",
    "def tokenize(w):\n",
    "    cut_w=jieba.cut(w)\n",
    "    ## 去除停用词\n",
    "    cut_w=[w.strip().lower() for w in cut_w if w not in stoplist and len(w.strip())>0]\n",
    "    return cut_w\n",
    "\n",
    "tokenized_data=map(tokenize,data['title'])\n",
    "tokenized_data=list(tokenized_data)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>沪</th>\n",
       "      <th>增值税</th>\n",
       "      <th>扩围</th>\n",
       "      <th>改革</th>\n",
       "      <th>试点</th>\n",
       "      <th>周小川</th>\n",
       "      <th>外部</th>\n",
       "      <th>施压</th>\n",
       "      <th>影响</th>\n",
       "      <th>人民币</th>\n",
       "      <th>...</th>\n",
       "      <th>执行</th>\n",
       "      <th>细则</th>\n",
       "      <th>6.5781</th>\n",
       "      <th>容忍度</th>\n",
       "      <th>提高</th>\n",
       "      <th>2010</th>\n",
       "      <th>商品住宅</th>\n",
       "      <th>销售</th>\n",
       "      <th>降</th>\n",
       "      <th>四成</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     沪  增值税   扩围   改革   试点  周小川   外部   施压   影响  人民币  ...   执行   细则  6.5781  \\\n",
       "0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  1.0  ...  0.0  0.0     0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "7  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0     0.0   \n",
       "8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0     1.0   \n",
       "9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "\n",
       "   容忍度   提高  2010  商品住宅   销售    降   四成  \n",
       "0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "5  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "6  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "7  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "8  1.0  1.0   0.0   0.0  0.0  0.0  0.0  \n",
       "9  0.0  0.0   1.0   1.0  1.0  1.0  1.0  \n",
       "\n",
       "[10 rows x 63 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 接下来进行词频统计\n",
    "\n",
    "def word_freq(wlist):\n",
    "    freq={}\n",
    "    for w in wlist:\n",
    "        if w in freq:\n",
    "            freq[w]+=1\n",
    "        else:\n",
    "            freq[w]=1\n",
    "            \n",
    "    return freq\n",
    "freqs=map(word_freq,tokenized_data)\n",
    "## 放在pandas里\n",
    "pd_freqs=pd.DataFrame(freqs)\n",
    "## 把NaN换成0\n",
    "pd_freqs=pd_freqs.fillna(0)\n",
    "pd_freqs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如此，一个简单的词袋模型就完成了。\n",
    "\n",
    "不过，可以看到虽然我们只有10条新闻，这个矩阵已经很大了。如果10万条新闻一起来，不仅词（列）多，而且行也多，最终这个矩阵的规模会变的非常巨大，甚至可能很轻易的会占满内存。\n",
    "\n",
    "然而注意到，这个矩阵里面多数的值都是0，这种类型的矩阵我们成为**稀疏矩阵**（**sparse matrix**），在SciPy和Pandas里面都提供了稀疏矩阵的存储结构和运算，所以更好的办法是使用稀疏矩阵进行存储："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>沪</th>\n",
       "      <th>增值税</th>\n",
       "      <th>扩围</th>\n",
       "      <th>改革</th>\n",
       "      <th>试点</th>\n",
       "      <th>周小川</th>\n",
       "      <th>外部</th>\n",
       "      <th>施压</th>\n",
       "      <th>影响</th>\n",
       "      <th>人民币</th>\n",
       "      <th>...</th>\n",
       "      <th>执行</th>\n",
       "      <th>细则</th>\n",
       "      <th>6.5781</th>\n",
       "      <th>容忍度</th>\n",
       "      <th>提高</th>\n",
       "      <th>2010</th>\n",
       "      <th>商品住宅</th>\n",
       "      <th>销售</th>\n",
       "      <th>降</th>\n",
       "      <th>四成</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     沪  增值税   扩围   改革   试点  周小川   外部   施压   影响  人民币  ...   执行   细则  6.5781  \\\n",
       "0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  1.0  ...  0.0  0.0     0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "7  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0     0.0   \n",
       "8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0     1.0   \n",
       "9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "\n",
       "   容忍度   提高  2010  商品住宅   销售    降   四成  \n",
       "0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "5  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "6  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "7  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "8  1.0  1.0   0.0   0.0  0.0  0.0  0.0  \n",
       "9  0.0  0.0   1.0   1.0  1.0  1.0  1.0  \n",
       "\n",
       "[10 rows x 63 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_freqs=pd_freqs.astype(pd.SparseDtype(\"float\", 0))\n",
    "sparse_freqs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的意思是通过类型转换，把数据框转换为稀疏类型，其中的「0」就不存储了，虽然看起来没有变化，但是如果我们查看类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "沪       Sparse[float64, 0]\n",
       "增值税     Sparse[float64, 0]\n",
       "扩围      Sparse[float64, 0]\n",
       "改革      Sparse[float64, 0]\n",
       "试点      Sparse[float64, 0]\n",
       "               ...        \n",
       "2010    Sparse[float64, 0]\n",
       "商品住宅    Sparse[float64, 0]\n",
       "销售      Sparse[float64, 0]\n",
       "降       Sparse[float64, 0]\n",
       "四成      Sparse[float64, 0]\n",
       "Length: 63, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_freqs.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "会发现都变成了Sparse的float64类型，且0不存储。\n",
    "\n",
    "当然，以上是自己手写的词袋生成步骤，实际上很多自然语言处理包已经有比较成熟的词袋处理机制。比如在Scikit-Learn中已经准备好了词袋的提取函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['##', 'a', 'ain', 'aren', 'c', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'i', 'isn', 'lex', 'll', 'm', 'mon', 'null', 's', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn', '±', '÷', 'β', 'δ', 'λ', 'ξ', 'ψ', 'в', '′', '″', 'ⅲ', '∈', '∧', '∪', '─', '☆', '一会', '一关', '一城', '一堆', '一对', '一批', '一方', '一期', '一村', '一根', '一派', '一班', '一百', '一眼', '一科', '一群', '一遍', '一道', '一部', '一集', '一页', '一颗', '三鲜', '为什', '什', '倒', '傥', '元', '元素', '先', '关', '兼', '前', '单元', '吨', '唷', '啪', '啷', '喔', '喜欢', '外', '多年', '大节', '大道', '大面儿', '天', '始', '子弹', '後', '抗拒', '敞开', '新', '昉', '更远', '有意', '有趣', '末', '次', '毫无保留', '波', '漫', '特', '特别', '理', '皆', '目前为止', '笑', '第三', '第五', '第四', '策略', '讲', '设', '话', '说', '赶早', '赶晚', '达', '钱', '限', '非', '面', '题', '麽', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｆ', 'ｇ', 'ｈ', 'ｉ', 'ｊ', 'ｌ', 'ｎ', 'ｏ', 'ｒ', 'ｔ', 'ｘ', 'ｚ'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<10x64 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 72 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "with open('Chinese/stopword.txt','rt') as f:\n",
    "    stoplist=f.readlines()\n",
    "    stoplist=[w.replace('\\n','') for w in stoplist]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect=CountVectorizer(tokenizer=jieba.cut, stop_words=stoplist, min_df=1)\n",
    "bag_words=count_vect.fit_transform(data['title'])\n",
    "words_names=count_vect.get_feature_names()\n",
    "bag_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意由于Scikit-Learn中一般只支持英文的自动分词（tokenize），为了让他能够处理中文的分词，我们把jieba.cut函数提交给了CountVectorizer，此外还额外提供了停用词列表。min_df选项设置了如果在所有文本中某个词出现的频率下线，如果出现太少则会被忽略，适当提高这个选项可以降低维数。\n",
    "\n",
    "上面的词袋结果是一个sparse matrix，实际上是SciPy中的系数矩阵形式，实际分析时已经可以使用。不过为了查看方便，我们不妨将其转换为Pandas的数据框（稀疏存储）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>19.5</th>\n",
       "      <th>20%</th>\n",
       "      <th>2010</th>\n",
       "      <th>590</th>\n",
       "      <th>6.5781</th>\n",
       "      <th>上海</th>\n",
       "      <th>上调</th>\n",
       "      <th>不探</th>\n",
       "      <th>九条</th>\n",
       "      <th>...</th>\n",
       "      <th>藏药</th>\n",
       "      <th>西藏</th>\n",
       "      <th>试点</th>\n",
       "      <th>调控</th>\n",
       "      <th>销售</th>\n",
       "      <th>降</th>\n",
       "      <th>限购</th>\n",
       "      <th>项目</th>\n",
       "      <th>首日</th>\n",
       "      <th>高</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      19.5  20%  2010  590  6.5781  上海  上调  不探  九条  ...  藏药  西藏  试点  调控  销售  \\\n",
       "0  0     0    0     0    0       0   0   0   0   0  ...   0   0   1   0   0   \n",
       "1  0     0    0     0    0       0   0   0   0   0  ...   0   0   0   0   0   \n",
       "2  1     1    0     0    0       0   0   1   0   0  ...   0   0   0   0   0   \n",
       "3  1     0    1     0    0       0   0   0   0   0  ...   0   0   0   1   0   \n",
       "4  1     0    0     0    0       0   0   0   0   0  ...   1   1   0   0   0   \n",
       "5  1     0    0     0    1       0   0   0   1   0  ...   0   0   0   0   0   \n",
       "6  0     0    0     0    0       0   0   0   0   0  ...   0   0   0   0   0   \n",
       "7  0     0    0     0    0       0   1   0   0   1  ...   0   0   0   0   0   \n",
       "8  0     0    0     0    0       1   0   0   0   0  ...   0   0   0   0   0   \n",
       "9  0     0    0     1    0       0   1   0   0   0  ...   0   0   0   0   1   \n",
       "\n",
       "   降  限购  项目  首日  高  \n",
       "0  0   0   0   0  0  \n",
       "1  0   0   0   0  0  \n",
       "2  0   0   0   0  1  \n",
       "3  0   0   0   0  0  \n",
       "4  0   0   0   0  0  \n",
       "5  0   0   1   0  0  \n",
       "6  0   1   0   1  0  \n",
       "7  0   1   0   0  0  \n",
       "8  0   0   0   0  0  \n",
       "9  1   0   0   0  0  \n",
       "\n",
       "[10 rows x 64 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_words_df=pd.DataFrame.sparse.from_spmatrix(bag_words, columns=words_names)\n",
    "bag_words_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也许你会觉着不舒服的是，分词和去除停用词等操作应该是第二步清洗数据完成的，现在如果都放到向量化的对象CountVectorizer中来，非常不灵活。\n",
    "\n",
    "比如显然上面的结果中，「13，19.5，20%」等都不是我们想要的，然而用停用词根本不可能将这些数字去除。\n",
    "\n",
    "那么如何将两者（手工清洗+自动计算词袋）结合起来呢？其实很简单，用空格将他们join起来就好了。比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['沪 增值税 扩围 改革 试点',\n",
       " '周小川 外部 施压 影响 人民币 升值 步伐',\n",
       " '准备金率 上调 创新 高',\n",
       " '定基 价格指数 若涨 政府 出手 调控',\n",
       " '政策 倾斜 加大 投入 西藏 做 做 强 藏药 产业',\n",
       " '新疆 地州 探矿权 项目 涉嫌 圈 不探',\n",
       " '北京 楼市 限购 首日 成交 环 比降 成',\n",
       " '上海 房管局 发布 沪 九条 限购 执行 细则',\n",
       " '人民币 升值 容忍度 提高',\n",
       " '上海 商品住宅 销售 降 四成']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "import re\n",
    "\n",
    "with open('Chinese/stopword.txt','rt') as f:\n",
    "    stoplist=f.readlines()\n",
    "    stoplist=[w.replace('\\n','') for w in stoplist]\n",
    "\n",
    "def not_digit(w):\n",
    "    w=w.replace(',','')\n",
    "    if re.match(r'\\d+',w)!=None or re.match(r'\\d%',w)!=None or re.match(r'\\d*\\.\\d+',w)!=None:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def tokenize(w):\n",
    "    cut_w=jieba.cut(w)\n",
    "    ## 去除停用词\n",
    "    cut_w=[w.strip().lower() for w in cut_w if ((w not in stoplist) and not_digit(w) and len(w.strip())>0)]\n",
    "    return cut_w\n",
    "\n",
    "tokenized_data=map(tokenize,data['title'])\n",
    "tokenized_data=[' '.join(t) for t in tokenized_data]\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>上海</th>\n",
       "      <th>上调</th>\n",
       "      <th>不探</th>\n",
       "      <th>九条</th>\n",
       "      <th>产业</th>\n",
       "      <th>人民币</th>\n",
       "      <th>价格指数</th>\n",
       "      <th>倾斜</th>\n",
       "      <th>准备金率</th>\n",
       "      <th>出手</th>\n",
       "      <th>...</th>\n",
       "      <th>细则</th>\n",
       "      <th>若涨</th>\n",
       "      <th>藏药</th>\n",
       "      <th>西藏</th>\n",
       "      <th>试点</th>\n",
       "      <th>调控</th>\n",
       "      <th>销售</th>\n",
       "      <th>限购</th>\n",
       "      <th>项目</th>\n",
       "      <th>首日</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   上海  上调  不探  九条  产业  人民币  价格指数  倾斜  准备金率  出手  ...  细则  若涨  藏药  西藏  试点  调控  \\\n",
       "0   0   0   0   0   0    0     0   0     0   0  ...   0   0   0   0   1   0   \n",
       "1   0   0   0   0   0    1     0   0     0   0  ...   0   0   0   0   0   0   \n",
       "2   0   1   0   0   0    0     0   0     1   0  ...   0   0   0   0   0   0   \n",
       "3   0   0   0   0   0    0     1   0     0   1  ...   0   1   0   0   0   1   \n",
       "4   0   0   0   0   1    0     0   1     0   0  ...   0   0   1   1   0   0   \n",
       "5   0   0   1   0   0    0     0   0     0   0  ...   0   0   0   0   0   0   \n",
       "6   0   0   0   0   0    0     0   0     0   0  ...   0   0   0   0   0   0   \n",
       "7   1   0   0   1   0    0     0   0     0   0  ...   1   0   0   0   0   0   \n",
       "8   0   0   0   0   0    1     0   0     0   0  ...   0   0   0   0   0   0   \n",
       "9   1   0   0   0   0    0     0   0     0   0  ...   0   0   0   0   0   0   \n",
       "\n",
       "   销售  限购  项目  首日  \n",
       "0   0   0   0   0  \n",
       "1   0   0   0   0  \n",
       "2   0   0   0   0  \n",
       "3   0   0   0   0  \n",
       "4   0   0   0   0  \n",
       "5   0   0   1   0  \n",
       "6   0   1   0   1  \n",
       "7   0   1   0   0  \n",
       "8   0   0   0   0  \n",
       "9   1   0   0   0  \n",
       "\n",
       "[10 rows x 50 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect=CountVectorizer()\n",
    "bag_words=count_vect.fit_transform(tokenized_data)\n",
    "words_names=count_vect.get_feature_names()\n",
    "bag_words_df=pd.DataFrame.sparse.from_spmatrix(bag_words, columns=words_names)\n",
    "bag_words_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，更优雅的方法是直接将手写的tokenize函数调入即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>上海</th>\n",
       "      <th>上调</th>\n",
       "      <th>不探</th>\n",
       "      <th>九条</th>\n",
       "      <th>产业</th>\n",
       "      <th>人民币</th>\n",
       "      <th>价格指数</th>\n",
       "      <th>倾斜</th>\n",
       "      <th>做</th>\n",
       "      <th>准备金率</th>\n",
       "      <th>...</th>\n",
       "      <th>藏药</th>\n",
       "      <th>西藏</th>\n",
       "      <th>试点</th>\n",
       "      <th>调控</th>\n",
       "      <th>销售</th>\n",
       "      <th>降</th>\n",
       "      <th>限购</th>\n",
       "      <th>项目</th>\n",
       "      <th>首日</th>\n",
       "      <th>高</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   上海  上调  不探  九条  产业  人民币  价格指数  倾斜  做  准备金率  ...  藏药  西藏  试点  调控  销售  降  限购  \\\n",
       "0   0   0   0   0   0    0     0   0  0     0  ...   0   0   1   0   0  0   0   \n",
       "1   0   0   0   0   0    1     0   0  0     0  ...   0   0   0   0   0  0   0   \n",
       "2   0   1   0   0   0    0     0   0  0     1  ...   0   0   0   0   0  0   0   \n",
       "3   0   0   0   0   0    0     1   0  0     0  ...   0   0   0   1   0  0   0   \n",
       "4   0   0   0   0   1    0     0   1  2     0  ...   1   1   0   0   0  0   0   \n",
       "5   0   0   1   0   0    0     0   0  0     0  ...   0   0   0   0   0  0   0   \n",
       "6   0   0   0   0   0    0     0   0  0     0  ...   0   0   0   0   0  0   1   \n",
       "7   1   0   0   1   0    0     0   0  0     0  ...   0   0   0   0   0  0   1   \n",
       "8   0   0   0   0   0    1     0   0  0     0  ...   0   0   0   0   0  0   0   \n",
       "9   1   0   0   0   0    0     0   0  0     0  ...   0   0   0   0   1  1   0   \n",
       "\n",
       "   项目  首日  高  \n",
       "0   0   0  0  \n",
       "1   0   0  0  \n",
       "2   0   0  1  \n",
       "3   0   0  0  \n",
       "4   0   0  0  \n",
       "5   1   0  0  \n",
       "6   0   1  0  \n",
       "7   0   0  0  \n",
       "8   0   0  0  \n",
       "9   0   0  0  \n",
       "\n",
       "[10 rows x 58 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect=CountVectorizer(tokenizer=tokenize)\n",
    "bag_words=count_vect.fit_transform(tokenized_data)\n",
    "words_names=count_vect.get_feature_names()\n",
    "bag_words_df=pd.DataFrame.sparse.from_spmatrix(bag_words, columns=words_names)\n",
    "bag_words_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上以上的词袋又被称为1元词袋，是N元（N-gram）词袋的一种特例。我们当然可以做成二元词袋："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>上海 商品住宅</th>\n",
       "      <th>上海 房管局</th>\n",
       "      <th>上调 创新</th>\n",
       "      <th>九条 限购</th>\n",
       "      <th>人民币 升值</th>\n",
       "      <th>价格指数 若涨</th>\n",
       "      <th>倾斜 加大</th>\n",
       "      <th>做 做</th>\n",
       "      <th>做 强</th>\n",
       "      <th>准备金率 上调</th>\n",
       "      <th>...</th>\n",
       "      <th>环 比降</th>\n",
       "      <th>若涨 政府</th>\n",
       "      <th>藏药 产业</th>\n",
       "      <th>西藏 做</th>\n",
       "      <th>销售 降</th>\n",
       "      <th>降 四成</th>\n",
       "      <th>限购 执行</th>\n",
       "      <th>限购 首日</th>\n",
       "      <th>项目 涉嫌</th>\n",
       "      <th>首日 成交</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   上海 商品住宅  上海 房管局  上调 创新  九条 限购  人民币 升值  价格指数 若涨  倾斜 加大  做 做  做 强  准备金率 上调  \\\n",
       "0        0       0      0      0       0        0      0    0    0        0   \n",
       "1        0       0      0      0       1        0      0    0    0        0   \n",
       "2        0       0      1      0       0        0      0    0    0        1   \n",
       "3        0       0      0      0       0        1      0    0    0        0   \n",
       "4        0       0      0      0       0        0      1    1    1        0   \n",
       "5        0       0      0      0       0        0      0    0    0        0   \n",
       "6        0       0      0      0       0        0      0    0    0        0   \n",
       "7        0       1      0      1       0        0      0    0    0        0   \n",
       "8        0       0      0      0       1        0      0    0    0        0   \n",
       "9        1       0      0      0       0        0      0    0    0        0   \n",
       "\n",
       "   ...  环 比降  若涨 政府  藏药 产业  西藏 做  销售 降  降 四成  限购 执行  限购 首日  项目 涉嫌  首日 成交  \n",
       "0  ...     0      0      0     0     0     0      0      0      0      0  \n",
       "1  ...     0      0      0     0     0     0      0      0      0      0  \n",
       "2  ...     0      0      0     0     0     0      0      0      0      0  \n",
       "3  ...     0      1      0     0     0     0      0      0      0      0  \n",
       "4  ...     0      0      1     1     0     0      0      0      0      0  \n",
       "5  ...     0      0      0     0     0     0      0      0      1      0  \n",
       "6  ...     1      0      0     0     0     0      0      1      0      1  \n",
       "7  ...     0      0      0     0     0     0      1      0      0      0  \n",
       "8  ...     0      0      0     0     0     0      0      0      0      0  \n",
       "9  ...     0      0      0     0     1     1      0      0      0      0  \n",
       "\n",
       "[10 rows x 53 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect=CountVectorizer(tokenizer=tokenize, ngram_range=(2,2))\n",
    "bag_words=count_vect.fit_transform(data['title'])\n",
    "words_names=count_vect.get_feature_names()\n",
    "bag_words_df=pd.DataFrame.sparse.from_spmatrix(bag_words, columns=words_names)\n",
    "bag_words_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中ngram_range选项给出了一个区间，如果是(1,2)那么就是1元、2元词袋同时存在，而(2,2)代表仅使用2元词袋，(1,1)为默认，即只使用一元词袋。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF模型\n",
    "\n",
    "词袋模型简单有效，但是哟一个缺点，即只考虑了词的绝对频率，而没有考虑相对频率。比如有的词天然的出现频率更高，那么在每个文档里面，其重要性应该是更低的。\n",
    "\n",
    "而TF-IDF模型就是在词袋的基础上修正这一点。TF-IDF模型是两个度量的乘积：$$TFIDF=TF\\times IDF$$其中TF即词频，而IDF为逆文档频率，IDF的定义为：$$IDF\\left(w\\right)=1+\\ln\\left(\\frac{N}{1+df\\left(w\\right)}\\right)$$其中$N$为文档总数量，而$df\\left(w\\right)$为包含单词$w$的文档个数。\n",
    "\n",
    "可以看到根据上面的定义，一个单词$w$如果出现的文档越多，那么其$IDF\\left(w\\right)$值就越小，或者说权重就越小。\n",
    "\n",
    "最终，我们将上面词袋中每个文档每个词的词频（$TF_i\\left(w\\right),i=1,...,N$）乘以权重就得到了TF-IDF值：$$TFIDF_i\\left(w\\right)=TF_i\\left(w\\right)\\times IDF\\left(w\\right),i=1,...,N$$\n",
    "\n",
    "最后，对每个文档，还需要将以上得到的向量进行标准化，一般使用$L2$范数进行标准化（从而每个向量都在$M$维单位球上，$M$为词的个数）：$$NormalizedTFIDF_i\\left(w\\right)=\\frac{TFIDF_i\\left(w\\right)}{\\sqrt{\\sum_{k=1}^{M}\\left[TFIDF_i\\left(w\\right)\\right]^2}},i=1,...,N$$\n",
    "\n",
    "比如，对于之前计算的词袋："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>上海</th>\n",
       "      <th>上调</th>\n",
       "      <th>不探</th>\n",
       "      <th>九条</th>\n",
       "      <th>产业</th>\n",
       "      <th>人民币</th>\n",
       "      <th>价格指数</th>\n",
       "      <th>倾斜</th>\n",
       "      <th>做</th>\n",
       "      <th>准备金率</th>\n",
       "      <th>...</th>\n",
       "      <th>藏药</th>\n",
       "      <th>西藏</th>\n",
       "      <th>试点</th>\n",
       "      <th>调控</th>\n",
       "      <th>销售</th>\n",
       "      <th>降</th>\n",
       "      <th>限购</th>\n",
       "      <th>项目</th>\n",
       "      <th>首日</th>\n",
       "      <th>高</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   上海  上调  不探  九条  产业  人民币  价格指数  倾斜  做  准备金率  ...  藏药  西藏  试点  调控  销售  降  限购  \\\n",
       "0   0   0   0   0   0    0     0   0  0     0  ...   0   0   1   0   0  0   0   \n",
       "1   0   0   0   0   0    1     0   0  0     0  ...   0   0   0   0   0  0   0   \n",
       "2   0   1   0   0   0    0     0   0  0     1  ...   0   0   0   0   0  0   0   \n",
       "3   0   0   0   0   0    0     1   0  0     0  ...   0   0   0   1   0  0   0   \n",
       "4   0   0   0   0   1    0     0   1  2     0  ...   1   1   0   0   0  0   0   \n",
       "5   0   0   1   0   0    0     0   0  0     0  ...   0   0   0   0   0  0   0   \n",
       "6   0   0   0   0   0    0     0   0  0     0  ...   0   0   0   0   0  0   1   \n",
       "7   1   0   0   1   0    0     0   0  0     0  ...   0   0   0   0   0  0   1   \n",
       "8   0   0   0   0   0    1     0   0  0     0  ...   0   0   0   0   0  0   0   \n",
       "9   1   0   0   0   0    0     0   0  0     0  ...   0   0   0   0   1  1   0   \n",
       "\n",
       "   项目  首日  高  \n",
       "0   0   0  0  \n",
       "1   0   0  0  \n",
       "2   0   0  1  \n",
       "3   0   0  0  \n",
       "4   0   0  0  \n",
       "5   1   0  0  \n",
       "6   0   1  0  \n",
       "7   0   0  0  \n",
       "8   0   0  0  \n",
       "9   0   0  0  \n",
       "\n",
       "[10 rows x 58 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect=CountVectorizer(tokenizer=tokenize)\n",
    "bag_words=count_vect.fit_transform(tokenized_data)\n",
    "words_names=count_vect.get_feature_names()\n",
    "bag_words_df=pd.DataFrame.sparse.from_spmatrix(bag_words, columns=words_names)\n",
    "bag_words_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以计算「上海」这个词在两个文档中出现，从而其$IDF=1+\\ln\\left(10\\right)-\\ln\\left(1+2\\right)=2.20397$，而「做」这个词只有一个文档出现，从而其$IDF=1+\\ln\\left(10\\right)-\\ln\\left(1+1\\right)=2.60944$。\n",
    "\n",
    "以上计算略显复杂，不过Scikit-Learn中也给出了方便的计算函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>上海</th>\n",
       "      <th>上调</th>\n",
       "      <th>不探</th>\n",
       "      <th>九条</th>\n",
       "      <th>产业</th>\n",
       "      <th>人民币</th>\n",
       "      <th>价格指数</th>\n",
       "      <th>倾斜</th>\n",
       "      <th>做</th>\n",
       "      <th>准备金率</th>\n",
       "      <th>...</th>\n",
       "      <th>藏药</th>\n",
       "      <th>西藏</th>\n",
       "      <th>试点</th>\n",
       "      <th>调控</th>\n",
       "      <th>销售</th>\n",
       "      <th>降</th>\n",
       "      <th>限购</th>\n",
       "      <th>项目</th>\n",
       "      <th>首日</th>\n",
       "      <th>高</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359846</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.317517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.37351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.457985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.391176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460158</td>\n",
       "      <td>0.460158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         上海   上调        不探       九条        产业       人民币      价格指数        倾斜  \\\n",
       "0  0.000000  0.0  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.0  0.000000  0.00000  0.000000  0.334845  0.000000  0.000000   \n",
       "2  0.000000  0.5  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.0  0.000000  0.00000  0.000000  0.000000  0.408248  0.000000   \n",
       "4  0.000000  0.0  0.000000  0.00000  0.288675  0.000000  0.000000  0.288675   \n",
       "5  0.000000  0.0  0.377964  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.0  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.317517  0.0  0.000000  0.37351  0.000000  0.000000  0.000000  0.000000   \n",
       "8  0.000000  0.0  0.000000  0.00000  0.000000  0.457985  0.000000  0.000000   \n",
       "9  0.391176  0.0  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         做  准备金率  ...        藏药        西藏        试点        调控        销售  \\\n",
       "0  0.00000   0.0  ...  0.000000  0.000000  0.460158  0.000000  0.000000   \n",
       "1  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.00000   0.5  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.408248  0.000000   \n",
       "4  0.57735   0.0  ...  0.288675  0.288675  0.000000  0.000000  0.000000   \n",
       "5  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.460158   \n",
       "\n",
       "          降        限购        项目        首日    高  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.0  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.0  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.5  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.0  \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.0  \n",
       "5  0.000000  0.000000  0.377964  0.000000  0.0  \n",
       "6  0.000000  0.305902  0.000000  0.359846  0.0  \n",
       "7  0.000000  0.317517  0.000000  0.000000  0.0  \n",
       "8  0.000000  0.000000  0.000000  0.000000  0.0  \n",
       "9  0.460158  0.000000  0.000000  0.000000  0.0  \n",
       "\n",
       "[10 rows x 58 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer=TfidfTransformer(norm='l2').fit(bag_words) ##使用L2范数，并fit模型（如计算IDF等）\n",
    "tfidf_words=tfidf_transformer.transform(bag_words) ##变换数据\n",
    "tfidf_words_df=pd.DataFrame.sparse.from_spmatrix(tfidf_words, columns=words_names)\n",
    "tfidf_words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于Scikit-Learn中词袋、TF-IDF计算的具体文档，可以查看：https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词嵌入\n",
    "\n",
    "上面的词袋模型和TF-IDF模型都非常直观，但是有一个缺点，就是维数非常高。我们仅仅使用了10条新闻标题，就得到了59列特征，虽然在存储和计算上我们可以使用稀疏矩阵，但是在分析中，维数太高的模型分析起来总是比较困难的。而**词嵌入**（**word embedding**）的出现很大程度上缓解了这个问题。\n",
    "\n",
    "所谓词嵌入，实际上是把高维空间的向量向低维空间映射的过程。一个经典的算法是谷歌提出的word2vec算法，关于该算法的原理我们再次不再赘述，我们这里主要通过例子来展示如何使用该方法。\n",
    "\n",
    "Python中可以使用Gensim包实现word2vec算法，在使用之前需要先安装：\n",
    "\n",
    "```shell\n",
    "sudo pip3 install gensim\n",
    "```\n",
    "\n",
    "该算法需要提供如下几个信息：\n",
    "\n",
    "* 语料库，对于中文可以提交已经分好词的语料库\n",
    "* window，窗宽，上下文可以联系起来的单词个数\n",
    "* size，输出的词向量的维度，几十到几千都可以\n",
    "* min_count，只有当某个词出现次数大于该数值时才会被加入到模型中。\n",
    "\n",
    "比如，使用以上新闻标题数据，可以训练如下模型：\n",
    "\n",
    "```python\n",
    "import gensim\n",
    "\n",
    "CORPUS=map(tokenize,RAW1['title'])\n",
    "w2v_model=gensim.models.Word2Vec(CORPUS, window=5, size=10, min_count=5)\n",
    "w2v_model.save('word2vec')##后面节省时间，先把模型保存下来\n",
    "```\n",
    "\n",
    "当然在现实中，有的时候我们也会使用其他人已经预训练的模型。其实在本例中，只使用标题信息训练出的模型精度并不好（比如标题中使用了沪就不会使用上海，因而很难侦测到这种相关性）。\n",
    "\n",
    "我们使用1992年到2016年的CSMAR上式公司新闻全文数据训练了不同维度的词向量，保存在“Chinese/word2vec/”文件夹中，其中\"word2vec.py\"为训练代码，由于数据量太大（共有74885004个词汇参与训练），我们不提供该数据，如有需要可以从CSMAR下载。此外，\"word2vec**\"位已经训练好的词向量模型，其中\\*\\*为词向量的维度，我们训练了30、100、300、500等不同维度。如果需要导入预训练的模型，可以直接使用load函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v_model=gensim.models.Word2Vec.load('./Chinese/word2vec/word2vec30')##节省时间，直接导入预训练的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型的具体语法和解释可以查看：https://radimrehurek.com/gensim/models/word2vec.html#module-gensim.models.word2vec\n",
    "\n",
    "有了模型后，我们可以查看每个词对应的向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "贵州茅台：\n",
      " [-2.6204739  -0.23999904 -0.4501495  -4.7471104  -2.6559854   6.1215234\n",
      " -1.19105     0.08704755  4.4575567  -5.9783154   1.6476333  -2.9682255\n",
      "  0.05026169 -2.8163173   3.1344664  -2.513116   -2.3200655  -6.645488\n",
      "  1.7873464  -0.28822428 -0.41796866  5.3326087  -1.7870761   0.31191105\n",
      " -2.4249952  -8.8343725   0.67447054  1.6748495  -2.1062326  -5.243416  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"贵州茅台：\\n\",w2v_model.wv['贵州茅台'])\n",
    "w2v_model.wv['贵州茅台'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以查看跟某些词最相关的词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上海： [('广州', 0.8821085095405579), ('北京', 0.8780174851417542), ('深圳', 0.8698581457138062), ('天津', 0.8337385654449463), ('上海浦东', 0.8273128271102905), ('浦东', 0.7818915843963623), ('成都', 0.7796352505683899), ('虹口', 0.768273651599884), ('重庆', 0.7635039687156677), ('穗三大', 0.7591032385826111)]\n",
      "贵州茅台： [('洋河股份', 0.9588034152984619), ('山西汾酒', 0.9121654033660889), ('张裕a', 0.9066104888916016), ('g茅台', 0.9013228416442871), ('古井贡酒', 0.8815902471542358), ('伊利股份', 0.8797951936721802), ('金种子酒', 0.8770319223403931), ('老白干酒', 0.8755484223365784), ('泸州老窖', 0.8753379583358765), ('g五粮液', 0.8694736361503601)]\n"
     ]
    }
   ],
   "source": [
    "print(\"上海：\",w2v_model.wv.most_similar(positive=['上海']))\n",
    "print(\"贵州茅台：\",w2v_model.wv.most_similar(positive=['贵州茅台']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者跟某个词的向量的相反数最相关的（与$-1\\times$上海 最相关的）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上海： [('仅当', 0.6410440802574158), ('恕', 0.62154620885849), ('杜晓锋', 0.6094518899917603), ('幸敬华', 0.6002947688102722), ('毛惟德', 0.5936871767044067), ('蓝保湾', 0.593168318271637), ('五年制', 0.5928232669830322), ('资本保全', 0.585964560508728), ('杜干', 0.5858868956565857), ('质量规划', 0.5837689638137817)]\n",
      "贵州茅台： [('先筑底', 0.7058587074279785), ('法定清算', 0.6609488725662231), ('经援', 0.6546812057495117), ('下除', 0.6526764631271362), ('动产担保', 0.6491248607635498), ('外部边界', 0.6484469175338745), ('混合贷款', 0.6440901756286621), ('国际金融组织贷款', 0.6322882771492004), ('所在国', 0.629910409450531), ('区域信用', 0.6297281384468079)]\n"
     ]
    }
   ],
   "source": [
    "print(\"上海：\",w2v_model.wv.most_similar(negative=['上海']))\n",
    "print(\"贵州茅台：\",w2v_model.wv.most_similar(negative=['贵州茅台']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者查看：上海+杭州-北京=？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上海+杭州-北京=\n",
      "[('宁波', 0.8586423397064209), ('广州', 0.8504917621612549), ('东莞', 0.822361409664154), ('南京', 0.8192952871322632), ('厦门', 0.8146063685417175), ('佛山', 0.8060692548751831), ('南一', 0.7999211549758911), ('溧阳', 0.7996322512626648), ('成都', 0.7981355786323547), ('浙江', 0.7962439656257629)]\n",
      "上海+北京-杭州=\n",
      "[('上海浦东', 0.7623594403266907), ('上海综合保税区', 0.7335734367370605), ('深圳', 0.7314644455909729), ('e-cbd', 0.7249362468719482), ('京', 0.7226858139038086), ('辐射式', 0.7162173986434937), ('浦东', 0.7120426297187805), ('三地', 0.7035840749740601), ('徐雯', 0.7003940343856812), ('盛汇', 0.6990733742713928)]\n"
     ]
    }
   ],
   "source": [
    "print(\"上海+杭州-北京=\\n%s\" % w2v_model.wv.most_similar(positive=['上海','杭州'],negative=['北京']))\n",
    "print(\"上海+北京-杭州=\\n%s\" % w2v_model.wv.most_similar(positive=['上海','北京'],negative=['杭州']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，直接看两个词的相似度（词向量的相关系数）也是可以的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上海,北京的相似度= 0.8780\n",
      "上海,日本的相似度= -0.0521\n",
      "北京,日本的相似度= 0.5247\n",
      "上海,人民币的相似度= 0.1460\n",
      "人民币,北京的相似度= 0.0441\n",
      "贵州茅台,比亚迪的相似度= 0.3611\n",
      "吉利汽车,沃尔沃的相似度= 0.8491\n",
      "华为,中兴的相似度= 0.8539\n"
     ]
    }
   ],
   "source": [
    "print(\"%s,%s的相似度= %.4f\"%(\"上海\",\"北京\",w2v_model.wv.similarity(\"上海\",\"北京\")))\n",
    "print(\"%s,%s的相似度= %.4f\"%(\"上海\",\"日本\",w2v_model.wv.similarity(\"上海\",\"日本\")))\n",
    "print(\"%s,%s的相似度= %.4f\"%(\"北京\",\"日本\",w2v_model.wv.similarity(\"医药\",\"医疗\")))\n",
    "print(\"%s,%s的相似度= %.4f\"%(\"上海\",\"人民币\",w2v_model.wv.similarity(\"上海\",\"人民币\")))\n",
    "print(\"%s,%s的相似度= %.4f\"%(\"人民币\",\"北京\",w2v_model.wv.similarity(\"人民币\",\"北京\")))\n",
    "print(\"%s,%s的相似度= %.4f\"%(\"贵州茅台\",\"比亚迪\",w2v_model.wv.similarity(\"贵州茅台\",\"比亚迪\")))\n",
    "print(\"%s,%s的相似度= %.4f\"%(\"吉利汽车\",\"沃尔沃\",w2v_model.wv.similarity(\"吉利汽车\",\"沃尔沃\")))\n",
    "print(\"%s,%s的相似度= %.4f\"%(\"华为\",\"中兴\",w2v_model.wv.similarity(\"华为\",\"中兴\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而，以上算法仅仅计算了每个词的向量，我们数据中的却是文档，因而需要将词向量加总成文档向量。为此，我们可以通过平均的方式求文档的向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['沪', '增值税', '扩围', '改革', '试点'],\n",
       " ['周小川', '外部', '施压', '影响', '人民币', '升值', '步伐'],\n",
       " ['准备金率', '上调', '创新', '高'],\n",
       " ['定基', '价格指数', '若涨', '政府', '出手', '调控'],\n",
       " ['政策', '倾斜', '加大', '投入', '西藏', '做', '做', '强', '藏药', '产业'],\n",
       " ['新疆', '地州', '探矿权', '项目', '涉嫌', '圈', '不探'],\n",
       " ['北京', '楼市', '限购', '首日', '成交', '环', '比降', '成'],\n",
       " ['上海', '房管局', '发布', '沪', '九条', '限购', '执行', '细则'],\n",
       " ['人民币', '升值', '容忍度', '提高'],\n",
       " ['上海', '商品住宅', '销售', '降', '四成']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data=map(tokenize,data['title'])\n",
    "tokenized_data=list(tokenized_data)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.297971</td>\n",
       "      <td>-3.161334</td>\n",
       "      <td>2.866893</td>\n",
       "      <td>-0.996126</td>\n",
       "      <td>0.284920</td>\n",
       "      <td>1.531972</td>\n",
       "      <td>0.906654</td>\n",
       "      <td>-4.272559</td>\n",
       "      <td>1.360739</td>\n",
       "      <td>-7.145058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618608</td>\n",
       "      <td>-2.027417</td>\n",
       "      <td>0.354830</td>\n",
       "      <td>-5.920053</td>\n",
       "      <td>2.606006</td>\n",
       "      <td>0.943722</td>\n",
       "      <td>0.710307</td>\n",
       "      <td>3.250579</td>\n",
       "      <td>4.710419</td>\n",
       "      <td>-0.751039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.090696</td>\n",
       "      <td>-2.051241</td>\n",
       "      <td>-0.290815</td>\n",
       "      <td>-1.770225</td>\n",
       "      <td>-0.407113</td>\n",
       "      <td>-4.449056</td>\n",
       "      <td>0.354794</td>\n",
       "      <td>-1.708066</td>\n",
       "      <td>-2.584469</td>\n",
       "      <td>-3.557123</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.584700</td>\n",
       "      <td>-3.989273</td>\n",
       "      <td>3.316375</td>\n",
       "      <td>0.735364</td>\n",
       "      <td>4.073279</td>\n",
       "      <td>1.923687</td>\n",
       "      <td>0.489768</td>\n",
       "      <td>2.243266</td>\n",
       "      <td>-2.430693</td>\n",
       "      <td>-3.075062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.476211</td>\n",
       "      <td>0.077883</td>\n",
       "      <td>2.481291</td>\n",
       "      <td>-3.441597</td>\n",
       "      <td>0.833566</td>\n",
       "      <td>-4.324602</td>\n",
       "      <td>-1.038114</td>\n",
       "      <td>1.693192</td>\n",
       "      <td>1.889202</td>\n",
       "      <td>-8.951756</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.262578</td>\n",
       "      <td>-2.274254</td>\n",
       "      <td>2.063563</td>\n",
       "      <td>1.068579</td>\n",
       "      <td>3.879284</td>\n",
       "      <td>0.288383</td>\n",
       "      <td>2.252155</td>\n",
       "      <td>2.311087</td>\n",
       "      <td>-0.089803</td>\n",
       "      <td>-3.682970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.604671</td>\n",
       "      <td>0.437567</td>\n",
       "      <td>0.390226</td>\n",
       "      <td>-1.791509</td>\n",
       "      <td>-0.129578</td>\n",
       "      <td>0.469662</td>\n",
       "      <td>-1.043747</td>\n",
       "      <td>-0.333066</td>\n",
       "      <td>0.313937</td>\n",
       "      <td>-4.538773</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.362519</td>\n",
       "      <td>-0.549756</td>\n",
       "      <td>-1.546997</td>\n",
       "      <td>0.896517</td>\n",
       "      <td>5.921604</td>\n",
       "      <td>0.422985</td>\n",
       "      <td>0.351355</td>\n",
       "      <td>-0.108700</td>\n",
       "      <td>-1.117601</td>\n",
       "      <td>0.397091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.415386</td>\n",
       "      <td>-1.761841</td>\n",
       "      <td>-0.055961</td>\n",
       "      <td>-1.782646</td>\n",
       "      <td>-2.699580</td>\n",
       "      <td>-1.282648</td>\n",
       "      <td>-0.357748</td>\n",
       "      <td>0.125751</td>\n",
       "      <td>5.077182</td>\n",
       "      <td>-2.138223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570974</td>\n",
       "      <td>1.046115</td>\n",
       "      <td>-0.687799</td>\n",
       "      <td>1.032049</td>\n",
       "      <td>4.162136</td>\n",
       "      <td>3.645957</td>\n",
       "      <td>2.473218</td>\n",
       "      <td>-0.688791</td>\n",
       "      <td>-0.590855</td>\n",
       "      <td>-1.642059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-2.042580</td>\n",
       "      <td>-0.545288</td>\n",
       "      <td>-2.933997</td>\n",
       "      <td>0.435380</td>\n",
       "      <td>-1.932752</td>\n",
       "      <td>2.237168</td>\n",
       "      <td>-0.882077</td>\n",
       "      <td>-1.852991</td>\n",
       "      <td>0.445002</td>\n",
       "      <td>1.514655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583058</td>\n",
       "      <td>0.847895</td>\n",
       "      <td>-0.823864</td>\n",
       "      <td>0.188446</td>\n",
       "      <td>0.955781</td>\n",
       "      <td>-0.026733</td>\n",
       "      <td>-2.018269</td>\n",
       "      <td>-0.606821</td>\n",
       "      <td>2.170595</td>\n",
       "      <td>2.485991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.136633</td>\n",
       "      <td>-1.237760</td>\n",
       "      <td>-1.183110</td>\n",
       "      <td>-0.815889</td>\n",
       "      <td>1.367206</td>\n",
       "      <td>1.674132</td>\n",
       "      <td>-6.512691</td>\n",
       "      <td>-1.017015</td>\n",
       "      <td>0.348660</td>\n",
       "      <td>-4.619732</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.668415</td>\n",
       "      <td>1.663260</td>\n",
       "      <td>-3.336064</td>\n",
       "      <td>-0.116358</td>\n",
       "      <td>3.205699</td>\n",
       "      <td>-4.671846</td>\n",
       "      <td>2.095880</td>\n",
       "      <td>3.111485</td>\n",
       "      <td>-1.464048</td>\n",
       "      <td>0.708383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.767114</td>\n",
       "      <td>2.096215</td>\n",
       "      <td>0.746133</td>\n",
       "      <td>0.287825</td>\n",
       "      <td>2.045303</td>\n",
       "      <td>2.857263</td>\n",
       "      <td>-2.008423</td>\n",
       "      <td>-2.189678</td>\n",
       "      <td>0.697154</td>\n",
       "      <td>-3.994444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.495719</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>-2.238388</td>\n",
       "      <td>-3.878916</td>\n",
       "      <td>4.265755</td>\n",
       "      <td>-1.114963</td>\n",
       "      <td>2.158192</td>\n",
       "      <td>2.761949</td>\n",
       "      <td>1.457491</td>\n",
       "      <td>2.958742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-3.401513</td>\n",
       "      <td>-0.506430</td>\n",
       "      <td>0.380262</td>\n",
       "      <td>-2.048207</td>\n",
       "      <td>1.514004</td>\n",
       "      <td>-5.812137</td>\n",
       "      <td>-1.194896</td>\n",
       "      <td>-0.759125</td>\n",
       "      <td>-1.582029</td>\n",
       "      <td>-5.668441</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.411451</td>\n",
       "      <td>-7.510658</td>\n",
       "      <td>2.072422</td>\n",
       "      <td>2.958090</td>\n",
       "      <td>2.926072</td>\n",
       "      <td>1.766376</td>\n",
       "      <td>0.580327</td>\n",
       "      <td>0.644158</td>\n",
       "      <td>-1.897927</td>\n",
       "      <td>-4.794060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.284938</td>\n",
       "      <td>0.939088</td>\n",
       "      <td>-3.202075</td>\n",
       "      <td>0.634389</td>\n",
       "      <td>2.169413</td>\n",
       "      <td>3.406785</td>\n",
       "      <td>-5.344977</td>\n",
       "      <td>-0.291755</td>\n",
       "      <td>0.007644</td>\n",
       "      <td>-3.117552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.304967</td>\n",
       "      <td>-0.849428</td>\n",
       "      <td>-5.568500</td>\n",
       "      <td>2.012764</td>\n",
       "      <td>2.936881</td>\n",
       "      <td>-1.867398</td>\n",
       "      <td>2.681815</td>\n",
       "      <td>1.888706</td>\n",
       "      <td>-1.198099</td>\n",
       "      <td>-2.365074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -2.297971 -3.161334  2.866893 -0.996126  0.284920  1.531972  0.906654   \n",
       "1  0.090696 -2.051241 -0.290815 -1.770225 -0.407113 -4.449056  0.354794   \n",
       "2 -2.476211  0.077883  2.481291 -3.441597  0.833566 -4.324602 -1.038114   \n",
       "3 -1.604671  0.437567  0.390226 -1.791509 -0.129578  0.469662 -1.043747   \n",
       "4 -2.415386 -1.761841 -0.055961 -1.782646 -2.699580 -1.282648 -0.357748   \n",
       "5 -2.042580 -0.545288 -2.933997  0.435380 -1.932752  2.237168 -0.882077   \n",
       "6 -0.136633 -1.237760 -1.183110 -0.815889  1.367206  1.674132 -6.512691   \n",
       "7  0.767114  2.096215  0.746133  0.287825  2.045303  2.857263 -2.008423   \n",
       "8 -3.401513 -0.506430  0.380262 -2.048207  1.514004 -5.812137 -1.194896   \n",
       "9 -1.284938  0.939088 -3.202075  0.634389  2.169413  3.406785 -5.344977   \n",
       "\n",
       "         7         8         9   ...        20        21        22        23  \\\n",
       "0 -4.272559  1.360739 -7.145058  ...  0.618608 -2.027417  0.354830 -5.920053   \n",
       "1 -1.708066 -2.584469 -3.557123  ... -2.584700 -3.989273  3.316375  0.735364   \n",
       "2  1.693192  1.889202 -8.951756  ... -1.262578 -2.274254  2.063563  1.068579   \n",
       "3 -0.333066  0.313937 -4.538773  ... -1.362519 -0.549756 -1.546997  0.896517   \n",
       "4  0.125751  5.077182 -2.138223  ...  0.570974  1.046115 -0.687799  1.032049   \n",
       "5 -1.852991  0.445002  1.514655  ...  0.583058  0.847895 -0.823864  0.188446   \n",
       "6 -1.017015  0.348660 -4.619732  ... -2.668415  1.663260 -3.336064 -0.116358   \n",
       "7 -2.189678  0.697154 -3.994444  ...  0.495719 -0.121336 -2.238388 -3.878916   \n",
       "8 -0.759125 -1.582029 -5.668441  ... -2.411451 -7.510658  2.072422  2.958090   \n",
       "9 -0.291755  0.007644 -3.117552  ... -0.304967 -0.849428 -5.568500  2.012764   \n",
       "\n",
       "         24        25        26        27        28        29  \n",
       "0  2.606006  0.943722  0.710307  3.250579  4.710419 -0.751039  \n",
       "1  4.073279  1.923687  0.489768  2.243266 -2.430693 -3.075062  \n",
       "2  3.879284  0.288383  2.252155  2.311087 -0.089803 -3.682970  \n",
       "3  5.921604  0.422985  0.351355 -0.108700 -1.117601  0.397091  \n",
       "4  4.162136  3.645957  2.473218 -0.688791 -0.590855 -1.642059  \n",
       "5  0.955781 -0.026733 -2.018269 -0.606821  2.170595  2.485991  \n",
       "6  3.205699 -4.671846  2.095880  3.111485 -1.464048  0.708383  \n",
       "7  4.265755 -1.114963  2.158192  2.761949  1.457491  2.958742  \n",
       "8  2.926072  1.766376  0.580327  0.644158 -1.897927 -4.794060  \n",
       "9  2.936881 -1.867398  2.681815  1.888706 -1.198099 -2.365074  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 知识提要：闭包\n",
    "def mean_vector(model):\n",
    "    def mean_vector_compute(sentence):\n",
    "        n_w=0\n",
    "        for w in sentence:\n",
    "            if w in model.wv:\n",
    "                try:\n",
    "                    mv+=model.wv[w]\n",
    "                except:\n",
    "                    mv=model.wv[w].copy()\n",
    "                n_w+=1\n",
    "        mv/=n_w\n",
    "        return mv\n",
    "    return mean_vector_compute\n",
    "\n",
    "mv_compute=mean_vector(w2v_model)\n",
    "mean_vecs=map(mv_compute,tokenized_data)\n",
    "mean_vecs=pd.DataFrame(mean_vecs)\n",
    "mean_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者我们可以使用TF-IDF进行加权："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.148581</td>\n",
       "      <td>-0.036319</td>\n",
       "      <td>-0.018068</td>\n",
       "      <td>0.009148</td>\n",
       "      <td>0.061901</td>\n",
       "      <td>0.120478</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>-0.216511</td>\n",
       "      <td>0.146551</td>\n",
       "      <td>-0.333161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046125</td>\n",
       "      <td>-0.019234</td>\n",
       "      <td>0.038737</td>\n",
       "      <td>-0.305739</td>\n",
       "      <td>0.201816</td>\n",
       "      <td>0.137147</td>\n",
       "      <td>0.045861</td>\n",
       "      <td>0.066238</td>\n",
       "      <td>0.124265</td>\n",
       "      <td>0.169765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.095334</td>\n",
       "      <td>-0.339919</td>\n",
       "      <td>-0.069402</td>\n",
       "      <td>0.052371</td>\n",
       "      <td>-0.282315</td>\n",
       "      <td>0.030940</td>\n",
       "      <td>-0.090470</td>\n",
       "      <td>-0.162104</td>\n",
       "      <td>0.081100</td>\n",
       "      <td>-0.100262</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036592</td>\n",
       "      <td>-0.020217</td>\n",
       "      <td>0.274612</td>\n",
       "      <td>0.158418</td>\n",
       "      <td>0.227180</td>\n",
       "      <td>0.140554</td>\n",
       "      <td>-0.001166</td>\n",
       "      <td>0.191003</td>\n",
       "      <td>-0.091173</td>\n",
       "      <td>-0.195360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.101324</td>\n",
       "      <td>-0.059106</td>\n",
       "      <td>0.015652</td>\n",
       "      <td>0.015128</td>\n",
       "      <td>0.030852</td>\n",
       "      <td>-0.377617</td>\n",
       "      <td>-0.230746</td>\n",
       "      <td>0.177777</td>\n",
       "      <td>0.244334</td>\n",
       "      <td>-0.295227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030645</td>\n",
       "      <td>-0.046572</td>\n",
       "      <td>-0.194047</td>\n",
       "      <td>0.151977</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.041661</td>\n",
       "      <td>0.033858</td>\n",
       "      <td>-0.158417</td>\n",
       "      <td>-0.074691</td>\n",
       "      <td>-0.223420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.034708</td>\n",
       "      <td>0.109602</td>\n",
       "      <td>0.150284</td>\n",
       "      <td>-0.061393</td>\n",
       "      <td>-0.019534</td>\n",
       "      <td>-0.011336</td>\n",
       "      <td>-0.143899</td>\n",
       "      <td>-0.011558</td>\n",
       "      <td>0.081243</td>\n",
       "      <td>-0.252050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078418</td>\n",
       "      <td>0.018511</td>\n",
       "      <td>-0.043188</td>\n",
       "      <td>0.082479</td>\n",
       "      <td>0.605613</td>\n",
       "      <td>0.021741</td>\n",
       "      <td>0.265413</td>\n",
       "      <td>0.100253</td>\n",
       "      <td>0.098343</td>\n",
       "      <td>0.056790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004909</td>\n",
       "      <td>-0.309575</td>\n",
       "      <td>0.045406</td>\n",
       "      <td>0.192595</td>\n",
       "      <td>-0.310284</td>\n",
       "      <td>0.184387</td>\n",
       "      <td>-0.032352</td>\n",
       "      <td>-0.138226</td>\n",
       "      <td>0.227460</td>\n",
       "      <td>-0.116081</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123986</td>\n",
       "      <td>0.069369</td>\n",
       "      <td>0.011410</td>\n",
       "      <td>0.055064</td>\n",
       "      <td>0.134197</td>\n",
       "      <td>0.362929</td>\n",
       "      <td>0.153390</td>\n",
       "      <td>-0.150784</td>\n",
       "      <td>0.022146</td>\n",
       "      <td>-0.203349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.217748</td>\n",
       "      <td>-0.081696</td>\n",
       "      <td>-0.159856</td>\n",
       "      <td>0.023670</td>\n",
       "      <td>-0.139118</td>\n",
       "      <td>0.087813</td>\n",
       "      <td>0.286302</td>\n",
       "      <td>-0.192930</td>\n",
       "      <td>0.070609</td>\n",
       "      <td>0.171712</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.153864</td>\n",
       "      <td>0.171508</td>\n",
       "      <td>-0.301370</td>\n",
       "      <td>-0.072271</td>\n",
       "      <td>0.378945</td>\n",
       "      <td>0.070726</td>\n",
       "      <td>-0.184511</td>\n",
       "      <td>-0.074120</td>\n",
       "      <td>0.327844</td>\n",
       "      <td>0.325883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.188053</td>\n",
       "      <td>-0.365733</td>\n",
       "      <td>0.128107</td>\n",
       "      <td>-0.058815</td>\n",
       "      <td>0.112551</td>\n",
       "      <td>0.065304</td>\n",
       "      <td>-0.214734</td>\n",
       "      <td>0.087087</td>\n",
       "      <td>0.064707</td>\n",
       "      <td>-0.258617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026033</td>\n",
       "      <td>0.340718</td>\n",
       "      <td>-0.110482</td>\n",
       "      <td>0.069005</td>\n",
       "      <td>0.029456</td>\n",
       "      <td>-0.129096</td>\n",
       "      <td>-0.312034</td>\n",
       "      <td>-0.101793</td>\n",
       "      <td>-0.217690</td>\n",
       "      <td>-0.227760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.113986</td>\n",
       "      <td>0.248894</td>\n",
       "      <td>0.027087</td>\n",
       "      <td>0.231291</td>\n",
       "      <td>-0.032114</td>\n",
       "      <td>0.048062</td>\n",
       "      <td>-0.143418</td>\n",
       "      <td>0.060145</td>\n",
       "      <td>0.016227</td>\n",
       "      <td>-0.305819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223890</td>\n",
       "      <td>0.143665</td>\n",
       "      <td>0.066184</td>\n",
       "      <td>-0.426862</td>\n",
       "      <td>0.333553</td>\n",
       "      <td>-0.009735</td>\n",
       "      <td>-0.001862</td>\n",
       "      <td>0.181096</td>\n",
       "      <td>0.035173</td>\n",
       "      <td>0.060070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.101069</td>\n",
       "      <td>0.166036</td>\n",
       "      <td>0.097794</td>\n",
       "      <td>-0.053011</td>\n",
       "      <td>0.014484</td>\n",
       "      <td>-0.277415</td>\n",
       "      <td>-0.094133</td>\n",
       "      <td>0.043053</td>\n",
       "      <td>0.205265</td>\n",
       "      <td>-0.320228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092974</td>\n",
       "      <td>-0.387319</td>\n",
       "      <td>-0.145867</td>\n",
       "      <td>0.229892</td>\n",
       "      <td>0.168001</td>\n",
       "      <td>0.228359</td>\n",
       "      <td>0.145979</td>\n",
       "      <td>-0.175168</td>\n",
       "      <td>-0.121800</td>\n",
       "      <td>-0.119020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.175843</td>\n",
       "      <td>-0.196618</td>\n",
       "      <td>-0.361817</td>\n",
       "      <td>0.185863</td>\n",
       "      <td>0.014346</td>\n",
       "      <td>0.061425</td>\n",
       "      <td>-0.123399</td>\n",
       "      <td>0.137843</td>\n",
       "      <td>-0.129746</td>\n",
       "      <td>-0.283890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174996</td>\n",
       "      <td>0.088702</td>\n",
       "      <td>-0.223863</td>\n",
       "      <td>0.004097</td>\n",
       "      <td>-0.065761</td>\n",
       "      <td>-0.478842</td>\n",
       "      <td>0.188946</td>\n",
       "      <td>0.146038</td>\n",
       "      <td>-0.237623</td>\n",
       "      <td>-0.299823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.148581 -0.036319 -0.018068  0.009148  0.061901  0.120478  0.000085   \n",
       "1  0.095334 -0.339919 -0.069402  0.052371 -0.282315  0.030940 -0.090470   \n",
       "2 -0.101324 -0.059106  0.015652  0.015128  0.030852 -0.377617 -0.230746   \n",
       "3 -0.034708  0.109602  0.150284 -0.061393 -0.019534 -0.011336 -0.143899   \n",
       "4  0.004909 -0.309575  0.045406  0.192595 -0.310284  0.184387 -0.032352   \n",
       "5 -0.217748 -0.081696 -0.159856  0.023670 -0.139118  0.087813  0.286302   \n",
       "6 -0.188053 -0.365733  0.128107 -0.058815  0.112551  0.065304 -0.214734   \n",
       "7  0.113986  0.248894  0.027087  0.231291 -0.032114  0.048062 -0.143418   \n",
       "8 -0.101069  0.166036  0.097794 -0.053011  0.014484 -0.277415 -0.094133   \n",
       "9 -0.175843 -0.196618 -0.361817  0.185863  0.014346  0.061425 -0.123399   \n",
       "\n",
       "         7         8         9   ...        20        21        22        23  \\\n",
       "0 -0.216511  0.146551 -0.333161  ...  0.046125 -0.019234  0.038737 -0.305739   \n",
       "1 -0.162104  0.081100 -0.100262  ... -0.036592 -0.020217  0.274612  0.158418   \n",
       "2  0.177777  0.244334 -0.295227  ... -0.030645 -0.046572 -0.194047  0.151977   \n",
       "3 -0.011558  0.081243 -0.252050  ... -0.078418  0.018511 -0.043188  0.082479   \n",
       "4 -0.138226  0.227460 -0.116081  ... -0.123986  0.069369  0.011410  0.055064   \n",
       "5 -0.192930  0.070609  0.171712  ... -0.153864  0.171508 -0.301370 -0.072271   \n",
       "6  0.087087  0.064707 -0.258617  ... -0.026033  0.340718 -0.110482  0.069005   \n",
       "7  0.060145  0.016227 -0.305819  ...  0.223890  0.143665  0.066184 -0.426862   \n",
       "8  0.043053  0.205265 -0.320228  ...  0.092974 -0.387319 -0.145867  0.229892   \n",
       "9  0.137843 -0.129746 -0.283890  ...  0.174996  0.088702 -0.223863  0.004097   \n",
       "\n",
       "         24        25        26        27        28        29  \n",
       "0  0.201816  0.137147  0.045861  0.066238  0.124265  0.169765  \n",
       "1  0.227180  0.140554 -0.001166  0.191003 -0.091173 -0.195360  \n",
       "2  0.000971  0.041661  0.033858 -0.158417 -0.074691 -0.223420  \n",
       "3  0.605613  0.021741  0.265413  0.100253  0.098343  0.056790  \n",
       "4  0.134197  0.362929  0.153390 -0.150784  0.022146 -0.203349  \n",
       "5  0.378945  0.070726 -0.184511 -0.074120  0.327844  0.325883  \n",
       "6  0.029456 -0.129096 -0.312034 -0.101793 -0.217690 -0.227760  \n",
       "7  0.333553 -0.009735 -0.001862  0.181096  0.035173  0.060070  \n",
       "8  0.168001  0.228359  0.145979 -0.175168 -0.121800 -0.119020  \n",
       "9 -0.065761 -0.478842  0.188946  0.146038 -0.237623 -0.299823  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_vector_tfidf_weight(model):\n",
    "    def mean_vector_compute(sentence,tfidf):\n",
    "        for w in sentence:\n",
    "            if w in model.wv:\n",
    "                try:\n",
    "                    mv+=model.wv[w]*tfidf[w][0]\n",
    "                except:\n",
    "                    mv=model.wv[w].copy()\n",
    "        ## L2规范化\n",
    "        import numpy as np\n",
    "        mv/=np.linalg.norm(mv)\n",
    "        return mv\n",
    "    return mean_vector_compute\n",
    "\n",
    "tokenized_data=map(tokenize,data['title'])\n",
    "##首先计算词袋\n",
    "count_vect=CountVectorizer(tokenizer=tokenize)\n",
    "bag_words=count_vect.fit_transform(data['title'])\n",
    "words_names=count_vect.get_feature_names()\n",
    "##计算TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer=TfidfTransformer(norm='l2').fit(bag_words)\n",
    "tfidf_words=tfidf_transformer.transform(bag_words)\n",
    "tfidf_words_df=pd.DataFrame.sparse.from_spmatrix(tfidf_words, columns=words_names)\n",
    "##带入模型\n",
    "tfidf_weight_mean_vec=mean_vector_tfidf_weight(w2v_model)\n",
    "mean_vecs=[]\n",
    "for i,sentence in enumerate(tokenized_data):\n",
    "    mean_vecs.append(tfidf_weight_mean_vec(sentence,tfidf_words_df.iloc[i,:]))\n",
    "mean_vecs=pd.DataFrame(mean_vecs)\n",
    "mean_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新进展\n",
    "\n",
    "以上介绍的特征提取技术都是非常规范化的提取技术，然而也有很大漏洞。比如「大败」这个词，如果是「巴塞罗那大败皇家马德里」，意思是巴塞罗那赢了，但是如果是「巴塞罗那大败」，那巴塞罗那输了。同理还有更显而易见的，「bank」这个词即是「银行」的意思，也是「河岸」的意思。但是在我们上面的处理中，并没有能够区分同一个词的这些差别。\n",
    "\n",
    "目前在文本挖掘领域，Google的BERT正发展的如火如荼，BERT通过一个大型的深度学习网络，将词语放在句子中进行理解，可以很大程度上克服以上问题。目前也有了包括中文在内的预训练模型，以及TensorFlow和PyTorch等主流框架的实现。\n",
    "\n",
    "此外，在传统的RNN以及卷积神经网络的基础上，最近还有Attention模型等新进展。这个领域仍然是机器学习的前沿领域，值得比较密切的关注其动向。\n",
    "\n",
    "最后还是要提示的是，对于非专业机器学习使用者，模型是相对固定的，特征提取是非常关键的。以上介绍的方法并不是唯一的方法，比如我们完全可以根据主观理解，通过一系列的正则表达式定义出很多模式，放在特征中，这些都是特征提取的重要方式。实际应用中需要根据应用背景灵活使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本距离与相似度\n",
    "\n",
    "距离和相似度在文本的概念上都是文本之间某种相似程度的度量，只不过距离（distance）通常用于比较短小的词汇、句子上的差异性有多大，而相似度则主要针对更长的文档等。\n",
    "\n",
    "## 编辑距离\n",
    "\n",
    "经过适当的向量化之后，我们可以轻松地使用向量进行向量空间的任何距离操作，距离越小代表两个字符串之间越相似。在这里我们额外介绍一种常用的距离，即基于编辑距离的Levenshtein距离，该距离度量了从一个字符串str1需要经过多少步的编辑（替换一个字符、插入一个字符、删除一个字符）才能变成另外一个字符串str2。这个步数的计算可以通过动态规划（dynamic programming）来完成。\n",
    "\n",
    "Python中可以安装Levenshtein包：\n",
    "```shell\n",
    "sudo pip3 install python-Levenshtein\n",
    "```\n",
    "\n",
    "计算Levenshtein距离："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "Levenshtein.distance('色即是空，空即是色','色不异空，空不异色')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上给出了最小修改次数，不过最好是将其规范化：$$ratio=\\frac{len\\left(str1\\right)+len\\left(str2\\right)-distance}{len\\left(str1\\right)+len\\left(str2\\right)}$$可以使用Levenstein.ratio()计算该比例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5555555555555556"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "Levenshtein.ratio('色即是空，空即是色','色不异空，空不异色')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编辑距离有很多用处，比如比照不同版本、侦测细微的字符串差异（比如可能存在的地址输入差异「上海松江文汇路」和「上海市松江区文汇路」）、检查抄袭等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.627906976744186"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "xj_jmls=\"观世音菩萨，行深般若波罗蜜时，照见五阴空，度一切苦厄。舍利弗，色空故，无恼坏相，受空故，无受相，想空故，无知相，行空故，无作相，识空故，无觉相。何以故？舍利弗，非色异空，非空异色，色即是空，空即是色，受想行识，亦复如是。\"\n",
    "xj_xz=\"观自在菩萨，行深般若波罗蜜多时，照见五蕴皆空，度一切苦厄。舍利子，色不异空，空不异色，色即是空，空即是色，受想行识亦复如是。\"\n",
    "Levenshtein.ratio(xj_jmls,xj_xz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本相似度\n",
    "\n",
    "不管是使用词袋模型，还是TF-IDF模型，或者使用词嵌入方法通过平均、加权平均的方式，都可以讲一个文本向量化。\n",
    "\n",
    "将文本向量化之后，计算文本之间的相似度就非常简单了：只要将两个文本的向量之间的相似度计算出来即可，常用的度量是余弦相似度（即两个向量的夹角）：$$cs\\left(u,v\\right)=\\frac{u\\cdot v}{\\left\\Vert u\\right\\Vert \\cdot \\left\\Vert v\\right\\Vert }$$其中分子为内积，分母上位L2范数的乘积。我们可以使用NumPy很快的计算出该值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cos_similarity=lambda u,v: np.inner(u,v)/(np.linalg.norm(u)*np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如如果我们使用词袋模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##计算词袋\n",
    "count_vect=CountVectorizer(tokenizer=tokenize)\n",
    "bag_words=count_vect.fit_transform(data['title'])\n",
    "words_names=count_vect.get_feature_names()\n",
    "bag_words_df=pd.DataFrame.sparse.from_spmatrix(bag_words, columns=words_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15811388300841897"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity(bag_words_df.iloc[7,:],bag_words_df.iloc[9,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然也可以找出于某一个标题最为相似的，内存限制只算前50000个（如果要算所有的可以分开来算）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沪争取增值税扩围改革试点\n",
      "增值税扩围和资源税改革今年有望取得突破\n"
     ]
    }
   ],
   "source": [
    "##计算词袋\n",
    "count_vect=CountVectorizer(tokenizer=tokenize)\n",
    "bag_words=count_vect.fit_transform(RAW1['title'][:50000])\n",
    "words_names=count_vect.get_feature_names()\n",
    "bag_words_df=pd.DataFrame.sparse.from_spmatrix(bag_words, columns=words_names)\n",
    "##计算相关系数（自己用循环试一下，奇慢无比，如果不向量化计算，估计要跑的时间按天算）\n",
    "ip=np.array(np.dot(bag_words_df,bag_words_df.iloc[0,:])) ## 这里用到了广播\n",
    "norm1=np.array(np.linalg.norm(bag_words_df, axis=1)) ## 第一行的norm都一样，所以不用除\n",
    "corr=ip/norm1\n",
    "corr=corr[1:]\n",
    "##找最大值\n",
    "argi=np.argmax(corr)\n",
    "print(RAW1['title'].iloc[0])\n",
    "print(RAW1['title'].iloc[argi+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在其他向量化方法下同理，在此不再赘述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类：情感分析\n",
    "\n",
    "情感分析（sentiment analysis）最初指对文本的情感，比如褒义还是贬义，以及文章中的情感倾向进行分析，实际上是文本分类的一种。实际上，我们之前学到过的所有的监督学习方法都可以使用在文本分类中。然而，监督学习需要大量的带有标签的词典，有时这种方法是行不通的，所以也会有根据特定模式对文本进行分类的方法，或者使用预训练模型的方法。比如，一个经常使用的方法是使用情感词典。我们将分别介绍使用情感词典的方法和使用监督学习的方法。\n",
    "\n",
    "## 基于词典\n",
    "\n",
    "情感词典即标记了情感得分的一个词典，这个词典可以看作是一个预训练的模型，模型的训练结果是情感得分，而我们只需要使用这些情感得分就可以得到情感的具体取值。\n",
    "\n",
    "情感词典方法的好处是非常的简单：只需要简单计算得分即可。然而缺点也是非常突出的：情感词典通常不对特殊问题进行优化，此外其标签是固定的。比如，一般情感词典也许会标记正面负面，但是当我们将其用在金融领域时，会发现“降准”这个正向词汇甚至不会出现在情感词典中，虽然这个词在金融领域应该对股票市场是一个正向词汇。\n",
    "\n",
    "此外，不同情境下也许同一个词也有不同的情感倾向，比如如果我们讨论股票，“降准”也许是一个正向词汇，然而如果我们讨论的是债券，“降准”就不见得是什么好词了。\n",
    "\n",
    "当然，情感词典在一般的领域中应用也许也可以达到比较高的精准度。在这里我们先介绍情感词典的使用方法。\n",
    "\n",
    "使用情感词典的第一步是获得情感词典，我们在这里列举了几个比较常用的情感词典：\n",
    "\n",
    "* 清华大学李军中文褒贬义词典（ http://nlp.csai.tsinghua.edu.cn/site2/index.php/13-sms ）（./Chinese/BosonNLP/）\n",
    "* 知网HowNet情感词典（./Chinese/HowNet/）\n",
    "* 玻森公司是情感词典（ http://static.bosonnlp.com/dev/resource ）（./Chinese/Tsinghua/）\n",
    "\n",
    "以上词典都可以在括号中的地址，或者括号中的路径里面找到。\n",
    "\n",
    "比如，对于一个句子，我们可以先将其使用词袋模型将其整理为词袋，然后对于每个词都是用其情感值对其进行打分，并处理否定词，最后根据每个词的打分情况汇总为这个句子的情感。\n",
    "\n",
    "比如，以HowNet的情感词典为例，其情感词典的内容大约为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Chinese/HowNet/正面情感词语（中文）.txt\", encoding='gb18030') as f:\n",
    "    posilist=f.readlines()\n",
    "del posilist[0]\n",
    "SentDict={}\n",
    "for w in posilist:\n",
    "    SentDict[w.strip()]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码我们将所有的正面词汇给以一个数值1，代表正面，后面我们还将加载负面词汇，用-1代表负面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentDict={}\n",
    "Files=['面情感词语（中文）.txt','面情感词语（英文）.txt','面评价词语（中文）.txt','面评价词语（英文）.txt']\n",
    "for p in ['正','负']:\n",
    "    v=(1 if p=='正' else -1)\n",
    "    for f in Files:\n",
    "        with open(\"Chinese/HowNet/\"+p+f, encoding='gb18030') as f:\n",
    "            posilist=f.readlines()\n",
    "        del posilist[0]\n",
    "        for w in posilist:\n",
    "            if w not in SentDict:\n",
    "                SentDict[w.strip()]=v\n",
    "            else:\n",
    "                SentDict[w.strip()]+=v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来导入否定词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no', '不', '没', '无', '非', '莫', '弗', '毋', '未', '否', '别', '无', '不够', '不是', '不可', '不曾', '未必', '没有', '不要', '难以', '未曾', '否认', '取消', '撤回']\n"
     ]
    }
   ],
   "source": [
    "NegaList=[]\n",
    "with open(\"Chinese/negative.txt\") as f:\n",
    "    for w in f:\n",
    "        NegaList.append(w.strip())\n",
    "print(NegaList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们以大众点评的数据作为例子展示情感词典的用法，首先读入数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cus_id</th>\n",
       "      <th>comment_time</th>\n",
       "      <th>comment_star</th>\n",
       "      <th>cus_comment</th>\n",
       "      <th>kouwei</th>\n",
       "      <th>huanjing</th>\n",
       "      <th>fuwu</th>\n",
       "      <th>shopID</th>\n",
       "      <th>stars</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>comment_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>迷糊泰迪</td>\n",
       "      <td>2018-09-20 06:48:00</td>\n",
       "      <td>sml-str40</td>\n",
       "      <td>南信 算是 广州 著名 甜品店 吧 好几个 时间段 路过 都 是 座无虚席 看着 餐单 上 ...</td>\n",
       "      <td>非常好</td>\n",
       "      <td>好</td>\n",
       "      <td>好</td>\n",
       "      <td>518986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>稱霸幼稚園</td>\n",
       "      <td>2018-09-22 21:49:00</td>\n",
       "      <td>sml-str40</td>\n",
       "      <td>中午 吃 完 了 所谓 的 早茶 回去 放下 行李 休息 了 会 就 来 吃 下午茶 了 服...</td>\n",
       "      <td>很好</td>\n",
       "      <td>很好</td>\n",
       "      <td>很好</td>\n",
       "      <td>518986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>爱吃的美美侠</td>\n",
       "      <td>2018-09-22 22:16:00</td>\n",
       "      <td>sml-str40</td>\n",
       "      <td>冲刺 王者 战队 吃遍 蓉城 战队 有 特权 五月份 和 好 朋友 毕业 旅行 来 了 广州...</td>\n",
       "      <td>很好</td>\n",
       "      <td>很好</td>\n",
       "      <td>很好</td>\n",
       "      <td>518986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>姜姜会吃胖</td>\n",
       "      <td>2018-09-19 06:36:00</td>\n",
       "      <td>sml-str40</td>\n",
       "      <td>都 说来 广州 吃 糖水 就要 来南信 招牌 姜撞奶 红豆 双皮奶 牛 三星 云吞面 一楼 ...</td>\n",
       "      <td>非常好</td>\n",
       "      <td>很好</td>\n",
       "      <td>很好</td>\n",
       "      <td>518986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forevercage</td>\n",
       "      <td>2018-08-24 17:58:00</td>\n",
       "      <td>sml-str50</td>\n",
       "      <td>一直 很 期待 也 最 爱 吃 甜品 广州 的 甜品 很 丰富 很 多样 来 之前 就 一直...</td>\n",
       "      <td>非常好</td>\n",
       "      <td>很好</td>\n",
       "      <td>很好</td>\n",
       "      <td>518986</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cus_id         comment_time comment_star  \\\n",
       "0         迷糊泰迪  2018-09-20 06:48:00    sml-str40   \n",
       "1        稱霸幼稚園  2018-09-22 21:49:00    sml-str40   \n",
       "2       爱吃的美美侠  2018-09-22 22:16:00    sml-str40   \n",
       "3        姜姜会吃胖  2018-09-19 06:36:00    sml-str40   \n",
       "4  forevercage  2018-08-24 17:58:00    sml-str50   \n",
       "\n",
       "                                         cus_comment kouwei huanjing fuwu  \\\n",
       "0  南信 算是 广州 著名 甜品店 吧 好几个 时间段 路过 都 是 座无虚席 看着 餐单 上 ...    非常好        好    好   \n",
       "1  中午 吃 完 了 所谓 的 早茶 回去 放下 行李 休息 了 会 就 来 吃 下午茶 了 服...     很好       很好   很好   \n",
       "2  冲刺 王者 战队 吃遍 蓉城 战队 有 特权 五月份 和 好 朋友 毕业 旅行 来 了 广州...     很好       很好   很好   \n",
       "3  都 说来 广州 吃 糖水 就要 来南信 招牌 姜撞奶 红豆 双皮奶 牛 三星 云吞面 一楼 ...    非常好       很好   很好   \n",
       "4  一直 很 期待 也 最 爱 吃 甜品 广州 的 甜品 很 丰富 很 多样 来 之前 就 一直...    非常好       很好   很好   \n",
       "\n",
       "   shopID  stars  year  month  weekday  hour  comment_len  \n",
       "0  518986    4.0  2018      9        3     6          184  \n",
       "1  518986    4.0  2018      9        5    21          266  \n",
       "2  518986    4.0  2018      9        5    22          341  \n",
       "3  518986    4.0  2018      9        2     6          197  \n",
       "4  518986    5.0  2018      8        4    17          261  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dianping=pd.read_csv(\"csv/dianping.csv\")\n",
    "dianping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，cus_comment里面是已经分好词的句子，此外还有详细的评分数据。我们可以写一个评分函数，然后使用map()方法进行评分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def score(s):\n",
    "    w_list=str(s).split(' ')\n",
    "    multiplier=1\n",
    "    senti=0\n",
    "    for w in w_list:\n",
    "        if w in NegaList:\n",
    "            multiplier*=(-1)\n",
    "        if w in SentDict:\n",
    "            senti+=(multiplier*SentDict[w])\n",
    "            multiplier=1\n",
    "    return np.sign(senti)\n",
    "dianping['score']=dianping['cus_comment'].map(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了查看分类效果，简单的可以分类求均值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>797.0</td>\n",
       "      <td>0.225847</td>\n",
       "      <td>0.866277</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>982.0</td>\n",
       "      <td>0.422607</td>\n",
       "      <td>0.818752</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5152.0</td>\n",
       "      <td>0.629658</td>\n",
       "      <td>0.688732</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>10849.0</td>\n",
       "      <td>0.787354</td>\n",
       "      <td>0.534436</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>9067.0</td>\n",
       "      <td>0.836109</td>\n",
       "      <td>0.465256</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         score                                             \n",
       "         count      mean       std  min  25%  50%  75%  max\n",
       "stars                                                      \n",
       "1.0      797.0  0.225847  0.866277 -1.0 -1.0  1.0  1.0  1.0\n",
       "2.0      982.0  0.422607  0.818752 -1.0  0.0  1.0  1.0  1.0\n",
       "3.0     5152.0  0.629658  0.688732 -1.0  1.0  1.0  1.0  1.0\n",
       "4.0    10849.0  0.787354  0.534436 -1.0  1.0  1.0  1.0  1.0\n",
       "5.0     9067.0  0.836109  0.465256 -1.0  1.0  1.0  1.0  1.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dianping[['stars','score']].groupby('stars').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到评分越高，用户的评星也越高，结果可以接受。不过即使是1星，最终的评分平均也是大于0的，所以在使用的时候也可能会出现问题，即很多评分为正的实际上却是负面评价，比如如果我们根据评分计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">stars</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>2060.0</td>\n",
       "      <td>3.336893</td>\n",
       "      <td>1.201463</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2765.0</td>\n",
       "      <td>3.755877</td>\n",
       "      <td>1.091709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22022.0</td>\n",
       "      <td>4.072700</td>\n",
       "      <td>0.900746</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         stars                                             \n",
       "         count      mean       std  min  25%  50%  75%  max\n",
       "score                                                      \n",
       "-1      2060.0  3.336893  1.201463  1.0  3.0  3.0  4.0  5.0\n",
       " 0      2765.0  3.755877  1.091709  1.0  3.0  4.0  5.0  5.0\n",
       " 1     22022.0  4.072700  0.900746  1.0  4.0  4.0  5.0  5.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dianping[['stars','score']].groupby('score').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到评价为负和正的之间，平均星级并没有很大差别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于机器学习\n",
    "\n",
    "如果像以上的点评数据，有星级、正负面的信息，我们当然可以使用机器学习的所有方法，结合词袋、TF-IDF以及词嵌入、深度学习等方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list=[]\n",
    "with open(\"Chinese/stopword.txt\") as f:\n",
    "    for w in f:\n",
    "        stop_list.append(w.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57371\n"
     ]
    }
   ],
   "source": [
    "ALL_words={}\n",
    "for sentence in dianping['cus_comment']:\n",
    "    ws=str(sentence).split(' ')\n",
    "    for w in ws:\n",
    "        wstrip=w.strip()\n",
    "        if wstrip not in stop_list:\n",
    "            if wstrip not in ALL_words:\n",
    "                ALL_words[wstrip]=1\n",
    "            else:\n",
    "                ALL_words[wstrip]+=1\n",
    "print(len(ALL_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9924\n"
     ]
    }
   ],
   "source": [
    "word_code={}\n",
    "code_len=1\n",
    "for k in ALL_words:\n",
    "    if ALL_words[k]>5:\n",
    "        if k not in word_code:\n",
    "            word_code[k]=code_len\n",
    "            code_len+=1\n",
    "\n",
    "print(code_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>word_series</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5091</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[86, 311, 178, 415, 416, 1455, 512, 509, 1130,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27813</th>\n",
       "      <td>5.0</td>\n",
       "      <td>[1191, 7971, 530, 1262, 749, 32, 347]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22860</th>\n",
       "      <td>4.0</td>\n",
       "      <td>[4422, 1457, 223, 63, 4422, 86, 6786, 741, 53,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23464</th>\n",
       "      <td>4.0</td>\n",
       "      <td>[9201, 131, 702, 63, 1191, 978, 134, 3821, 115...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19979</th>\n",
       "      <td>4.0</td>\n",
       "      <td>[255, 1267, 255, 221, 603, 1939, 7710, 5823, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9642</th>\n",
       "      <td>4.0</td>\n",
       "      <td>[180, 3766, 63, 120]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21592</th>\n",
       "      <td>4.0</td>\n",
       "      <td>[694, 1457, 4422, 14, 2438, 2095, 364]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28522</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[7699, 82, 1168, 6337, 794, 571, 1000, 86]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18232</th>\n",
       "      <td>4.0</td>\n",
       "      <td>[604, 2990, 6670, 831, 794, 7438, 4026, 31, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3799</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[41, 261, 31, 115, 159]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26844 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       stars                                        word_series\n",
       "5091     5.0  [86, 311, 178, 415, 416, 1455, 512, 509, 1130,...\n",
       "27813    5.0              [1191, 7971, 530, 1262, 749, 32, 347]\n",
       "22860    4.0  [4422, 1457, 223, 63, 4422, 86, 6786, 741, 53,...\n",
       "23464    4.0  [9201, 131, 702, 63, 1191, 978, 134, 3821, 115...\n",
       "19979    4.0  [255, 1267, 255, 221, 603, 1939, 7710, 5823, 1...\n",
       "...      ...                                                ...\n",
       "9642     4.0                               [180, 3766, 63, 120]\n",
       "21592    4.0             [694, 1457, 4422, 14, 2438, 2095, 364]\n",
       "28522    3.0         [7699, 82, 1168, 6337, 794, 571, 1000, 86]\n",
       "18232    4.0  [604, 2990, 6670, 831, 794, 7438, 4026, 31, 10...\n",
       "3799     3.0                            [41, 261, 31, 115, 159]\n",
       "\n",
       "[26844 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_series=[]\n",
    "word_len=[]\n",
    "for sentence in dianping['cus_comment']:\n",
    "    sentence_series=[]\n",
    "    ws=str(sentence).split(' ')\n",
    "    for w in ws:\n",
    "        wstrip=w.strip()\n",
    "        if wstrip in word_code:\n",
    "            sentence_series.append(word_code[wstrip])\n",
    "    word_series.append(sentence_series)\n",
    "    word_len.append(len(sentence_series))\n",
    "dianping['word_series']=word_series\n",
    "dianping['word_len']=word_len\n",
    "dianping=dianping[dianping['word_len']>0]\n",
    "sub_dianping=dianping[['stars','word_series']]\n",
    "sub_dianping=sub_dianping.dropna()\n",
    "sub_dianping['random']=np.random.random(sub_dianping.shape[0])\n",
    "sub_dianping=sub_dianping.sort_values('random')\n",
    "del sub_dianping['random']\n",
    "sub_dianping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个比较棘手的问题是，由于不同文本的长度是不一样的，所以在输入给神经网络的时候很容易出问题，这里我们可以使用pad的方法将短的句子用0进行填充，从而得到一个等长的序列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([1, 2, 3, 4, 5, 6]), 6), (tensor([1, 2, 3]), 3), (tensor([1]), 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [2, 2, 0],\n",
       "        [3, 3, 0],\n",
       "        [4, 0, 0],\n",
       "        [5, 0, 0],\n",
       "        [6, 0, 0]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "a=torch.tensor([1,2,3])\n",
    "b=torch.tensor([1])\n",
    "c=torch.tensor([1,2,3,4,5,6])\n",
    "# 记录长度\n",
    "tensors_with_len=[(i,len(i)) for i in [a,b,c]]\n",
    "tensors_with_len.sort(key=lambda t: t[1], reverse=True)\n",
    "print(tensors_with_len)\n",
    "tensors=[t[0] for t in tensors_with_len]\n",
    "lens=[t[1] for t in tensors_with_len]\n",
    "# 进行padding\n",
    "padded_tensor=nn.utils.rnn.pad_sequence(tensors)\n",
    "padded_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad以后可以进行embedding操作，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7287, -0.6451],\n",
       "         [-0.7287, -0.6451],\n",
       "         [-0.7287, -0.6451]],\n",
       "\n",
       "        [[-0.9325,  0.0581],\n",
       "         [-0.9325,  0.0581],\n",
       "         [ 0.0000,  0.0000]],\n",
       "\n",
       "        [[-1.2502,  0.9808],\n",
       "         [-1.2502,  0.9808],\n",
       "         [ 0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.3096, -2.1525],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.2819, -0.5410],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.4554,  1.1922],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding=nn.Embedding(7,2,padding_idx=0) # 指定0为pad\n",
    "embedded=embedding(padded_tensor)\n",
    "print(embedded.shape)\n",
    "embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将序列交给RNN或者LSTM模型时，可以将以上padded tensor进行打包（pack）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-0.7287, -0.6451],\n",
       "        [-0.7287, -0.6451],\n",
       "        [-0.7287, -0.6451],\n",
       "        [-0.9325,  0.0581],\n",
       "        [-0.9325,  0.0581],\n",
       "        [-1.2502,  0.9808],\n",
       "        [-1.2502,  0.9808],\n",
       "        [ 0.3096, -2.1525],\n",
       "        [-0.2819, -0.5410],\n",
       "        [-0.4554,  1.1922]], grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([3, 2, 2, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 长度必须从大到小排序\n",
    "packed_tensor1=nn.utils.rnn.pack_padded_sequence(embedded, lengths=lens)\n",
    "packed_tensor1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如此，在RNN或者LSTM中，就不会对如果需要从packed还原，只需要使用unpack就可以了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.7287, -0.6451],\n",
       "          [-0.7287, -0.6451],\n",
       "          [-0.7287, -0.6451]],\n",
       " \n",
       "         [[-0.9325,  0.0581],\n",
       "          [-0.9325,  0.0581],\n",
       "          [ 0.0000,  0.0000]],\n",
       " \n",
       "         [[-1.2502,  0.9808],\n",
       "          [-1.2502,  0.9808],\n",
       "          [ 0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.3096, -2.1525],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.2819, -0.5410],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.4554,  1.1922],\n",
       "          [ 0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000]]], grad_fn=<CopySlices>),\n",
       " tensor([6, 3, 1]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.utils.rnn.pad_packed_sequence(packed_tensor1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用如上特性，我们可以定义数据了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "Y_train=sub_dianping.iloc[:20000,0]>=4\n",
    "X_train=sub_dianping.iloc[:20000,1]\n",
    "Y_test=sub_dianping.iloc[20000:,0]>=4\n",
    "X_test=sub_dianping.iloc[20000:,1]\n",
    "\n",
    "class dianping_data(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X=X\n",
    "        self.Y=Y\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        x=(self.X.iloc[i],len(self.X.iloc[i]),self.Y.iloc[i])\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过值得注意的是，如果使用默认的Dataloader，会在最终将每一条数据合并为一个Tensor，而由于我们这里的数据是变长的，还需要进行padding等操作，所以我们先不用原来的dataloader的collate_fn，而是定义一个新的clolate_fn，在其中完成排序、padding的过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 255,   86, 4422, 9201, 1191],\n",
      "        [1267,  311, 1457,  131, 7971],\n",
      "        [ 255,  178,  223,  702,  530],\n",
      "        [ 221,  415,   63,   63, 1262],\n",
      "        [ 603,  416, 4422, 1191,  749],\n",
      "        [1939, 1455,   86,  978,   32],\n",
      "        [7710,  512, 6786,  134,  347],\n",
      "        [5823,  509,  741, 3821,    0],\n",
      "        [1175, 1130,   53,  115,    0],\n",
      "        [  84,  995, 1457,   61,    0],\n",
      "        [1030, 2992,  620, 5869,    0],\n",
      "        [1939, 5143,  514,   82,    0],\n",
      "        [  70,  548, 2095,  433,    0],\n",
      "        [1971,   71, 9653,    0,    0],\n",
      "        [2661,   61, 9766,    0,    0],\n",
      "        [1047,  163,   63,    0,    0],\n",
      "        [  99,  164, 4072,    0,    0],\n",
      "        [4165,   22, 4072,    0,    0],\n",
      "        [  96,  268, 3302,    0,    0],\n",
      "        [ 433, 1992,  720,    0,    0],\n",
      "        [1118,   61, 7199,    0,    0],\n",
      "        [6040, 2276,   39,    0,    0],\n",
      "        [ 569,  370,   63,    0,    0],\n",
      "        [2394, 2674, 1191,    0,    0],\n",
      "        [ 220, 1212,  978,    0,    0],\n",
      "        [ 647,  830, 9201,    0,    0],\n",
      "        [ 648, 6661, 1415,    0,    0],\n",
      "        [ 836, 7955, 1433,    0,    0],\n",
      "        [1617,  221, 1191,    0,    0],\n",
      "        [ 118,  163,  978,    0,    0],\n",
      "        [ 445,  164, 1192,    0,    0],\n",
      "        [ 620, 1187,   63,    0,    0],\n",
      "        [5984,  569, 3373,    0,    0],\n",
      "        [3781,  438,  569,    0,    0],\n",
      "        [  22,  569,  370,    0,    0],\n",
      "        [ 308, 1007, 1895,    0,    0],\n",
      "        [ 995,  406,  227,    0,    0],\n",
      "        [ 685, 1974,    0,    0,    0],\n",
      "        [3184,  358,    0,    0,    0],\n",
      "        [1381,   93,    0,    0,    0],\n",
      "        [ 892,    0,    0,    0,    0],\n",
      "        [ 221,    0,    0,    0,    0],\n",
      "        [ 860,    0,    0,    0,    0],\n",
      "        [1127,    0,    0,    0,    0],\n",
      "        [1557,    0,    0,    0,    0],\n",
      "        [4141,    0,    0,    0,    0],\n",
      "        [2122,    0,    0,    0,    0],\n",
      "        [  66,    0,    0,    0,    0],\n",
      "        [3107,    0,    0,    0,    0],\n",
      "        [ 860,    0,    0,    0,    0],\n",
      "        [ 354,    0,    0,    0,    0],\n",
      "        [ 221,    0,    0,    0,    0],\n",
      "        [ 738,    0,    0,    0,    0],\n",
      "        [2124,    0,    0,    0,    0],\n",
      "        [6744,    0,    0,    0,    0],\n",
      "        [1127,    0,    0,    0,    0],\n",
      "        [1939,    0,    0,    0,    0],\n",
      "        [ 738,    0,    0,    0,    0],\n",
      "        [5142,    0,    0,    0,    0],\n",
      "        [ 347,    0,    0,    0,    0],\n",
      "        [ 118,    0,    0,    0,    0],\n",
      "        [ 104,    0,    0,    0,    0],\n",
      "        [2366,    0,    0,    0,    0]])\n",
      "[63, 40, 37, 13, 7]\n"
     ]
    }
   ],
   "source": [
    "batch_size=5\n",
    "train_data=dianping_data(X_train, Y_train)\n",
    "\n",
    "def collate(x):\n",
    "    x.sort(key=lambda t:t[1], reverse=True)\n",
    "    X=[torch.tensor(t[0]).long() for t in x]\n",
    "    L=[t[1] for t in x]\n",
    "    Y=[t[2] for t in x]\n",
    "    X=nn.utils.rnn.pad_sequence(X)\n",
    "    Y=torch.tensor(Y).float()\n",
    "    return X,L,Y\n",
    "\n",
    "dl=DataLoader(train_data, batch_size=batch_size, collate_fn=collate)\n",
    "for x,l,y in dl:\n",
    "    print(x)\n",
    "    print(l)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1000\n",
    "train_data=dianping_data(X_train, Y_train)\n",
    "dl=DataLoader(train_data, shuffle=True, batch_size=batch_size, pin_memory=True, num_workers=10,\n",
    "             collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来就可以定义模型了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class classifier(nn.Module):\n",
    "    def __init__(self, word_count, batch_size, embedding_size=300, lstm_hidden_size=2000, num_nurons=2000):\n",
    "        super(classifier, self).__init__()\n",
    "        self.batch_size=batch_size #批大小\n",
    "        self.embedding_size=embedding_size #嵌入层词向量大小\n",
    "        self.lstm_hidden_size=lstm_hidden_size #隐藏状态大小\n",
    "        \n",
    "        self.embedding=nn.Embedding(word_count, embedding_size,padding_idx=0)\n",
    "        self.lstm=nn.LSTM(input_size=embedding_size, hidden_size=lstm_hidden_size, num_layers=2)\n",
    "        self.layer3=nn.Sequential(nn.Linear(lstm_hidden_size, num_nurons),nn.LeakyReLU(inplace=True),\n",
    "                                  nn.LayerNorm(num_nurons),\n",
    "                                  nn.Linear(num_nurons,num_nurons),nn.LeakyReLU(inplace=True),\n",
    "                                  nn.Dropout(0.5),nn.Linear(num_nurons,1),\n",
    "                                  nn.Sigmoid())\n",
    "        self.hidden=self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        h=torch.zeros(2, self.batch_size, self.lstm_hidden_size).to(device)\n",
    "        c=torch.zeros(2, self.batch_size, self.lstm_hidden_size).to(device)\n",
    "        return (h, c)\n",
    "\n",
    "    def forward(self, x, l):\n",
    "        x=self.embedding(x)\n",
    "        x=nn.utils.rnn.pack_padded_sequence(x, lengths=l)\n",
    "        lstm_out, (h,c) = self.lstm(x, self.hidden)\n",
    "        y=self.layer3(h[-1])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，定义学习率和损失函数，进行求解："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次epoch，Smoothed Loss=25.600000381469727，LR=[0.099]\n"
     ]
    }
   ],
   "source": [
    "model=classifier(code_len, batch_size).to(device)\n",
    "# 注意我们对损失函数进行了加权\n",
    "criterion=nn.BCELoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "lr_scheduler=torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "losses=[]\n",
    "Exp_Smoothed_Loss=0\n",
    "for i in range(1000):\n",
    "    for x,l,y  in dl:\n",
    "        # 将x计算预测值\n",
    "        y_pred=model(x.to(device),l)\n",
    "        # 计算损失\n",
    "        loss=criterion(y_pred, y.to(device).unsqueeze(1))\n",
    "        losses.append(loss.item())\n",
    "        if i==0:\n",
    "            Exp_Smoothed_Loss=loss.item()\n",
    "        else:\n",
    "            Exp_Smoothed_Loss=0.01*loss.item()+0.99*Exp_Smoothed_Loss\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    if i%20==0:\n",
    "        print(\"第%s次epoch，Smoothed Loss=%s，LR=%s\" % (i, Exp_Smoothed_Loss, lr_scheduler.get_last_lr()))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15.0, 5.0)\n",
    "\n",
    "i=np.arange(len(losses))+1\n",
    "plt.plot(i,np.array(losses))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=dianping_data(X_test, Y_test)\n",
    "tdl=DataLoader(test_data, shuffle=False, batch_size=batch_size,collate_fn=collate, drop_last=True)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pred=torch.tensor([]).to(device)\n",
    "for x, l, y in tdl:\n",
    "    p=model(x.to(device),l)\n",
    "    pred=torch.cat([pred,p])\n",
    "\n",
    "result=pd.DataFrame({'true_value':Y_test[:len(pred)],'Predicted':pred.squeeze(1).cpu().detach().numpy()})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画ROC曲线\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(result['true_value'],result['Predicted'])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set()\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0)\n",
    "plt.plot(fpr,tpr,label='ROC, AUC=%.2f' % roc_auc)\n",
    "plt.legend(loc='upper left', frameon=True)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类：金融新闻\n",
    "\n",
    "同样，建立在以上特征提取的基础上，我们已经能够将文本数据转化为向量，那么接下来，任何的机器学习算法也都可以应用到这些词向量上面。\n",
    "\n",
    "但是，由于文本数据的高度复杂性和非线性性、高维性，传统的Logistic回归等方法肯定不再适用，此时支持向量机、贝叶斯方法、决策树、随机森林都可以试一下。\n",
    "\n",
    "当然，模型是固定的，模型的好坏很大程度上取决于特征提取的方式以及参数设定。\n",
    "\n",
    "\n",
    "我们下面给一个例子作为示例，即使用每天的新闻数据预测沪深300指数的涨跌。\n",
    "\n",
    "当然，以下模型仅仅是为了展示用法，我们并没有对文本进行更详细的清洗和特征提取工作，此外以下新闻很多都是针对个股的，我们也没有将新闻具体匹配到个股，更没有使用更加规范地计算超额收益率，所以准确性必然很差，后续还有相当大的改进空间。\n",
    "\n",
    "其中沪深300的数据和处理过程我们放在下面，为了不造成服务器压力，我们把数据已经存储下来：\n",
    "\n",
    "```python\n",
    "## 导入股票数据\n",
    "import akshare as ak\n",
    "import pandas as pd\n",
    "stock_data=ak.stock_zh_index_daily(symbol=\"sh000300\")\n",
    "## 由于pandas的时间滞后会导致缺失值，手动产生滞后\n",
    "stock_data=stock_data.reset_index()\n",
    "last_trade_day=stock_data['close']\n",
    "last_trade_day=last_trade_day.reset_index()\n",
    "last_trade_day=last_trade_day.drop('index',axis=1)\n",
    "stock_data=stock_data.drop(0)\n",
    "stock_data=stock_data.reset_index()\n",
    "stock_data['last_close']=last_trade_day\n",
    "stock_data['tag']=stock_data['last_close']<stock_data['close']\n",
    "## 设定时间\n",
    "stock_data['date']=pd.to_datetime(stock_data['date'])\n",
    "stock_data.to_csv('csv/stock_index.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stock_data=pd.read_csv('csv/stock_index.csv')\n",
    "stock_data['date']=pd.to_datetime(stock_data['date'])\n",
    "stock_data=stock_data.drop(['Unnamed: 0','index'], axis=1)\n",
    "stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 导入数据\n",
    "RAW=pd.read_csv(\"csv/stocknews1.csv\")\n",
    "RAW['date']=pd.to_datetime(RAW['date'])\n",
    "jianbao=RAW['title'].str.match(r'.+\\d{4}年\\d{2}月\\d{2}日.+简报')\n",
    "RAW=RAW.iloc[list(~jianbao),:]\n",
    "##合并数据\n",
    "data=pd.merge(RAW,stock_data[['date','tag']], left_on='date', right_on='date')\n",
    "y=data['tag'].to_numpy()\n",
    "##接下来处理文本数据，我们使用简单的词袋模型作为预测特征\n",
    "import jieba\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "with open('Chinese/stopword.txt','rt') as f:\n",
    "    stoplist=f.readlines()\n",
    "    stoplist=[w.replace('\\n','') for w in stoplist]\n",
    "\n",
    "def not_digit(w):\n",
    "    w=w.replace(',','')\n",
    "    if re.match(r'\\d+',w)!=None or re.match(r'\\d%',w)!=None or re.match(r'\\d*\\.\\d+',w)!=None:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def tokenize(w):\n",
    "    cut_w=jieba.cut(w)\n",
    "    ## 去除停用词\n",
    "    cut_w=[w.strip().lower() for w in cut_w if ((w not in stoplist) and not_digit(w) and len(w.strip())>0)]\n",
    "    return cut_w\n",
    "\n",
    "count_vect=CountVectorizer(tokenizer=tokenize,  min_df=5)\n",
    "bag_words=count_vect.fit_transform(data['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "##进行一个简单的screening（Fan, 2007）\n",
    "feature_y=bag_words.T@y\n",
    "feature_y=np.abs(feature_y.T/(bag_words.sum(axis=0))-0.5)\n",
    "print(\"总特征数=\",feature_y.shape)\n",
    "##只选取10%最相关的特征\n",
    "cut_off=np.percentile(feature_y,90)\n",
    "print(\"临界值为：\",cut_off)\n",
    "selected_tag=feature_y>cut_off\n",
    "selected_feature=bag_words[:,np.array(selected_tag.tolist()[0])]\n",
    "print(\"样本量，选取特征数=\",selected_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##训练模型\n",
    "print(\"开始训练模型......\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "##2000颗树，特征数量为总数量的log2\n",
    "rfc=RandomForestClassifier(n_estimators=2000, max_samples=0.8, max_features='log2')\n",
    "rfc.fit(selected_feature,y)\n",
    "print(\"模型训练完成......\")\n",
    "## 预测及概率\n",
    "data['prob']=rfc.predict_proba(selected_feature)[:,1]\n",
    "data['pred']=data['prob']>=0.5\n",
    "## 计算指标\n",
    "TP=np.sum(data['tag'] & data['pred'])\n",
    "TN=np.sum((~data['tag'] & (~data['pred'])))\n",
    "FP=np.sum((~data['tag'] & (data['pred'])))\n",
    "FN=np.sum((data['tag']) & (~data['pred']))\n",
    "print(\"预测完毕......\")\n",
    "print(\"TP=\",TP)\n",
    "print(\"TN=\",TN)\n",
    "print(\"FP=\",FP)\n",
    "print(\"FN=\",FN)\n",
    "print(\"查全率=敏感性=\",TP/(TP+FN))\n",
    "print(\"查准率=\",TP/(TP+FP))\n",
    "print(\"特异性=\",TN/(TN+FP))\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, threshold = roc_curve(data['tag'], data['prob'])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set()\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0)\n",
    "plt.plot(fpr,tpr,label='ROC, AUC=%.2f' % roc_auc)\n",
    "plt.legend(loc='upper left', frameon=True)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
