{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分析的基本概念\n",
    "\n",
    "近些年，随着算力和数据的逐渐积累，人们越来越多的关注文本，比如新闻、上市公司公告、国家机构公告等，这些文本中包含了大量的信息可以被挖掘。在经济学、金融学、会计学等领域，也有很多文献开始从文本数据中提取数据，与传统的统计、计量经济学等工具相结合，解决经济学中的问题。比如，Gentzkow, Kelly和Taddy（2017）就提供了一个使用文本作为数据的一个综述。使用文本分析的研究课题也越来越广，比如：\n",
    "\n",
    "* 政策不确定性：Baker, Blom和Davis（2016）；Benguria等（2022）\n",
    "* 政治/媒体倾向：Piotroski, Wong和Zhang（2017）；Gentzkow和Shapiro（2010）\n",
    "* 宏观经济：林建浩等（2021）\n",
    "* 公司金融、公司治理：罗进辉（2018）；汝毅、薛健和张乾（2019）；王雄元和高曦（2018）\n",
    "* 金融市场：阮睿等（2021）；Born, Ehrmann和Fratzscher（2014）\n",
    "* 股票价格、波动率\n",
    "\n",
    "而文本分析的方法也多种多样，比如：\n",
    "\n",
    "* 直接使用Baidu、Google的搜索指数，比如罗进辉（2018）\n",
    "* 从文本中找到相关词汇，比如山立威、甘犁和郑涛（2008）\n",
    "* 使用词频（或者TF-IDF），比如Piotroski, Wong和Zhang（2017）\n",
    "* 使用字符串长度，比如王雄元和高曦（2018）\n",
    "* 基于词典，比如阮睿等（2021）；Gentzkow和Shapiro（2010）;姚加权等（2021）\n",
    "* 情感分析，比如汝毅、薛健和张乾（2019）\n",
    "* 机器学，比如林建浩等（2021）\n",
    "\n",
    "与我们之前介绍的分析不同，文本数据是典型的非结构化数据，而且本身具有非常复杂的结构，相对之前的数据分析相比有其特有的分析技巧。这里我们将简单介绍文本分析的基本流程以及一些基础的工具。\n",
    "\n",
    "虽然差别很大，但是基本流程与之前的分析还是想通的，一般都需要如下步骤：\n",
    "\n",
    "1. 准备数据，在这一步除了要准备需要进行分析的数据之外，可能还需要准备额外的语料库（corpus）。\n",
    "2. 文本规范化处理，也就是我们之前清洗数据的步骤，比如分词、去除停用词、去除特殊符号等无意义字符、同义词转换、缩写转换等等。\n",
    "3. 特征工程，从已经清洗好的数据中提取特征。由于计算机只能处理数值型的变量，因而在这一步有一个比较关键的步骤是将文字转换为计算机可以理解的向量等数值型变量。\n",
    "4. 训练模型，经过这些步骤后，针对不同的目的，模型训练可能与之前的算法比较类似，但是也有针对文本数据特有的模型。\n",
    "5. 模型评价，评价模型的性能，重复以上步骤，改进模型。这一步有时涉及到人工评价模型，比如使用词典的方法就非常需要人口去查验效果。这一步非常关键，比如Baker, Blom和Davis（2016）就提供了人工查验数据的一个很好范例。\n",
    "\n",
    "针对文本数据，除了其他数据同样可以进行的相关性计算、聚类、分类等模型之外，还有一些任务是文本数据特有的，比如：\n",
    "\n",
    "* 分词\n",
    "* 词性标注\n",
    "* 词嵌入\n",
    "* 摘要和主题建模\n",
    "* 实体识别\n",
    "* 知识图谱\n",
    "* 语义分析\n",
    "* ......\n",
    "\n",
    "其中有的模型结果是其他模型的基础，比如分词、词性标注等是很多其他模型的基础。\n",
    "\n",
    "在本节，我们将主要使用Python中的**NLTK**（http://www.nltk.org ）、**Scikit-Learn**、**Jieba**（https://github.com/fxsjy/jieba ）、**Gensim**（https://github.com/RaRe-Technologies/gensim ）等工具介绍文本分析的基本原理。不过与此同时，自然语言处理，包括中文的自然语言处理正在蓬勃发展，很多新的工具可以使用，比如对标NLTK并且号称有更好性能的**spaCy**（ https://spacy.io ），以及已经经过预训练可以直接拿来用的模型比如最近如火如荼的**BERT**、**HanLP**（https://github.com/hankcs/HanLP ）、**Stanford CoreNLP**（https://github.com/stanfordnlp/CoreNLP ）等等，学习基本原理后可以直接使用这些包进行自己的研究。\n",
    "\n",
    "在这里我们从文本的规范化处理开始，介绍文本分析的基本原理和方法。\n",
    "\n",
    "此外，在这里可以提供一些文本分析、自然语言处理的比较好的学习资料：\n",
    "\n",
    "1. Anand, V., Bochkay, K., Chychyla, R., & Leone, A. J. (2020). Using Python for text analysis in accounting research. Vic Anand, Khrystyna Bochkay, Roman Chychyla and Andrew Leone (2020),\" Using Python for Text Analysis in Accounting Research\", Foundations and Trends®in Accounting, 14(3–4), 128–359.\n",
    "2. Sarkar, D. (2016). Text Analytics with python. Springer. （中文版：《Python文本分析》）\n",
    "\n",
    "**参考文献**\n",
    "1. Anand, V., Bochkay, K., Chychyla, R., & Leone, A. J. (2020). Using Python for text analysis in accounting research (Vol. 14, Issues 3–4).\n",
    "2. Baker, S. R., Bloom, N., & Davis, S. J. (2016). Measuring economic policy uncertainty. The Quarterly Journal of Economics, 131(4), 1593–1636.\n",
    "3. Benguria, F., Choi, J., Swenson, D. L., & Xu, M. J. (2022). Anxiety or pain? The impact of tariffs and uncertainty on Chinese firms in the trade war. Journal of International Economics, 103608.\n",
    "4. Born, B., Ehrmann, M., & Fratzscher, M. (2014). Central bank communication on financial stability. The Economic Journal, 124(577), 701–734.\n",
    "5. Gentzkow, M., Kelly, B., & Taddy, M. (2019). Text as data. Journal of Economic Literature, 57(3), 535–574.\n",
    "6. Gentzkow, M., & Shapiro, J. M. (2010). What drives media slant? Evidence from US daily newspapers. Econometrica, 78(1), 35–71.\n",
    "7. Piotroski, J. D., Wong, T. J., & Zhang, T. (2017). Political bias in corporate news: The role of conglomeration reform in China. The Journal of Law and Economics, 60(1), 173–207.\n",
    "8. 林建浩,陈良源,罗子豪,张一帆.央行沟通有助于改善宏观经济预测吗?——基于文本数据的高维稀疏建模[J].经济研究,2021,56(03):48-64.\n",
    "9. 罗进辉. 媒体报道与高管薪酬契约有效性[J]. 金融研究, 2018, 453(3): 190-206.\n",
    "10. 阮睿, 孙宇辰, 唐悦, 等. 资本市场开放能否提高企业信息披露质量?——基于 “沪港通” 和年报文本挖掘的分析[J]. 金融研究, 2021, 488(2): 188-206.\n",
    "11. 汝毅, 薛健, 张乾. 媒体新闻报道的声誉溢出效应[J]. 金融研究, 2019, 470(8): 189-206.\n",
    "12. 山立威, 甘犁, 郑涛. 公司捐款与经济动机[J]. 经济研究, 2008, 11: 51-60.\n",
    "13. 王雄元, 高曦. 年报风险披露与权益资本成本[J]. 金融研究, 2018, 451(1): 174-190.\n",
    "14. 姚加权, 冯绪, 王赞钧, 纪荣嵘, & 张维. (2021). 题名: 语调、情绪及市场影响:基于金融情绪词典. 管理科学学报, 24(5), 26–46.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本规范化\n",
    "\n",
    "文本是典型的非结构化数据，我们需要将文本转换为高度结构化的数据，首先要对文本进行有意义的划分，一般涉及到句子的**切分**（**tokenization**）以及其他清洗步骤。\n",
    "\n",
    "**标识**（**token**）是文本的有意义的最小成分，文本处理的最简单操作即将文本切成一个个的token，通常包括句子切分和词语切分。接下来我们很少有研究句子的成分和语义，更多时候是针对词语的分析，因而接下来主要介绍句子的切分方法。\n",
    "\n",
    "## 英文切分\n",
    "\n",
    "英文的词语切分一般比较简单，主要原因是因为英文的单词之间都有空格进行分割，而中文的切分就复杂很多。常见的自然语言处理包比如NKTL以及spaCy都肯定包含了切分的函数，比如在NLTK中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am working very hard to help New York City & State. Dealing with both Mayor & Governor and producing tremendously for them, including four new medical centers and four new hospitals. Fake News that I won’t help them because I don’t like Cuomo (I do). Just sent 4000 ventilators!'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I am working very hard to help New York City & State. Dealing with both Mayor & Governor and producing tremendously for them, including four new medical centers and four new hospitals. Fake News that I won’t help them because I don’t like Cuomo (I do). Just sent 4000 ventilators!\"\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'working', 'very', 'hard', 'to', 'help', 'New', 'York', 'City', '&', 'State', '.', 'Dealing', 'with', 'both', 'Mayor', '&', 'Governor', 'and', 'producing', 'tremendously', 'for', 'them', ',', 'including', 'four', 'new', 'medical', 'centers', 'and', 'four', 'new', 'hospitals', '.', 'Fake', 'News', 'that', 'I', 'won', '’', 't', 'help', 'them', 'because', 'I', 'don', '’', 't', 'like', 'Cuomo', '(', 'I', 'do', ')', '.', 'Just', 'sent', '4000', 'ventilators', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意运行以上命令可能先要下载相应的包：在Python解释器中运行：nltk.download('punkt') ，如果提示错误，可以参考：https://www.cnblogs.com/sddai/p/10543359.html\n",
    "\n",
    "当然NLTK不止支持这一种切分方法，比如我们可以使用正则表达式切分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'working', 'very', 'hard', 'to', 'help', 'New', 'York', 'City', 'State', 'Dealing', 'with', 'both', 'Mayor', 'Governor', 'and', 'producing', 'tremendously', 'for', 'them', 'including', 'four', 'new', 'medical', 'centers', 'and', 'four', 'new', 'hospitals', 'Fake', 'News', 'that', 'I', 'won’t', 'help', 'them', 'because', 'I', 'don’t', 'like', 'Cuomo', 'I', 'do', 'Just', 'sent', '4000', 'ventilators']\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "\n",
    "Tokenizer = RegexpTokenizer(pattern=r\"[\\w\\-’']+\")\n",
    "words = Tokenizer.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外还可以使用空白字符（空格、缩进、换行）等进行切分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'working', 'very', 'hard', 'to', 'help', 'New', 'York', 'City', '&', 'State.', 'Dealing', 'with', 'both', 'Mayor', '&', 'Governor', 'and', 'producing', 'tremendously', 'for', 'them,', 'including', 'four', 'new', 'medical', 'centers', 'and', 'four', 'new', 'hospitals.', 'Fake', 'News', 'that', 'I', 'won’t', 'help', 'them', 'because', 'I', 'don’t', 'like', 'Cuomo', '(I', 'do).', 'Just', 'sent', '4000', 'ventilators!']\n"
     ]
    }
   ],
   "source": [
    "from nltk import WhitespaceTokenizer\n",
    "\n",
    "Tokenizer = WhitespaceTokenizer()\n",
    "words = Tokenizer.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "英文分词虽然原理简单，但是还是有很多细节的坑，比如上面Trump先生的「don’t」和「don't」，如果在切分或者其他清洗步骤中予以重视，计算机会认为这是两个词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文分词\n",
    "\n",
    "中文文本分析与英文的文本分析一个最重要的区别在于，英文使用空格分割每个单词，但是中文没有分割单词的概念。\n",
    "\n",
    "为了克服这个问题，分词就应运而生了。结合字典和算法，分词软件可以帮助我们将中文的文章、句子分解为一个个的中文单词。\n",
    "\n",
    "目前已经有很多成熟的分词工具，比如中科院的NLPIR汉语分词系统、结巴分词以及腾讯、阿里、百度的分词系统等等。在这里我们以开源的结巴分词为例，介绍分词工具的用法。\n",
    "\n",
    "为了使用结巴分词，首先需要安装。在terminal中输入：\n",
    "```shell\n",
    "pip install jieba\n",
    "```\n",
    "\n",
    "就可以进行安装了。安装好之后，可以将jieba模块导入到Python程序中，就可以正常使用了：\n",
    "```shell\n",
    "import jieba\n",
    "```\n",
    "\n",
    "比如，最简单的用法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.283 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年以来', '，', '我国', '持续', '推进', '减税', '降费', '、', '提高', '最低工资', '标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人所得税', '改革', '以及', '专项', '附加', '扣除', '方案', '，', '有效', '增加', '了', '居民', '可', '支配', '收入', '。', '与此同时', '，', '不断', '消除', '居民消费', '的', '后顾之忧', '。', '消费', '需求', '进一步', '释放', '，', '消费市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "line = \"今年以来，我国持续推进减税降费、提高最低工资标准、促进就业，特别是年初开始实施的个人所得税改革以及专项附加扣除方案，有效增加了居民可支配收入。与此同时，不断消除居民消费的后顾之忧。消费需求进一步释放，消费市场亮点纷呈\"\n",
    "wlist = jieba.cut(line)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cut()函数有三个参数：必须要提供的是需要进行分词的字符串；此外，cut_all参数控制是否采用全模式；HMM参数用来控制是否使用HMM模型。其区别是：\n",
    "\n",
    "* cut_all=True， 代表使用全模式，全模式可以切出混合不同粒度的词\n",
    "* HMM=True，代表使用HMM模型，用于推断字典中没有的词\n",
    "\n",
    "比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年', '今年以来', '以来', '，', '我国', '持续', '推进', '减税', '降', '费', '、', '提高', '最低', '最低工资', '低工资', '工资', '工资标准', '标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人', '个人所得', '个人所得税', '所得', '所得税', '税改', '改革', '以及', '专项', '附加', '扣除', '方案', '，', '有效', '增加', '了', '居民', '可支配', '支配', '收入', '。', '与此', '与此同时', '同时', '，', '不断', '消除', '居民', '居民消费', '消费', '的', '后顾之忧', '。', '消费', '需求', '求进', '进一步', '一步', '释放', '，', '消费', '消费市场', '市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "wlist = jieba.cut(line, cut_all=True)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年以来', '，', '我国', '持续', '推进', '减税', '降费', '、', '提高', '最低工资', '标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人所得税', '改革', '以及', '专项', '附加', '扣除', '方案', '，', '有效', '增加', '了', '居民', '可', '支配', '收入', '。', '与此同时', '，', '不断', '消除', '居民消费', '的', '后顾之忧', '。', '消费', '需求', '进一步', '释放', '，', '消费市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "wlist = jieba.cut(line, HMM=True)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年', '今年以来', '以来', '，', '我国', '持续', '推进', '减税', '降', '费', '、', '提高', '最低', '最低工资', '低工资', '工资', '工资标准', '标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人', '个人所得', '个人所得税', '所得', '所得税', '税改', '改革', '以及', '专项', '附加', '扣除', '方案', '，', '有效', '增加', '了', '居民', '可支配', '支配', '收入', '。', '与此', '与此同时', '同时', '，', '不断', '消除', '居民', '居民消费', '消费', '的', '后顾之忧', '。', '消费', '需求', '求进', '进一步', '一步', '释放', '，', '消费', '消费市场', '市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "wlist = jieba.cut(line, HMM=True, cut_all=True)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，上面的分词结果......一言难尽。实际上，任何分词算法都不可避免的不能跟上时代的潮流，特别是网络时代，新的词语层出不穷，而在一些专业领域中，一些专有名词往往普通词典无法完全覆盖。比如“减费降税”、“可支配收入”这些专有名词，都没有被正确分出来。\n",
    "\n",
    "为了克服这个问题，往往需要用户自己添加字典。比如，我们可以把“减费降税”、“可支配收入”这些名词放在一个文本文件中，每个新词写成一行，然后使用load_userdict()函数给定这个文件，就可以添加自己的新词列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年以来', '，', '我国', '持续', '推进', '减税降费', '、', '提高', '最低工资标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人所得税', '改革', '以及', '专项附加扣除', '方案', '，', '有效', '增加', '了', '居民', '可支配收入', '。', '与此同时', '，', '不断', '消除', '居民消费', '的', '后顾之忧', '。', '消费需求', '进一步', '释放', '，', '消费市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "with open('user_dict.txt', 'wt') as f:\n",
    "    f.write(\"减税降费\\n可支配收入\\n最低工资\\n最低工资标准\\n专项附加扣除\\n消费需求\")\n",
    "jieba.load_userdict('user_dict.txt')\n",
    "wlist = jieba.cut(line)\n",
    "print(list(wlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入了用户字典后，新的分词更加准确了。在实际应用的时候，无论使用什么分词工具，用户字典的构建往往是非常关键的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大小写转换\n",
    "\n",
    "主要针对英文等字母文字，一般的做法是统一转换为小写字母，避免出现「FAKE NEWS$\\neq$fake news」的情况出现。可以使用字符串的.lower()方法很容易的完成大小写转换："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'working', 'very', 'hard', 'to', 'help', 'new', 'york', 'city', '&', 'state.', 'dealing', 'with', 'both', 'mayor', '&', 'governor', 'and', 'producing', 'tremendously', 'for', 'them,', 'including', 'four', 'new', 'medical', 'centers', 'and', 'four', 'new', 'hospitals.', 'fake', 'news', 'that', 'i', 'won’t', 'help', 'them', 'because', 'i', 'don’t', 'like', 'cuomo', '(i', 'do).', 'just', 'sent', '4000', 'ventilators!']\n"
     ]
    }
   ],
   "source": [
    "words = [w.lower() for w in words]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 删除停用词和特殊字符\n",
    "\n",
    "分词之后，在进一步进行处理之前，消除停用词和特殊符号往往是非常关键的。比如在上面的分词结果中，“的”、“是”、“与此同时”等，含义并不是非常明显，对其分析的意义不大，留着这些词只会空占内存，并且可能对分析结果产生巨大影响。一个常用的做法是，使用一个停用词列表，分词结束之后，把停用词列表中的词全都剔除出去。\n",
    "\n",
    "比如，在文本文档“Chinese/stopword.txt”中，我们列举出了一些常用的停用词以及特殊字符，我们可以使用如下方法消除停用词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年以来', '，', '我国', '持续', '推进', '减税降费', '、', '提高', '最低工资标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个人所得税', '改革', '以及', '专项附加扣除', '方案', '，', '有效', '增加', '了', '居民', '可支配收入', '。', '与此同时', '，', '不断', '消除', '居民消费', '的', '后顾之忧', '。', '消费需求', '进一步', '释放', '，', '消费市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "with open('Chinese/stopword.txt', 'rt') as f:\n",
    "    stoplist = f.readlines()\n",
    "    stoplist = set([w.lower() for w in stoplist])\n",
    "wlist = jieba.cut(line)\n",
    "wlist = [w.lower() for w in wlist if w.lower() not in stoplist]\n",
    "print(wlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意上面的函数里面我们还同时对停用词和词语转换为了小写。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 扩展缩写词、同义词转换\n",
    "\n",
    "缩写词即诸如：「isn't==is not」之类的缩写，一般而言也需要特殊处理。一般来说我们可以通过定义一个映射关系来处理这种情况，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年以来', '，', '我国', '持续', '推进', '减税降费', '、', '提高', '最低工资标准', '、', '促进', '就业', '，', '特别', '是', '年初', '开始', '实施', '的', '个税', '改革', '以及', '专项附加扣除', '方案', '，', '有效', '增加', '了', '居民', '可支配收入', '。', '与此同时', '，', '不断', '消除', '居民消费', '的', '后顾之忧', '。', '消费需求', '进一步', '释放', '，', '消费市场', '亮点', '纷呈']\n"
     ]
    }
   ],
   "source": [
    "syno = {\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"个人所得税\": \"个税\"\n",
    "}\n",
    "wlist = [syno[w] if w in syno.keys() else w for w in wlist]\n",
    "print(wlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，还有一些更加细致的工作，比如检查拼写错误、矫正重复字符等等，这些都需要大量细致的工作，特别是针对不同的应用场景进行特定的优化是非常有必要的。\n",
    "\n",
    "而对于英文，由于英文存在时态、单复数的问题，一个常见的操作是使用stemmer算法将其转化为词根，比如economics, economically, economic等都代表同一个词，所以可以使用相同的词根来表示。对于英文，比较常见的算法是所谓的Porter Stemmer，在NLTK中可以直接调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "econom\n",
      "econom\n",
      "econom\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "print(porter.stem('economically'))\n",
    "print(porter.stem('economics'))\n",
    "print(porter.stem('economic'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如此我们就有了三个词的共同词根：econom。使用stemmer可以极大的减少词的个数，从而达到降维的目的。比如对于以上英文，我们可以先分词、转化大小写再进行stem操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'work', 'veri', 'hard', 'to', 'help', 'new', 'york', 'citi', '&', 'state', '.', 'deal', 'with', 'both', 'mayor', '&', 'governor', 'and', 'produc', 'tremend', 'for', 'them', ',', 'includ', 'four', 'new', 'medic', 'center', 'and', 'four', 'new', 'hospit', '.', 'fake', 'new', 'that', 'i', 'won', '’', 't', 'help', 'them', 'becaus', 'i', 'don', '’', 't', 'like', 'cuomo', '(', 'i', 'do', ')', '.', 'just', 'sent', '4000', 'ventil', '!']\n"
     ]
    }
   ],
   "source": [
    "words = [porter.stem(w).lower() for w in word_tokenize(sentence)]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而对于中文，同样也可以进行同义词的转换步骤，如果需要，https://github.com/fighting41love/funNLP/tree/master/data 中提供了一个可用的近义词表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个简单的例子：中文词频统计\n",
    "\n",
    "以下实现了对《越女剑》的词频统计："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>范蠡</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>道</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>剑士</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>青衣</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>阿青</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>勾践</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>锦衫</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>长剑</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>说道</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>吴国</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word  frequency\n",
       "0   范蠡        118\n",
       "1    道        113\n",
       "2   剑士        105\n",
       "3   青衣         47\n",
       "4   阿青         47\n",
       "5   勾践         44\n",
       "6   锦衫         35\n",
       "7   长剑         34\n",
       "8   说道         31\n",
       "9   吴国         30"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "## 停用词\n",
    "with open('Chinese/stopword.txt', 'rt') as f:\n",
    "    stoplist = f.readlines()\n",
    "    stoplist = [w.lower().strip() for w in stoplist]\n",
    "## 读入小说\n",
    "wordlist = []\n",
    "with open('Chinese/越女剑.txt', 'rt') as f:\n",
    "    for l in f:\n",
    "        line_cut = jieba.cut(l)\n",
    "        line = [w.strip().lower() for w in line_cut]\n",
    "        wordlist.extend(line)\n",
    "wordlist = [w.lower() for w in wordlist if w.lower() not in stoplist]\n",
    "## 统计\n",
    "text_dict = dict()\n",
    "for l in wordlist:\n",
    "    if l not in text_dict:\n",
    "        text_dict[l] = 1\n",
    "    else:\n",
    "        text_dict[l] += 1\n",
    "text_freq = []\n",
    "for k in text_dict:\n",
    "    text_freq.append((k, text_dict[k]))\n",
    "## 排序\n",
    "text_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "## 用pandas显示，更好看\n",
    "import pandas as pd\n",
    "\n",
    "freq = pd.DataFrame(text_freq, columns=['word', 'frequency'])\n",
    "freq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词典与词频的结合\n",
    "\n",
    "我们可以将词典与词频相结合，构造一些需要的指标。比如Benguria等（2022）就使用了类似的方法研究贸易政策不确定性。\n",
    "\n",
    "这里，作为示例，我们仿照Benguria等（2022）的方法，使用（部分）金融新闻的标题数据，构造一个宏观政策不确定性的指数。\n",
    "\n",
    "在Benguria等（2022），他们使用了两个不同的词典：\n",
    "* 贸易政策词典：国际贸易、出口、进口、关税、壁垒、反倾销、外包、保护主义、单边主义\n",
    "* 不确定性词典：不确定、不明确、不明朗、未明、（不明）、难料、难以估计、难以预计、难以预测、难以预料、风险、危险、危机、威胁、未知\n",
    "\n",
    "我们仿照他们的做法，不过选择一个与宏观经济政策有关的词典：\n",
    "* 宏观政策词典：政策、宏观、货币、财政、土地\n",
    "\n",
    "Benguria等（2022）使用了完整的报告，我们这里暂时只用标题。\n",
    "\n",
    "首先导入数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59021767</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>沪争取增值税扩围改革试点</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59021769</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>周小川：外部施压不会影响人民币升值步伐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59021771</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>准备金率再上调 达到19.5％创新高</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59021774</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>定基价格指数若涨20% 政府或出手调控</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59021776</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>政策倾斜和加大投入 西藏将做大做强藏药产业</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       date                  title\n",
       "0  59021767 2011-02-19           沪争取增值税扩围改革试点\n",
       "1  59021769 2011-02-19    周小川：外部施压不会影响人民币升值步伐\n",
       "2  59021771 2011-02-19     准备金率再上调 达到19.5％创新高\n",
       "4  59021774 2011-02-19    定基价格指数若涨20% 政府或出手调控\n",
       "5  59021776 2011-02-19  政策倾斜和加大投入 西藏将做大做强藏药产业"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "StockNews = pd.read_csv(\"csv/stocknews1.csv\")\n",
    "StockNews['date'] = pd.to_datetime(StockNews['date'])\n",
    "jianbao = StockNews['title'].str.match(\n",
    "    r'(.+\\d{4}年\\d{2}月\\d{2}日.+简报|.*二级市场.*简报|.*业绩公报.*|.*登记日期.*|.*交易安排表.*)')\n",
    "StockNews = StockNews[~jianbao]\n",
    "StockNews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来定义模式，如果在句子中同时存在不确定性词典的词，以及宏观政策词典的词，那么就认为这是一条关于宏观政策不确定新的标题，具体做法可以使用正则表达式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(.*不确定.*|.*不明确.*|.*不明朗.*|.*未明.*|.*不明.*|.*难料.*|.*难以估计.*|.*难以预计.*|.*难以预测.*|.*难以预料.*|.*风险.*|.*危险.*|.*危机.*|.*威胁.*|.*未知.*)\n",
      "(.*政策.*|.*宏观.*|.*货币.*|.*财政.*|.*土地.*|.*经济.*|.*改革.*|.*中国.*)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2612"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncertainty_dict = [\n",
    "    \"不确定\", \"不明确\", \"不明朗\", \"未明\", \"不明\", \"难料\", \"难以估计\", \"难以预计\", \"难以预测\", \"难以预料\",\n",
    "    \"风险\", \"危险\", \"危机\", \"威胁\", \"未知\"\n",
    "]\n",
    "macro_dict = [\"政策\", \"宏观\", \"货币\", \"财政\", \"土地\", \"经济\", \"改革\", \"中国\"]\n",
    "\n",
    "uncertainty_dict=['.*'+d+'.*' for d in uncertainty_dict]\n",
    "macro_dict=['.*'+d+'.*' for d in macro_dict]\n",
    "uncertainty_re = \"(\" + '|'.join(uncertainty_dict) + \")\"\n",
    "macro_re = \"(\" + '|'.join(macro_dict) + \")\"\n",
    "print(uncertainty_re)\n",
    "print(macro_re)\n",
    "\n",
    "StockNews['uncertainty'] = StockNews['title'].str.match(uncertainty_re)\n",
    "StockNews['macro'] = StockNews['title'].str.match(macro_re)\n",
    "StockNews['macro_uncertainty'] = StockNews['uncertainty'] & StockNews['macro']\n",
    "StockNews['macro_uncertainty'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们根据年份对不确定性进行一个加总："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>macro_uncertainty</th>\n",
       "      <th>count</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>0.0</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>44</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>109</td>\n",
       "      <td>4.784689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>19.0</td>\n",
       "      <td>4489</td>\n",
       "      <td>4.140336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>68.0</td>\n",
       "      <td>16542</td>\n",
       "      <td>4.086047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>80.0</td>\n",
       "      <td>35024</td>\n",
       "      <td>2.277645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>136.0</td>\n",
       "      <td>63149</td>\n",
       "      <td>2.150232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>95.0</td>\n",
       "      <td>40383</td>\n",
       "      <td>2.346664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>70.0</td>\n",
       "      <td>35759</td>\n",
       "      <td>1.952090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>127.0</td>\n",
       "      <td>87728</td>\n",
       "      <td>1.446008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>151.0</td>\n",
       "      <td>49529</td>\n",
       "      <td>3.042576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>934.0</td>\n",
       "      <td>211787</td>\n",
       "      <td>4.408010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>277.0</td>\n",
       "      <td>63917</td>\n",
       "      <td>4.326976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>227.0</td>\n",
       "      <td>69334</td>\n",
       "      <td>3.269292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>87.0</td>\n",
       "      <td>45786</td>\n",
       "      <td>1.896003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>136.0</td>\n",
       "      <td>43196</td>\n",
       "      <td>3.141168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>153.0</td>\n",
       "      <td>53787</td>\n",
       "      <td>2.839275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>12.0</td>\n",
       "      <td>6154</td>\n",
       "      <td>1.918772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>39.0</td>\n",
       "      <td>23263</td>\n",
       "      <td>1.669306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      macro_uncertainty   count     ratio\n",
       "year                                     \n",
       "1900                0.0      41  0.000000\n",
       "1986                0.0       1  0.000000\n",
       "1992                0.0       1  0.000000\n",
       "1993                0.0      13  0.000000\n",
       "1994                0.0      16  0.000000\n",
       "1995                0.0      33  0.000000\n",
       "1996                0.0      38  0.000000\n",
       "1997                0.0      44  0.000000\n",
       "1998                0.0      30  0.000000\n",
       "1999                1.0     109  4.784689\n",
       "2000               19.0    4489  4.140336\n",
       "2001               68.0   16542  4.086047\n",
       "2002               80.0   35024  2.277645\n",
       "2003              136.0   63149  2.150232\n",
       "2004               95.0   40383  2.346664\n",
       "2005               70.0   35759  1.952090\n",
       "2006              127.0   87728  1.446008\n",
       "2007              151.0   49529  3.042576\n",
       "2008              934.0  211787  4.408010\n",
       "2009              277.0   63917  4.326976\n",
       "2010              227.0   69334  3.269292\n",
       "2011               87.0   45786  1.896003\n",
       "2012              136.0   43196  3.141168\n",
       "2013              153.0   53787  2.839275\n",
       "2014               12.0    6154  1.918772\n",
       "2015               39.0   23263  1.669306"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StockNews['year'] = pd.DatetimeIndex(StockNews['date']).year\n",
    "aggregate_index = StockNews[['year',\n",
    "                             'macro_uncertainty']].groupby('year').sum()\n",
    "aggregate_index['count'] = StockNews[['year', 'macro_uncertainty'\n",
    "                                      ]].groupby('year').count()\n",
    "aggregate_index['ratio'] = aggregate_index['macro_uncertainty'] / (\n",
    "    100 + aggregate_index['count']) * 1000\n",
    "aggregate_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取特征\n",
    "\n",
    "由于计算机只能处理数值变量进行运算，在文本数据清洗完毕后，接下来通常需要将其转换为计算机可以识别的向量。目前有多重方法可以完成这项任务，我们在这里主要介绍其中常见的三种：词袋、TF-IDF以及词嵌入三种模型。\n",
    "\n",
    "## 词袋模型\n",
    "\n",
    "**词袋**（**bag of words**）模型是最基础的一种将文本数据结构化为向量的一种方法。实际上词袋可以简单理解为每个文本的词频统计。比如，我们接下来使用一些上市公司的标题数据，并使用上面介绍的方法将每个标题转换为向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59021767</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>沪争取增值税扩围改革试点</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59021769</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>周小川：外部施压不会影响人民币升值步伐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59021771</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>准备金率再上调 达到19.5％创新高</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59021772</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>浙江亚太药业股份有限公司股票2011年02月14日2011年02月18日二级市场表现周简报</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59021774</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>定基价格指数若涨20% 政府或出手调控</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59021776</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>政策倾斜和加大投入 西藏将做大做强藏药产业</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59021778</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>新疆13个地州探矿权年 590个项目涉嫌“圈而不探”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>59021779</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>沪士电子股份有限公司股票2011年02月14日2011年02月18日二级市场表现周简报</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>59021783</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>北京：楼市限购首日成交环比降9成</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>59021787</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>上海：房管局发布“沪九条”限购执行细则</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       date                                          title\n",
       "0  59021767 2011-02-19                                   沪争取增值税扩围改革试点\n",
       "1  59021769 2011-02-19                            周小川：外部施压不会影响人民币升值步伐\n",
       "2  59021771 2011-02-19                             准备金率再上调 达到19.5％创新高\n",
       "3  59021772 2011-02-19  浙江亚太药业股份有限公司股票2011年02月14日2011年02月18日二级市场表现周简报\n",
       "4  59021774 2011-02-19                            定基价格指数若涨20% 政府或出手调控\n",
       "5  59021776 2011-02-19                          政策倾斜和加大投入 西藏将做大做强藏药产业\n",
       "6  59021778 2011-02-19                     新疆13个地州探矿权年 590个项目涉嫌“圈而不探”\n",
       "7  59021779 2011-02-19    沪士电子股份有限公司股票2011年02月14日2011年02月18日二级市场表现周简报\n",
       "8  59021783 2011-02-19                               北京：楼市限购首日成交环比降9成\n",
       "9  59021787 2011-02-19                            上海：房管局发布“沪九条”限购执行细则"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAW = pd.read_csv(\"csv/stocknews1.csv\")\n",
    "RAW['date'] = pd.to_datetime(RAW['date'])\n",
    "RAW.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们额外进行一些处理，注意到其中的类似「2011年02月14日2011年02月18日二级市场表现周简报」之类的title很没有营养，我们打算去除他，可以使用正则表达式方便的达到目的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59021767</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>沪争取增值税扩围改革试点</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59021769</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>周小川：外部施压不会影响人民币升值步伐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59021771</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>准备金率再上调 达到19.5％创新高</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59021774</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>定基价格指数若涨20% 政府或出手调控</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59021776</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>政策倾斜和加大投入 西藏将做大做强藏药产业</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134769</th>\n",
       "      <td>19839999</td>\n",
       "      <td>2006-12-27</td>\n",
       "      <td>定价基准不同 新利率基准将冲击旧浮动利率债</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134770</th>\n",
       "      <td>19840000</td>\n",
       "      <td>2006-12-27</td>\n",
       "      <td>债市延续调整格局 投资者宜缩短投资久期</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134771</th>\n",
       "      <td>19840023</td>\n",
       "      <td>2006-12-27</td>\n",
       "      <td>曾培炎:利用外汇储备优势</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134772</th>\n",
       "      <td>19840026</td>\n",
       "      <td>2006-12-27</td>\n",
       "      <td>广州小时最低工资标准7.5元</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134773</th>\n",
       "      <td>19840032</td>\n",
       "      <td>2006-12-27</td>\n",
       "      <td>左小蕾:警惕股市系统性风险</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>850153 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id       date                  title\n",
       "0        59021767 2011-02-19           沪争取增值税扩围改革试点\n",
       "1        59021769 2011-02-19    周小川：外部施压不会影响人民币升值步伐\n",
       "2        59021771 2011-02-19     准备金率再上调 达到19.5％创新高\n",
       "4        59021774 2011-02-19    定基价格指数若涨20% 政府或出手调控\n",
       "5        59021776 2011-02-19  政策倾斜和加大投入 西藏将做大做强藏药产业\n",
       "...           ...        ...                    ...\n",
       "1134769  19839999 2006-12-27  定价基准不同 新利率基准将冲击旧浮动利率债\n",
       "1134770  19840000 2006-12-27    债市延续调整格局 投资者宜缩短投资久期\n",
       "1134771  19840023 2006-12-27           曾培炎:利用外汇储备优势\n",
       "1134772  19840026 2006-12-27         广州小时最低工资标准7.5元\n",
       "1134773  19840032 2006-12-27         左小蕾:警惕股市系统性风险 \n",
       "\n",
       "[850153 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jianbao = RAW['title'].str.match(r'(.+\\d{4}年\\d{2}月\\d{2}日.+简报|.*二级市场.*简报|.*业绩公报.*|.*登记日期.*|.*交易安排表.*)')\n",
    "RAW1 = RAW.iloc[list(~jianbao), :]\n",
    "RAW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便起见，我们选取其中的前十条先进行分析："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59021767</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>沪争取增值税扩围改革试点</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59021769</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>周小川：外部施压不会影响人民币升值步伐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59021771</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>准备金率再上调 达到19.5％创新高</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59021774</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>定基价格指数若涨20% 政府或出手调控</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59021776</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>政策倾斜和加大投入 西藏将做大做强藏药产业</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59021778</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>新疆13个地州探矿权年 590个项目涉嫌“圈而不探”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>59021783</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>北京：楼市限购首日成交环比降9成</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>59021787</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>上海：房管局发布“沪九条”限购执行细则</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>59021789</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>6.5781！人民币升值容忍度继续提高？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>59021791</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>2010年上海商品住宅销售降四成</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id       date                       title\n",
       "0   59021767 2011-02-19                沪争取增值税扩围改革试点\n",
       "1   59021769 2011-02-19         周小川：外部施压不会影响人民币升值步伐\n",
       "2   59021771 2011-02-19          准备金率再上调 达到19.5％创新高\n",
       "4   59021774 2011-02-19         定基价格指数若涨20% 政府或出手调控\n",
       "5   59021776 2011-02-19       政策倾斜和加大投入 西藏将做大做强藏药产业\n",
       "6   59021778 2011-02-19  新疆13个地州探矿权年 590个项目涉嫌“圈而不探”\n",
       "8   59021783 2011-02-19            北京：楼市限购首日成交环比降9成\n",
       "9   59021787 2011-02-19         上海：房管局发布“沪九条”限购执行细则\n",
       "10  59021789 2011-02-19        6.5781！人民币升值容忍度继续提高？\n",
       "11  59021791 2011-02-19            2010年上海商品住宅销售降四成"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = RAW1.iloc[:10, :]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['沪', '增值税', '扩围', '改革', '试点'],\n",
       " ['周小川', '外部', '施压', '影响', '人民币', '升值', '步伐'],\n",
       " ['准备金率', '上调', '19.5', '创新', '高'],\n",
       " ['定基', '价格指数', '若涨', '20%', '政府', '出手', '调控'],\n",
       " ['政策', '倾斜', '加大', '投入', '西藏', '做', '做', '强', '藏药', '产业'],\n",
       " ['新疆', '地州', '探矿权', '590', '项目', '涉嫌', '圈', '不探'],\n",
       " ['北京', '楼市', '限购', '首日', '成交', '环', '比降', '成'],\n",
       " ['上海', '房管局', '发布', '沪', '九条', '限购', '执行', '细则'],\n",
       " ['6.5781', '人民币', '升值', '容忍度', '提高'],\n",
       " ['2010', '上海', '商品住宅', '销售', '降', '四成']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 首先进行分词、去除停用词等\n",
    "import jieba\n",
    "\n",
    "with open('Chinese/stopword.txt', 'rt') as f:\n",
    "    stoplist = f.readlines()\n",
    "    stoplist = [w.replace('\\n', '') for w in stoplist]\n",
    "\n",
    "\n",
    "def tokenize(w):\n",
    "    cut_w = jieba.cut(w)\n",
    "    ## 去除停用词\n",
    "    cut_w = [\n",
    "        w.strip().lower() for w in cut_w\n",
    "        if w.strip().lower() not in stoplist and len(w.strip()) > 0\n",
    "    ]\n",
    "    return cut_w\n",
    "\n",
    "\n",
    "tokenized_data = map(tokenize, data['title'])\n",
    "tokenized_data = list(tokenized_data)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>沪</th>\n",
       "      <th>增值税</th>\n",
       "      <th>扩围</th>\n",
       "      <th>改革</th>\n",
       "      <th>试点</th>\n",
       "      <th>周小川</th>\n",
       "      <th>外部</th>\n",
       "      <th>施压</th>\n",
       "      <th>影响</th>\n",
       "      <th>人民币</th>\n",
       "      <th>...</th>\n",
       "      <th>执行</th>\n",
       "      <th>细则</th>\n",
       "      <th>6.5781</th>\n",
       "      <th>容忍度</th>\n",
       "      <th>提高</th>\n",
       "      <th>2010</th>\n",
       "      <th>商品住宅</th>\n",
       "      <th>销售</th>\n",
       "      <th>降</th>\n",
       "      <th>四成</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     沪  增值税   扩围   改革   试点  周小川   外部   施压   影响  人民币  ...   执行   细则  6.5781  \\\n",
       "0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  1.0  ...  0.0  0.0     0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "7  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0     0.0   \n",
       "8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0     1.0   \n",
       "9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "\n",
       "   容忍度   提高  2010  商品住宅   销售    降   四成  \n",
       "0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "5  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "6  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "7  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "8  1.0  1.0   0.0   0.0  0.0  0.0  0.0  \n",
       "9  0.0  0.0   1.0   1.0  1.0  1.0  1.0  \n",
       "\n",
       "[10 rows x 63 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 接下来进行词频统计\n",
    "\n",
    "\n",
    "def word_freq(wlist):\n",
    "    freq = {}\n",
    "    for w in wlist:\n",
    "        if w in freq:\n",
    "            freq[w] += 1\n",
    "        else:\n",
    "            freq[w] = 1\n",
    "\n",
    "    return freq\n",
    "\n",
    "\n",
    "freqs = map(word_freq, tokenized_data)\n",
    "## 放在pandas里\n",
    "pd_freqs = pd.DataFrame(freqs)\n",
    "## 把NaN换成0\n",
    "pd_freqs = pd_freqs.fillna(0)\n",
    "pd_freqs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如此，一个简单的词袋模型就完成了。\n",
    "\n",
    "不过，可以看到虽然我们只有10条新闻，这个矩阵已经很大了。如果10万条新闻一起来，不仅词（列）多，而且行也多，最终这个矩阵的规模会变的非常巨大，甚至可能很轻易的会占满内存。\n",
    "\n",
    "然而注意到，这个矩阵里面多数的值都是0，这种类型的矩阵我们成为**稀疏矩阵**（**sparse matrix**），在SciPy和Pandas里面都提供了稀疏矩阵的存储结构和运算，所以更好的办法是使用稀疏矩阵进行存储："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>沪</th>\n",
       "      <th>增值税</th>\n",
       "      <th>扩围</th>\n",
       "      <th>改革</th>\n",
       "      <th>试点</th>\n",
       "      <th>周小川</th>\n",
       "      <th>外部</th>\n",
       "      <th>施压</th>\n",
       "      <th>影响</th>\n",
       "      <th>人民币</th>\n",
       "      <th>...</th>\n",
       "      <th>执行</th>\n",
       "      <th>细则</th>\n",
       "      <th>6.5781</th>\n",
       "      <th>容忍度</th>\n",
       "      <th>提高</th>\n",
       "      <th>2010</th>\n",
       "      <th>商品住宅</th>\n",
       "      <th>销售</th>\n",
       "      <th>降</th>\n",
       "      <th>四成</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     沪  增值税   扩围   改革   试点  周小川   外部   施压   影响  人民币  ...   执行   细则  6.5781  \\\n",
       "0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  1.0  ...  0.0  0.0     0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "6  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "7  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0     0.0   \n",
       "8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0     1.0   \n",
       "9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0     0.0   \n",
       "\n",
       "   容忍度   提高  2010  商品住宅   销售    降   四成  \n",
       "0  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "5  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "6  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "7  0.0  0.0   0.0   0.0  0.0  0.0  0.0  \n",
       "8  1.0  1.0   0.0   0.0  0.0  0.0  0.0  \n",
       "9  0.0  0.0   1.0   1.0  1.0  1.0  1.0  \n",
       "\n",
       "[10 rows x 63 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_freqs = pd_freqs.astype(pd.SparseDtype(\"float\", 0))\n",
    "sparse_freqs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的意思是通过类型转换，把数据框转换为稀疏类型，其中的「0」就不存储了，虽然看起来没有变化，但是如果我们查看类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "沪       Sparse[float64, 0]\n",
       "增值税     Sparse[float64, 0]\n",
       "扩围      Sparse[float64, 0]\n",
       "改革      Sparse[float64, 0]\n",
       "试点      Sparse[float64, 0]\n",
       "               ...        \n",
       "2010    Sparse[float64, 0]\n",
       "商品住宅    Sparse[float64, 0]\n",
       "销售      Sparse[float64, 0]\n",
       "降       Sparse[float64, 0]\n",
       "四成      Sparse[float64, 0]\n",
       "Length: 63, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_freqs.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "会发现都变成了Sparse的float64类型，且0不存储。\n",
    "\n",
    "当然，以上是自己手写的词袋生成步骤，实际上很多自然语言处理包已经有比较成熟的词袋处理机制。比如在Scikit-Learn中已经准备好了词袋的提取函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['##', 'a', 'ain', 'aren', 'c', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'i', 'isn', 'lex', 'll', 'm', 'mon', 'null', 's', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn', '±', '÷', 'β', 'δ', 'λ', 'ξ', 'ψ', 'в', '′', '″', 'ⅲ', '∈', '∧', '∪', '─', '☆', '一会', '一关', '一城', '一堆', '一对', '一批', '一方', '一期', '一村', '一根', '一派', '一班', '一百', '一眼', '一科', '一群', '一遍', '一道', '一部', '一集', '一页', '一颗', '三鲜', '为什', '什', '倒', '傥', '元', '元素', '先', '关', '兼', '前', '单元', '吨', '唷', '啪', '啷', '喔', '喜欢', '外', '多年', '大节', '大道', '大面儿', '天', '始', '子弹', '後', '抗拒', '敞开', '新', '昉', '更远', '有意', '有趣', '末', '次', '毫无保留', '波', '漫', '特', '特别', '理', '皆', '目前为止', '笑', '第三', '第五', '第四', '策略', '讲', '设', '话', '说', '赶早', '赶晚', '达', '钱', '限', '非', '面', '题', '麽', 'ａ', 'ｂ', 'ｃ', 'ｄ', 'ｅ', 'ｆ', 'ｇ', 'ｈ', 'ｉ', 'ｊ', 'ｌ', 'ｎ', 'ｏ', 'ｒ', 'ｔ', 'ｘ', 'ｚ'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<10x64 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 72 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "with open('Chinese/stopword.txt', 'rt') as f:\n",
    "    stoplist = f.readlines()\n",
    "    stoplist = [w.replace('\\n', '') for w in stoplist]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(tokenizer=jieba.cut,\n",
    "                             stop_words=stoplist,\n",
    "                             min_df=1)\n",
    "bag_words = count_vect.fit_transform(data['title'])\n",
    "words_names = count_vect.get_feature_names()\n",
    "bag_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意由于Scikit-Learn中一般只支持英文的自动分词（tokenize），为了让他能够处理中文的分词，我们把jieba.cut函数提交给了CountVectorizer，此外还额外提供了停用词列表。min_df选项设置了如果在所有文本中某个词出现的频率下线，如果出现太少则会被忽略，适当提高这个选项可以降低维数。\n",
    "\n",
    "上面的词袋结果是一个sparse matrix，实际上是SciPy中的系数矩阵形式，实际分析时已经可以使用。不过为了查看方便，我们不妨将其转换为Pandas的数据框（稀疏存储）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>19.5</th>\n",
       "      <th>20%</th>\n",
       "      <th>2010</th>\n",
       "      <th>590</th>\n",
       "      <th>6.5781</th>\n",
       "      <th>上海</th>\n",
       "      <th>上调</th>\n",
       "      <th>不探</th>\n",
       "      <th>九条</th>\n",
       "      <th>...</th>\n",
       "      <th>藏药</th>\n",
       "      <th>西藏</th>\n",
       "      <th>试点</th>\n",
       "      <th>调控</th>\n",
       "      <th>销售</th>\n",
       "      <th>降</th>\n",
       "      <th>限购</th>\n",
       "      <th>项目</th>\n",
       "      <th>首日</th>\n",
       "      <th>高</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      19.5  20%  2010  590  6.5781  上海  上调  不探  九条  ...  藏药  西藏  试点  调控  销售  \\\n",
       "0  0     0    0     0    0       0   0   0   0   0  ...   0   0   1   0   0   \n",
       "1  0     0    0     0    0       0   0   0   0   0  ...   0   0   0   0   0   \n",
       "2  1     1    0     0    0       0   0   1   0   0  ...   0   0   0   0   0   \n",
       "3  1     0    1     0    0       0   0   0   0   0  ...   0   0   0   1   0   \n",
       "4  1     0    0     0    0       0   0   0   0   0  ...   1   1   0   0   0   \n",
       "5  1     0    0     0    1       0   0   0   1   0  ...   0   0   0   0   0   \n",
       "6  0     0    0     0    0       0   0   0   0   0  ...   0   0   0   0   0   \n",
       "7  0     0    0     0    0       0   1   0   0   1  ...   0   0   0   0   0   \n",
       "8  0     0    0     0    0       1   0   0   0   0  ...   0   0   0   0   0   \n",
       "9  0     0    0     1    0       0   1   0   0   0  ...   0   0   0   0   1   \n",
       "\n",
       "   降  限购  项目  首日  高  \n",
       "0  0   0   0   0  0  \n",
       "1  0   0   0   0  0  \n",
       "2  0   0   0   0  1  \n",
       "3  0   0   0   0  0  \n",
       "4  0   0   0   0  0  \n",
       "5  0   0   1   0  0  \n",
       "6  0   1   0   1  0  \n",
       "7  0   1   0   0  0  \n",
       "8  0   0   0   0  0  \n",
       "9  1   0   0   0  0  \n",
       "\n",
       "[10 rows x 64 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_words_df = pd.DataFrame.sparse.from_spmatrix(bag_words,\n",
    "                                                 columns=words_names)\n",
    "bag_words_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也许你会觉着不舒服的是，分词和去除停用词等操作应该是第二步清洗数据完成的，现在如果都放到向量化的对象CountVectorizer中来，非常不灵活。\n",
    "\n",
    "比如显然上面的结果中，「13，19.5，20%」等都不是我们想要的，然而用停用词根本不可能将这些数字去除。\n",
    "\n",
    "那么如何将两者（手工清洗+自动计算词袋）结合起来呢？其实很简单，用空格将他们join起来就好了。比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['沪 增值税 扩围 改革 试点',\n",
       " '周小川 外部 施压 影响 人民币 升值 步伐',\n",
       " '准备金率 上调 创新 高',\n",
       " '定基 价格指数 若涨 政府 出手 调控',\n",
       " '政策 倾斜 加大 投入 西藏 做 做 强 藏药 产业',\n",
       " '新疆 地州 探矿权 项目 涉嫌 圈 不探',\n",
       " '北京 楼市 限购 首日 成交 环 比降 成',\n",
       " '上海 房管局 发布 沪 九条 限购 执行 细则',\n",
       " '人民币 升值 容忍度 提高',\n",
       " '上海 商品住宅 销售 降 四成']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "import re\n",
    "\n",
    "with open('Chinese/stopword.txt', 'rt') as f:\n",
    "    stoplist = f.readlines()\n",
    "    stoplist = [w.replace('\\n', '') for w in stoplist]\n",
    "\n",
    "\n",
    "def not_digit(w):\n",
    "    w = w.replace(',', '')\n",
    "    if re.match(r'\\d+', w) != None or re.match(r'\\d%', w) != None or re.match(\n",
    "            r'\\d*\\.\\d+', w) != None:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def tokenize(w):\n",
    "    cut_w = jieba.cut(w)\n",
    "    ## 去除停用词\n",
    "    cut_w = [\n",
    "        w.strip().lower() for w in cut_w\n",
    "        if ((w.strip().lower() not in stoplist) and not_digit(w) and len(w.strip()) > 0)\n",
    "    ]\n",
    "    return cut_w\n",
    "\n",
    "\n",
    "tokenized_data = map(tokenize, data['title'])\n",
    "tokenized_data = [' '.join(t) for t in tokenized_data]\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>上海</th>\n",
       "      <th>上调</th>\n",
       "      <th>不探</th>\n",
       "      <th>九条</th>\n",
       "      <th>产业</th>\n",
       "      <th>人民币</th>\n",
       "      <th>价格指数</th>\n",
       "      <th>倾斜</th>\n",
       "      <th>准备金率</th>\n",
       "      <th>出手</th>\n",
       "      <th>...</th>\n",
       "      <th>细则</th>\n",
       "      <th>若涨</th>\n",
       "      <th>藏药</th>\n",
       "      <th>西藏</th>\n",
       "      <th>试点</th>\n",
       "      <th>调控</th>\n",
       "      <th>销售</th>\n",
       "      <th>限购</th>\n",
       "      <th>项目</th>\n",
       "      <th>首日</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   上海  上调  不探  九条  产业  人民币  价格指数  倾斜  准备金率  出手  ...  细则  若涨  藏药  西藏  试点  调控  \\\n",
       "0   0   0   0   0   0    0     0   0     0   0  ...   0   0   0   0   1   0   \n",
       "1   0   0   0   0   0    1     0   0     0   0  ...   0   0   0   0   0   0   \n",
       "2   0   1   0   0   0    0     0   0     1   0  ...   0   0   0   0   0   0   \n",
       "3   0   0   0   0   0    0     1   0     0   1  ...   0   1   0   0   0   1   \n",
       "4   0   0   0   0   1    0     0   1     0   0  ...   0   0   1   1   0   0   \n",
       "5   0   0   1   0   0    0     0   0     0   0  ...   0   0   0   0   0   0   \n",
       "6   0   0   0   0   0    0     0   0     0   0  ...   0   0   0   0   0   0   \n",
       "7   1   0   0   1   0    0     0   0     0   0  ...   1   0   0   0   0   0   \n",
       "8   0   0   0   0   0    1     0   0     0   0  ...   0   0   0   0   0   0   \n",
       "9   1   0   0   0   0    0     0   0     0   0  ...   0   0   0   0   0   0   \n",
       "\n",
       "   销售  限购  项目  首日  \n",
       "0   0   0   0   0  \n",
       "1   0   0   0   0  \n",
       "2   0   0   0   0  \n",
       "3   0   0   0   0  \n",
       "4   0   0   0   0  \n",
       "5   0   0   1   0  \n",
       "6   0   1   0   1  \n",
       "7   0   1   0   0  \n",
       "8   0   0   0   0  \n",
       "9   1   0   0   0  \n",
       "\n",
       "[10 rows x 50 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "bag_words = count_vect.fit_transform(tokenized_data)\n",
    "words_names = count_vect.get_feature_names()\n",
    "bag_words_df = pd.DataFrame.sparse.from_spmatrix(bag_words,\n",
    "                                                 columns=words_names)\n",
    "bag_words_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，更优雅的方法是直接将手写的tokenize函数调入即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>上海</th>\n",
       "      <th>上调</th>\n",
       "      <th>不探</th>\n",
       "      <th>九条</th>\n",
       "      <th>产业</th>\n",
       "      <th>人民币</th>\n",
       "      <th>价格指数</th>\n",
       "      <th>倾斜</th>\n",
       "      <th>做</th>\n",
       "      <th>准备金率</th>\n",
       "      <th>...</th>\n",
       "      <th>藏药</th>\n",
       "      <th>西藏</th>\n",
       "      <th>试点</th>\n",
       "      <th>调控</th>\n",
       "      <th>销售</th>\n",
       "      <th>降</th>\n",
       "      <th>限购</th>\n",
       "      <th>项目</th>\n",
       "      <th>首日</th>\n",
       "      <th>高</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   上海  上调  不探  九条  产业  人民币  价格指数  倾斜  做  准备金率  ...  藏药  西藏  试点  调控  销售  降  限购  \\\n",
       "0   0   0   0   0   0    0     0   0  0     0  ...   0   0   1   0   0  0   0   \n",
       "1   0   0   0   0   0    1     0   0  0     0  ...   0   0   0   0   0  0   0   \n",
       "2   0   1   0   0   0    0     0   0  0     1  ...   0   0   0   0   0  0   0   \n",
       "3   0   0   0   0   0    0     1   0  0     0  ...   0   0   0   1   0  0   0   \n",
       "4   0   0   0   0   1    0     0   1  2     0  ...   1   1   0   0   0  0   0   \n",
       "5   0   0   1   0   0    0     0   0  0     0  ...   0   0   0   0   0  0   0   \n",
       "6   0   0   0   0   0    0     0   0  0     0  ...   0   0   0   0   0  0   1   \n",
       "7   1   0   0   1   0    0     0   0  0     0  ...   0   0   0   0   0  0   1   \n",
       "8   0   0   0   0   0    1     0   0  0     0  ...   0   0   0   0   0  0   0   \n",
       "9   1   0   0   0   0    0     0   0  0     0  ...   0   0   0   0   1  1   0   \n",
       "\n",
       "   项目  首日  高  \n",
       "0   0   0  0  \n",
       "1   0   0  0  \n",
       "2   0   0  1  \n",
       "3   0   0  0  \n",
       "4   0   0  0  \n",
       "5   1   0  0  \n",
       "6   0   1  0  \n",
       "7   0   0  0  \n",
       "8   0   0  0  \n",
       "9   0   0  0  \n",
       "\n",
       "[10 rows x 58 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(tokenizer=tokenize)\n",
    "bag_words = count_vect.fit_transform(data['title'])\n",
    "words_names = count_vect.get_feature_names()\n",
    "bag_words_df = pd.DataFrame.sparse.from_spmatrix(bag_words,\n",
    "                                                 columns=words_names)\n",
    "bag_words_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上以上的词袋又被称为1元词袋，是N元（N-gram）词袋的一种特例。我们当然可以做成二元词袋："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>上海 商品住宅</th>\n",
       "      <th>上海 房管局</th>\n",
       "      <th>上调 创新</th>\n",
       "      <th>九条 限购</th>\n",
       "      <th>人民币 升值</th>\n",
       "      <th>价格指数 若涨</th>\n",
       "      <th>倾斜 加大</th>\n",
       "      <th>做 做</th>\n",
       "      <th>做 强</th>\n",
       "      <th>准备金率 上调</th>\n",
       "      <th>...</th>\n",
       "      <th>环 比降</th>\n",
       "      <th>若涨 政府</th>\n",
       "      <th>藏药 产业</th>\n",
       "      <th>西藏 做</th>\n",
       "      <th>销售 降</th>\n",
       "      <th>降 四成</th>\n",
       "      <th>限购 执行</th>\n",
       "      <th>限购 首日</th>\n",
       "      <th>项目 涉嫌</th>\n",
       "      <th>首日 成交</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   上海 商品住宅  上海 房管局  上调 创新  九条 限购  人民币 升值  价格指数 若涨  倾斜 加大  做 做  做 强  准备金率 上调  \\\n",
       "0        0       0      0      0       0        0      0    0    0        0   \n",
       "1        0       0      0      0       1        0      0    0    0        0   \n",
       "2        0       0      1      0       0        0      0    0    0        1   \n",
       "3        0       0      0      0       0        1      0    0    0        0   \n",
       "4        0       0      0      0       0        0      1    1    1        0   \n",
       "5        0       0      0      0       0        0      0    0    0        0   \n",
       "6        0       0      0      0       0        0      0    0    0        0   \n",
       "7        0       1      0      1       0        0      0    0    0        0   \n",
       "8        0       0      0      0       1        0      0    0    0        0   \n",
       "9        1       0      0      0       0        0      0    0    0        0   \n",
       "\n",
       "   ...  环 比降  若涨 政府  藏药 产业  西藏 做  销售 降  降 四成  限购 执行  限购 首日  项目 涉嫌  首日 成交  \n",
       "0  ...     0      0      0     0     0     0      0      0      0      0  \n",
       "1  ...     0      0      0     0     0     0      0      0      0      0  \n",
       "2  ...     0      0      0     0     0     0      0      0      0      0  \n",
       "3  ...     0      1      0     0     0     0      0      0      0      0  \n",
       "4  ...     0      0      1     1     0     0      0      0      0      0  \n",
       "5  ...     0      0      0     0     0     0      0      0      1      0  \n",
       "6  ...     1      0      0     0     0     0      0      1      0      1  \n",
       "7  ...     0      0      0     0     0     0      1      0      0      0  \n",
       "8  ...     0      0      0     0     0     0      0      0      0      0  \n",
       "9  ...     0      0      0     0     1     1      0      0      0      0  \n",
       "\n",
       "[10 rows x 53 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(tokenizer=tokenize, ngram_range=(2, 2))\n",
    "bag_words = count_vect.fit_transform(data['title'])\n",
    "words_names = count_vect.get_feature_names()\n",
    "bag_words_df = pd.DataFrame.sparse.from_spmatrix(bag_words,\n",
    "                                                 columns=words_names)\n",
    "bag_words_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中ngram_range选项给出了一个区间，如果是(1,2)那么就是1元、2元词袋同时存在，而(2,2)代表仅使用2元词袋，(1,1)为默认，即只使用一元词袋。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF模型\n",
    "\n",
    "词袋模型简单有效，但是有一个缺点，即只考虑了词的绝对频率，而没有考虑相对频率。比如有的词天然的出现频率更高，那么在每个文档里面，其重要性应该是更低的。\n",
    "\n",
    "而TF-IDF模型就是在词袋的基础上修正这一点。TF-IDF模型是两个度量的乘积：$$TFIDF=TF\\times IDF$$其中TF即词频，而IDF为逆文档频率，IDF的定义为：$$IDF\\left(w\\right)=1+\\ln\\left(\\frac{N}{1+df\\left(w\\right)}\\right)$$其中$N$为文档总数量，而$df\\left(w\\right)$为包含单词$w$的文档个数。\n",
    "\n",
    "可以看到根据上面的定义，一个单词$w$如果出现的文档越多，那么其$IDF\\left(w\\right)$值就越小，或者说权重就越小。\n",
    "\n",
    "最终，我们将上面词袋中每个文档每个词的词频（$TF_i\\left(w\\right),i=1,...,N$）乘以权重就得到了TF-IDF值：$$TFIDF_i\\left(w\\right)=TF_i\\left(w\\right)\\times IDF\\left(w\\right),i=1,...,N$$\n",
    "\n",
    "最后，对每个文档，还需要将以上得到的向量进行标准化，一般使用$L2$范数进行标准化（从而每个向量都在$M$维单位球上，$M$为词的个数）：$$NormalizedTFIDF_i\\left(w\\right)=\\frac{TFIDF_i\\left(w\\right)}{\\sqrt{\\sum_{k=1}^{M}\\left[TFIDF_i\\left(w\\right)\\right]^2}},i=1,...,N$$\n",
    "\n",
    "比如，对于之前计算的词袋："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>上海</th>\n",
       "      <th>上调</th>\n",
       "      <th>不探</th>\n",
       "      <th>九条</th>\n",
       "      <th>产业</th>\n",
       "      <th>人民币</th>\n",
       "      <th>价格指数</th>\n",
       "      <th>倾斜</th>\n",
       "      <th>做</th>\n",
       "      <th>准备金率</th>\n",
       "      <th>...</th>\n",
       "      <th>藏药</th>\n",
       "      <th>西藏</th>\n",
       "      <th>试点</th>\n",
       "      <th>调控</th>\n",
       "      <th>销售</th>\n",
       "      <th>降</th>\n",
       "      <th>限购</th>\n",
       "      <th>项目</th>\n",
       "      <th>首日</th>\n",
       "      <th>高</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   上海  上调  不探  九条  产业  人民币  价格指数  倾斜  做  准备金率  ...  藏药  西藏  试点  调控  销售  降  限购  \\\n",
       "0   0   0   0   0   0    0     0   0  0     0  ...   0   0   1   0   0  0   0   \n",
       "1   0   0   0   0   0    1     0   0  0     0  ...   0   0   0   0   0  0   0   \n",
       "2   0   1   0   0   0    0     0   0  0     1  ...   0   0   0   0   0  0   0   \n",
       "3   0   0   0   0   0    0     1   0  0     0  ...   0   0   0   1   0  0   0   \n",
       "4   0   0   0   0   1    0     0   1  2     0  ...   1   1   0   0   0  0   0   \n",
       "5   0   0   1   0   0    0     0   0  0     0  ...   0   0   0   0   0  0   0   \n",
       "6   0   0   0   0   0    0     0   0  0     0  ...   0   0   0   0   0  0   1   \n",
       "7   1   0   0   1   0    0     0   0  0     0  ...   0   0   0   0   0  0   1   \n",
       "8   0   0   0   0   0    1     0   0  0     0  ...   0   0   0   0   0  0   0   \n",
       "9   1   0   0   0   0    0     0   0  0     0  ...   0   0   0   0   1  1   0   \n",
       "\n",
       "   项目  首日  高  \n",
       "0   0   0  0  \n",
       "1   0   0  0  \n",
       "2   0   0  1  \n",
       "3   0   0  0  \n",
       "4   0   0  0  \n",
       "5   1   0  0  \n",
       "6   0   1  0  \n",
       "7   0   0  0  \n",
       "8   0   0  0  \n",
       "9   0   0  0  \n",
       "\n",
       "[10 rows x 58 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(tokenizer=tokenize)\n",
    "bag_words = count_vect.fit_transform(tokenized_data)\n",
    "words_names = count_vect.get_feature_names()\n",
    "bag_words_df = pd.DataFrame.sparse.from_spmatrix(bag_words,\n",
    "                                                 columns=words_names)\n",
    "bag_words_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以计算「上海」这个词在两个文档中出现，从而其$IDF=1+\\ln\\left(10\\right)-\\ln\\left(1+2\\right)=2.20397$，而「做」这个词只有一个文档出现，从而其$IDF=1+\\ln\\left(10\\right)-\\ln\\left(1+1\\right)=2.60944$。\n",
    "\n",
    "以上计算略显复杂，不过Scikit-Learn中也给出了方便的计算函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>上海</th>\n",
       "      <th>上调</th>\n",
       "      <th>不探</th>\n",
       "      <th>九条</th>\n",
       "      <th>产业</th>\n",
       "      <th>人民币</th>\n",
       "      <th>价格指数</th>\n",
       "      <th>倾斜</th>\n",
       "      <th>做</th>\n",
       "      <th>准备金率</th>\n",
       "      <th>...</th>\n",
       "      <th>藏药</th>\n",
       "      <th>西藏</th>\n",
       "      <th>试点</th>\n",
       "      <th>调控</th>\n",
       "      <th>销售</th>\n",
       "      <th>降</th>\n",
       "      <th>限购</th>\n",
       "      <th>项目</th>\n",
       "      <th>首日</th>\n",
       "      <th>高</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359846</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.317517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.37351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.457985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.391176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460158</td>\n",
       "      <td>0.460158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         上海   上调        不探       九条        产业       人民币      价格指数        倾斜  \\\n",
       "0  0.000000  0.0  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.0  0.000000  0.00000  0.000000  0.334845  0.000000  0.000000   \n",
       "2  0.000000  0.5  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.0  0.000000  0.00000  0.000000  0.000000  0.408248  0.000000   \n",
       "4  0.000000  0.0  0.000000  0.00000  0.288675  0.000000  0.000000  0.288675   \n",
       "5  0.000000  0.0  0.377964  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.0  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.317517  0.0  0.000000  0.37351  0.000000  0.000000  0.000000  0.000000   \n",
       "8  0.000000  0.0  0.000000  0.00000  0.000000  0.457985  0.000000  0.000000   \n",
       "9  0.391176  0.0  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         做  准备金率  ...        藏药        西藏        试点        调控        销售  \\\n",
       "0  0.00000   0.0  ...  0.000000  0.000000  0.460158  0.000000  0.000000   \n",
       "1  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.00000   0.5  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.408248  0.000000   \n",
       "4  0.57735   0.0  ...  0.288675  0.288675  0.000000  0.000000  0.000000   \n",
       "5  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9  0.00000   0.0  ...  0.000000  0.000000  0.000000  0.000000  0.460158   \n",
       "\n",
       "          降        限购        项目        首日    高  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.0  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.0  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.5  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.0  \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.0  \n",
       "5  0.000000  0.000000  0.377964  0.000000  0.0  \n",
       "6  0.000000  0.305902  0.000000  0.359846  0.0  \n",
       "7  0.000000  0.317517  0.000000  0.000000  0.0  \n",
       "8  0.000000  0.000000  0.000000  0.000000  0.0  \n",
       "9  0.460158  0.000000  0.000000  0.000000  0.0  \n",
       "\n",
       "[10 rows x 58 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(norm='l2').fit(\n",
    "    bag_words)  ##使用L2范数，并fit模型（如计算IDF等）\n",
    "tfidf_words = tfidf_transformer.transform(bag_words)  ##变换数据\n",
    "tfidf_words_df = pd.DataFrame.sparse.from_spmatrix(tfidf_words,\n",
    "                                                   columns=words_names)\n",
    "tfidf_words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于Scikit-Learn中词袋、TF-IDF计算的具体文档，可以查看：https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "除了将句子转换为向量，TF-IDF还有很多其他用法，比如：\n",
    "\n",
    "* 使用TF-IDF降维：删掉TF-IDF比较小的词：林建浩等（2021）\n",
    "* 直接使用TF-IDF作为一个指标：Piotroski, Wong和Zhang（2017）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词嵌入\n",
    "\n",
    "上面的词袋模型和TF-IDF模型都非常直观，但是有一个缺点，就是维数非常高。我们仅仅使用了10条新闻标题，就得到了59列特征，虽然在存储和计算上我们可以使用稀疏矩阵，但是在分析中，维数太高的模型分析起来总是比较困难的。而**词嵌入**（**word embedding**）的出现很大程度上缓解了这个问题。\n",
    "\n",
    "所谓词嵌入，实际上是把高维空间的向量向低维空间映射的过程。一个经典的算法是谷歌提出的word2vec算法，关于该算法的原理我们再次不再赘述，我们这里主要通过例子来展示如何使用该方法。\n",
    "\n",
    "Python中可以使用Gensim包实现word2vec算法，在使用之前需要先安装：\n",
    "\n",
    "```shell\n",
    "sudo pip3 install gensim\n",
    "```\n",
    "\n",
    "该算法需要提供如下几个信息：\n",
    "\n",
    "* 语料库，对于中文可以提交已经分好词的语料库\n",
    "* window，窗宽，上下文可以联系起来的单词个数\n",
    "* size，输出的词向量的维度，几十到几千都可以\n",
    "* min_count，只有当某个词出现次数大于该数值时才会被加入到模型中。\n",
    "\n",
    "比如，使用以上新闻标题数据，可以训练如下模型：\n",
    "\n",
    "```python\n",
    "import gensim\n",
    "\n",
    "CORPUS=map(tokenize,RAW1['title'])\n",
    "w2v_model=gensim.models.Word2Vec(CORPUS, window=5, size=10, min_count=5)\n",
    "w2v_model.save('word2vec')##后面节省时间，先把模型保存下来\n",
    "```\n",
    "\n",
    "当然在现实中，有的时候我们也会使用其他人已经预训练的模型。其实在本例中，只使用标题信息训练出的模型精度并不好（比如标题中使用了沪就不会使用上海，因而很难侦测到这种相关性）。\n",
    "\n",
    "我们使用1992年到2016年的CSMAR上式公司新闻全文数据训练了不同维度的词向量，保存在“Chinese/word2vec/”文件夹中（由于文件很大，我们在这个文件夹汇总值保留了30维模型的训练结果，更多维数的结果请从百度网盘：https://pan.baidu.com/s/1GkxOSFXlzpMzJ6StYN23Sg \n",
    "提取码：ly9c 下载），其中\"word2vec.py\"为训练代码，由于数据量太大（共有74885004个词汇参与训练），我们不提供该原始数据，如有需要可以从CSMAR下载。\"word2vec**\"位已经训练好的词向量模型，其中\\*\\*为词向量的维度，我们训练了30、100、300、500等不同维度。如果需要导入预训练的模型，可以直接使用load函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec.load(\n",
    "    './Chinese/word2vec/word2vec30')  ##节省时间，直接导入预训练的模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型的具体语法和解释可以查看：https://radimrehurek.com/gensim/models/word2vec.html#module-gensim.models.word2vec\n",
    "\n",
    "有了模型后，我们可以查看每个词对应的向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "贵州茅台：\n",
      " [-2.6204739  -0.23999904 -0.4501495  -4.7471104  -2.6559854   6.1215234\n",
      " -1.19105     0.08704755  4.4575567  -5.9783154   1.6476333  -2.9682255\n",
      "  0.05026169 -2.8163173   3.1344664  -2.513116   -2.3200655  -6.645488\n",
      "  1.7873464  -0.28822428 -0.41796866  5.3326087  -1.7870761   0.31191105\n",
      " -2.4249952  -8.8343725   0.67447054  1.6748495  -2.1062326  -5.243416  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"贵州茅台：\\n\", w2v_model.wv['贵州茅台'])\n",
    "w2v_model.wv['贵州茅台'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以查看跟某些词最相关的词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上海： [('广州', 0.8821085691452026), ('北京', 0.8780176639556885), ('深圳', 0.8698582053184509), ('天津', 0.8337385654449463), ('上海浦东', 0.8273128271102905), ('浦东', 0.7818917036056519), ('成都', 0.7796352505683899), ('虹口', 0.7682735919952393), ('重庆', 0.763504147529602), ('穗三大', 0.7591032981872559)]\n",
      "贵州茅台： [('洋河股份', 0.9588034749031067), ('山西汾酒', 0.9121653437614441), ('张裕a', 0.9066104292869568), ('g茅台', 0.9013229012489319), ('古井贡酒', 0.8815901279449463), ('伊利股份', 0.879795253276825), ('金种子酒', 0.8770319819450378), ('老白干酒', 0.8755484819412231), ('泸州老窖', 0.8753378987312317), ('g五粮液', 0.8694737553596497)]\n"
     ]
    }
   ],
   "source": [
    "print(\"上海：\", w2v_model.wv.most_similar(positive=['上海']))\n",
    "print(\"贵州茅台：\", w2v_model.wv.most_similar(positive=['贵州茅台']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者跟某个词的向量的相反数最相关的（与$-1\\times$上海 最相关的）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上海： [('仅当', 0.641044020652771), ('恕', 0.62154620885849), ('杜晓锋', 0.6094518303871155), ('幸敬华', 0.6002946496009827), ('毛惟德', 0.5936871767044067), ('蓝保湾', 0.5931684970855713), ('五年制', 0.592823326587677), ('资本保全', 0.585964560508728), ('杜干', 0.5858868956565857), ('质量规划', 0.5837689638137817)]\n",
      "贵州茅台： [('先筑底', 0.7058587074279785), ('法定清算', 0.6609488129615784), ('经援', 0.6546810269355774), ('下除', 0.652676522731781), ('动产担保', 0.6491249203681946), ('外部边界', 0.6484469175338745), ('混合贷款', 0.6440901160240173), ('国际金融组织贷款', 0.6322883367538452), ('所在国', 0.6299104690551758), ('区域信用', 0.6297280788421631)]\n"
     ]
    }
   ],
   "source": [
    "print(\"上海：\", w2v_model.wv.most_similar(negative=['上海']))\n",
    "print(\"贵州茅台：\", w2v_model.wv.most_similar(negative=['贵州茅台']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者查看：上海+杭州-北京=？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上海+杭州-北京=\n",
      "[('宁波', 0.8586425185203552), ('广州', 0.8504918217658997), ('东莞', 0.822361409664154), ('南京', 0.8192955255508423), ('厦门', 0.8146064281463623), ('佛山', 0.8060693144798279), ('南一', 0.7999212145805359), ('溧阳', 0.79963219165802), ('成都', 0.7981355786323547), ('浙江', 0.7962439060211182)]\n",
      "上海+北京-杭州=\n",
      "[('上海浦东', 0.7623591423034668), ('上海综合保税区', 0.7335735559463501), ('深圳', 0.7314643859863281), ('e-cbd', 0.7249361276626587), ('京', 0.7226858139038086), ('辐射式', 0.7162173986434937), ('浦东', 0.7120426893234253), ('三地', 0.7035839557647705), ('徐雯', 0.7003939747810364), ('盛汇', 0.6990733742713928)]\n"
     ]
    }
   ],
   "source": [
    "print(\"上海+杭州-北京=\\n%s\" %\n",
    "      w2v_model.wv.most_similar(positive=['上海', '杭州'], negative=['北京']))\n",
    "print(\"上海+北京-杭州=\\n%s\" %\n",
    "      w2v_model.wv.most_similar(positive=['上海', '北京'], negative=['杭州']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，直接看两个词的相似度（词向量的相关系数）也是可以的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上海,北京的相似度= 0.8780\n",
      "上海,日本的相似度= -0.0521\n",
      "北京,日本的相似度= 0.5247\n",
      "上海,人民币的相似度= 0.1460\n",
      "人民币,北京的相似度= 0.0441\n",
      "贵州茅台,比亚迪的相似度= 0.3611\n",
      "吉利汽车,沃尔沃的相似度= 0.8491\n",
      "华为,中兴的相似度= 0.8539\n"
     ]
    }
   ],
   "source": [
    "print(\"%s,%s的相似度= %.4f\" % (\"上海\", \"北京\", w2v_model.wv.similarity(\"上海\", \"北京\")))\n",
    "print(\"%s,%s的相似度= %.4f\" % (\"上海\", \"日本\", w2v_model.wv.similarity(\"上海\", \"日本\")))\n",
    "print(\"%s,%s的相似度= %.4f\" % (\"北京\", \"日本\", w2v_model.wv.similarity(\"医药\", \"医疗\")))\n",
    "print(\"%s,%s的相似度= %.4f\" % (\"上海\", \"人民币\", w2v_model.wv.similarity(\"上海\", \"人民币\")))\n",
    "print(\"%s,%s的相似度= %.4f\" % (\"人民币\", \"北京\", w2v_model.wv.similarity(\"人民币\", \"北京\")))\n",
    "print(\"%s,%s的相似度= %.4f\" %\n",
    "      (\"贵州茅台\", \"比亚迪\", w2v_model.wv.similarity(\"贵州茅台\", \"比亚迪\")))\n",
    "print(\"%s,%s的相似度= %.4f\" %\n",
    "      (\"吉利汽车\", \"沃尔沃\", w2v_model.wv.similarity(\"吉利汽车\", \"沃尔沃\")))\n",
    "print(\"%s,%s的相似度= %.4f\" % (\"华为\", \"中兴\", w2v_model.wv.similarity(\"华为\", \"中兴\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而，以上算法仅仅计算了每个词的向量，我们数据中的却是文档，因而需要将词向量加总成文档向量。为此，我们可以通过平均的方式求文档的向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['沪', '增值税', '扩围', '改革', '试点'],\n",
       " ['周小川', '外部', '施压', '影响', '人民币', '升值', '步伐'],\n",
       " ['准备金率', '上调', '创新', '高'],\n",
       " ['定基', '价格指数', '若涨', '政府', '出手', '调控'],\n",
       " ['政策', '倾斜', '加大', '投入', '西藏', '做', '做', '强', '藏药', '产业'],\n",
       " ['新疆', '地州', '探矿权', '项目', '涉嫌', '圈', '不探'],\n",
       " ['北京', '楼市', '限购', '首日', '成交', '环', '比降', '成'],\n",
       " ['上海', '房管局', '发布', '沪', '九条', '限购', '执行', '细则'],\n",
       " ['人民币', '升值', '容忍度', '提高'],\n",
       " ['上海', '商品住宅', '销售', '降', '四成']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = map(tokenize, data['title'])\n",
    "tokenized_data = list(tokenized_data)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.297971</td>\n",
       "      <td>-3.161334</td>\n",
       "      <td>2.866893</td>\n",
       "      <td>-0.996126</td>\n",
       "      <td>0.284920</td>\n",
       "      <td>1.531972</td>\n",
       "      <td>0.906654</td>\n",
       "      <td>-4.272559</td>\n",
       "      <td>1.360739</td>\n",
       "      <td>-7.145058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618608</td>\n",
       "      <td>-2.027417</td>\n",
       "      <td>0.354830</td>\n",
       "      <td>-5.920053</td>\n",
       "      <td>2.606006</td>\n",
       "      <td>0.943722</td>\n",
       "      <td>0.710307</td>\n",
       "      <td>3.250579</td>\n",
       "      <td>4.710419</td>\n",
       "      <td>-0.751039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.090696</td>\n",
       "      <td>-2.051241</td>\n",
       "      <td>-0.290815</td>\n",
       "      <td>-1.770225</td>\n",
       "      <td>-0.407113</td>\n",
       "      <td>-4.449056</td>\n",
       "      <td>0.354794</td>\n",
       "      <td>-1.708066</td>\n",
       "      <td>-2.584469</td>\n",
       "      <td>-3.557123</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.584700</td>\n",
       "      <td>-3.989273</td>\n",
       "      <td>3.316375</td>\n",
       "      <td>0.735364</td>\n",
       "      <td>4.073279</td>\n",
       "      <td>1.923687</td>\n",
       "      <td>0.489768</td>\n",
       "      <td>2.243266</td>\n",
       "      <td>-2.430693</td>\n",
       "      <td>-3.075062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.476211</td>\n",
       "      <td>0.077883</td>\n",
       "      <td>2.481291</td>\n",
       "      <td>-3.441597</td>\n",
       "      <td>0.833566</td>\n",
       "      <td>-4.324602</td>\n",
       "      <td>-1.038114</td>\n",
       "      <td>1.693192</td>\n",
       "      <td>1.889202</td>\n",
       "      <td>-8.951756</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.262578</td>\n",
       "      <td>-2.274254</td>\n",
       "      <td>2.063563</td>\n",
       "      <td>1.068579</td>\n",
       "      <td>3.879284</td>\n",
       "      <td>0.288383</td>\n",
       "      <td>2.252155</td>\n",
       "      <td>2.311087</td>\n",
       "      <td>-0.089803</td>\n",
       "      <td>-3.682970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.604671</td>\n",
       "      <td>0.437567</td>\n",
       "      <td>0.390226</td>\n",
       "      <td>-1.791509</td>\n",
       "      <td>-0.129578</td>\n",
       "      <td>0.469662</td>\n",
       "      <td>-1.043747</td>\n",
       "      <td>-0.333066</td>\n",
       "      <td>0.313937</td>\n",
       "      <td>-4.538773</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.362519</td>\n",
       "      <td>-0.549756</td>\n",
       "      <td>-1.546997</td>\n",
       "      <td>0.896517</td>\n",
       "      <td>5.921604</td>\n",
       "      <td>0.422985</td>\n",
       "      <td>0.351355</td>\n",
       "      <td>-0.108700</td>\n",
       "      <td>-1.117601</td>\n",
       "      <td>0.397091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.415386</td>\n",
       "      <td>-1.761841</td>\n",
       "      <td>-0.055961</td>\n",
       "      <td>-1.782646</td>\n",
       "      <td>-2.699580</td>\n",
       "      <td>-1.282648</td>\n",
       "      <td>-0.357748</td>\n",
       "      <td>0.125751</td>\n",
       "      <td>5.077182</td>\n",
       "      <td>-2.138223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570974</td>\n",
       "      <td>1.046115</td>\n",
       "      <td>-0.687799</td>\n",
       "      <td>1.032049</td>\n",
       "      <td>4.162136</td>\n",
       "      <td>3.645957</td>\n",
       "      <td>2.473218</td>\n",
       "      <td>-0.688791</td>\n",
       "      <td>-0.590855</td>\n",
       "      <td>-1.642059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-2.042580</td>\n",
       "      <td>-0.545288</td>\n",
       "      <td>-2.933997</td>\n",
       "      <td>0.435380</td>\n",
       "      <td>-1.932752</td>\n",
       "      <td>2.237168</td>\n",
       "      <td>-0.882077</td>\n",
       "      <td>-1.852991</td>\n",
       "      <td>0.445002</td>\n",
       "      <td>1.514655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583058</td>\n",
       "      <td>0.847895</td>\n",
       "      <td>-0.823864</td>\n",
       "      <td>0.188446</td>\n",
       "      <td>0.955781</td>\n",
       "      <td>-0.026733</td>\n",
       "      <td>-2.018269</td>\n",
       "      <td>-0.606821</td>\n",
       "      <td>2.170595</td>\n",
       "      <td>2.485991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.136633</td>\n",
       "      <td>-1.237760</td>\n",
       "      <td>-1.183110</td>\n",
       "      <td>-0.815889</td>\n",
       "      <td>1.367206</td>\n",
       "      <td>1.674132</td>\n",
       "      <td>-6.512691</td>\n",
       "      <td>-1.017015</td>\n",
       "      <td>0.348660</td>\n",
       "      <td>-4.619732</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.668415</td>\n",
       "      <td>1.663260</td>\n",
       "      <td>-3.336064</td>\n",
       "      <td>-0.116358</td>\n",
       "      <td>3.205699</td>\n",
       "      <td>-4.671846</td>\n",
       "      <td>2.095880</td>\n",
       "      <td>3.111485</td>\n",
       "      <td>-1.464048</td>\n",
       "      <td>0.708383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.767114</td>\n",
       "      <td>2.096215</td>\n",
       "      <td>0.746133</td>\n",
       "      <td>0.287825</td>\n",
       "      <td>2.045303</td>\n",
       "      <td>2.857263</td>\n",
       "      <td>-2.008423</td>\n",
       "      <td>-2.189678</td>\n",
       "      <td>0.697154</td>\n",
       "      <td>-3.994444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.495719</td>\n",
       "      <td>-0.121336</td>\n",
       "      <td>-2.238388</td>\n",
       "      <td>-3.878916</td>\n",
       "      <td>4.265755</td>\n",
       "      <td>-1.114963</td>\n",
       "      <td>2.158192</td>\n",
       "      <td>2.761949</td>\n",
       "      <td>1.457491</td>\n",
       "      <td>2.958742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-3.401513</td>\n",
       "      <td>-0.506430</td>\n",
       "      <td>0.380262</td>\n",
       "      <td>-2.048207</td>\n",
       "      <td>1.514004</td>\n",
       "      <td>-5.812137</td>\n",
       "      <td>-1.194896</td>\n",
       "      <td>-0.759125</td>\n",
       "      <td>-1.582029</td>\n",
       "      <td>-5.668441</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.411451</td>\n",
       "      <td>-7.510658</td>\n",
       "      <td>2.072422</td>\n",
       "      <td>2.958090</td>\n",
       "      <td>2.926072</td>\n",
       "      <td>1.766376</td>\n",
       "      <td>0.580327</td>\n",
       "      <td>0.644158</td>\n",
       "      <td>-1.897927</td>\n",
       "      <td>-4.794060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.284938</td>\n",
       "      <td>0.939088</td>\n",
       "      <td>-3.202075</td>\n",
       "      <td>0.634389</td>\n",
       "      <td>2.169413</td>\n",
       "      <td>3.406785</td>\n",
       "      <td>-5.344977</td>\n",
       "      <td>-0.291755</td>\n",
       "      <td>0.007644</td>\n",
       "      <td>-3.117552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.304967</td>\n",
       "      <td>-0.849428</td>\n",
       "      <td>-5.568500</td>\n",
       "      <td>2.012764</td>\n",
       "      <td>2.936881</td>\n",
       "      <td>-1.867398</td>\n",
       "      <td>2.681815</td>\n",
       "      <td>1.888706</td>\n",
       "      <td>-1.198099</td>\n",
       "      <td>-2.365074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -2.297971 -3.161334  2.866893 -0.996126  0.284920  1.531972  0.906654   \n",
       "1  0.090696 -2.051241 -0.290815 -1.770225 -0.407113 -4.449056  0.354794   \n",
       "2 -2.476211  0.077883  2.481291 -3.441597  0.833566 -4.324602 -1.038114   \n",
       "3 -1.604671  0.437567  0.390226 -1.791509 -0.129578  0.469662 -1.043747   \n",
       "4 -2.415386 -1.761841 -0.055961 -1.782646 -2.699580 -1.282648 -0.357748   \n",
       "5 -2.042580 -0.545288 -2.933997  0.435380 -1.932752  2.237168 -0.882077   \n",
       "6 -0.136633 -1.237760 -1.183110 -0.815889  1.367206  1.674132 -6.512691   \n",
       "7  0.767114  2.096215  0.746133  0.287825  2.045303  2.857263 -2.008423   \n",
       "8 -3.401513 -0.506430  0.380262 -2.048207  1.514004 -5.812137 -1.194896   \n",
       "9 -1.284938  0.939088 -3.202075  0.634389  2.169413  3.406785 -5.344977   \n",
       "\n",
       "         7         8         9   ...        20        21        22        23  \\\n",
       "0 -4.272559  1.360739 -7.145058  ...  0.618608 -2.027417  0.354830 -5.920053   \n",
       "1 -1.708066 -2.584469 -3.557123  ... -2.584700 -3.989273  3.316375  0.735364   \n",
       "2  1.693192  1.889202 -8.951756  ... -1.262578 -2.274254  2.063563  1.068579   \n",
       "3 -0.333066  0.313937 -4.538773  ... -1.362519 -0.549756 -1.546997  0.896517   \n",
       "4  0.125751  5.077182 -2.138223  ...  0.570974  1.046115 -0.687799  1.032049   \n",
       "5 -1.852991  0.445002  1.514655  ...  0.583058  0.847895 -0.823864  0.188446   \n",
       "6 -1.017015  0.348660 -4.619732  ... -2.668415  1.663260 -3.336064 -0.116358   \n",
       "7 -2.189678  0.697154 -3.994444  ...  0.495719 -0.121336 -2.238388 -3.878916   \n",
       "8 -0.759125 -1.582029 -5.668441  ... -2.411451 -7.510658  2.072422  2.958090   \n",
       "9 -0.291755  0.007644 -3.117552  ... -0.304967 -0.849428 -5.568500  2.012764   \n",
       "\n",
       "         24        25        26        27        28        29  \n",
       "0  2.606006  0.943722  0.710307  3.250579  4.710419 -0.751039  \n",
       "1  4.073279  1.923687  0.489768  2.243266 -2.430693 -3.075062  \n",
       "2  3.879284  0.288383  2.252155  2.311087 -0.089803 -3.682970  \n",
       "3  5.921604  0.422985  0.351355 -0.108700 -1.117601  0.397091  \n",
       "4  4.162136  3.645957  2.473218 -0.688791 -0.590855 -1.642059  \n",
       "5  0.955781 -0.026733 -2.018269 -0.606821  2.170595  2.485991  \n",
       "6  3.205699 -4.671846  2.095880  3.111485 -1.464048  0.708383  \n",
       "7  4.265755 -1.114963  2.158192  2.761949  1.457491  2.958742  \n",
       "8  2.926072  1.766376  0.580327  0.644158 -1.897927 -4.794060  \n",
       "9  2.936881 -1.867398  2.681815  1.888706 -1.198099 -2.365074  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 知识提要：闭包\n",
    "def mean_vector(model):\n",
    "\n",
    "    def mean_vector_compute(sentence):\n",
    "        n_w = 0\n",
    "        for w in sentence:\n",
    "            if w in model.wv:\n",
    "                try:\n",
    "                    mv += model.wv[w]\n",
    "                except:\n",
    "                    mv = model.wv[w].copy()\n",
    "                n_w += 1\n",
    "        mv /= n_w\n",
    "        return mv\n",
    "\n",
    "    return mean_vector_compute\n",
    "\n",
    "\n",
    "mv_compute = mean_vector(w2v_model)\n",
    "mean_vecs = map(mv_compute, tokenized_data)\n",
    "mean_vecs = pd.DataFrame(mean_vecs)\n",
    "mean_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者我们可以使用TF-IDF进行加权："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.148581</td>\n",
       "      <td>-0.036319</td>\n",
       "      <td>-0.018068</td>\n",
       "      <td>0.009148</td>\n",
       "      <td>0.061901</td>\n",
       "      <td>0.120478</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>-0.216511</td>\n",
       "      <td>0.146551</td>\n",
       "      <td>-0.333161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046125</td>\n",
       "      <td>-0.019234</td>\n",
       "      <td>0.038737</td>\n",
       "      <td>-0.305739</td>\n",
       "      <td>0.201816</td>\n",
       "      <td>0.137147</td>\n",
       "      <td>0.045861</td>\n",
       "      <td>0.066238</td>\n",
       "      <td>0.124265</td>\n",
       "      <td>0.169765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.095334</td>\n",
       "      <td>-0.339919</td>\n",
       "      <td>-0.069402</td>\n",
       "      <td>0.052371</td>\n",
       "      <td>-0.282315</td>\n",
       "      <td>0.030940</td>\n",
       "      <td>-0.090470</td>\n",
       "      <td>-0.162104</td>\n",
       "      <td>0.081100</td>\n",
       "      <td>-0.100262</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036592</td>\n",
       "      <td>-0.020217</td>\n",
       "      <td>0.274612</td>\n",
       "      <td>0.158418</td>\n",
       "      <td>0.227180</td>\n",
       "      <td>0.140554</td>\n",
       "      <td>-0.001166</td>\n",
       "      <td>0.191003</td>\n",
       "      <td>-0.091173</td>\n",
       "      <td>-0.195360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.101324</td>\n",
       "      <td>-0.059106</td>\n",
       "      <td>0.015652</td>\n",
       "      <td>0.015128</td>\n",
       "      <td>0.030852</td>\n",
       "      <td>-0.377617</td>\n",
       "      <td>-0.230746</td>\n",
       "      <td>0.177777</td>\n",
       "      <td>0.244334</td>\n",
       "      <td>-0.295227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030645</td>\n",
       "      <td>-0.046572</td>\n",
       "      <td>-0.194047</td>\n",
       "      <td>0.151977</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.041661</td>\n",
       "      <td>0.033858</td>\n",
       "      <td>-0.158417</td>\n",
       "      <td>-0.074691</td>\n",
       "      <td>-0.223420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.034708</td>\n",
       "      <td>0.109602</td>\n",
       "      <td>0.150284</td>\n",
       "      <td>-0.061393</td>\n",
       "      <td>-0.019534</td>\n",
       "      <td>-0.011336</td>\n",
       "      <td>-0.143899</td>\n",
       "      <td>-0.011558</td>\n",
       "      <td>0.081243</td>\n",
       "      <td>-0.252050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078418</td>\n",
       "      <td>0.018511</td>\n",
       "      <td>-0.043188</td>\n",
       "      <td>0.082479</td>\n",
       "      <td>0.605613</td>\n",
       "      <td>0.021741</td>\n",
       "      <td>0.265413</td>\n",
       "      <td>0.100253</td>\n",
       "      <td>0.098343</td>\n",
       "      <td>0.056790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004909</td>\n",
       "      <td>-0.309575</td>\n",
       "      <td>0.045406</td>\n",
       "      <td>0.192595</td>\n",
       "      <td>-0.310284</td>\n",
       "      <td>0.184387</td>\n",
       "      <td>-0.032352</td>\n",
       "      <td>-0.138226</td>\n",
       "      <td>0.227460</td>\n",
       "      <td>-0.116081</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123986</td>\n",
       "      <td>0.069369</td>\n",
       "      <td>0.011410</td>\n",
       "      <td>0.055064</td>\n",
       "      <td>0.134197</td>\n",
       "      <td>0.362929</td>\n",
       "      <td>0.153390</td>\n",
       "      <td>-0.150784</td>\n",
       "      <td>0.022146</td>\n",
       "      <td>-0.203349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.217749</td>\n",
       "      <td>-0.081696</td>\n",
       "      <td>-0.159856</td>\n",
       "      <td>0.023670</td>\n",
       "      <td>-0.139118</td>\n",
       "      <td>0.087813</td>\n",
       "      <td>0.286302</td>\n",
       "      <td>-0.192930</td>\n",
       "      <td>0.070609</td>\n",
       "      <td>0.171712</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.153864</td>\n",
       "      <td>0.171508</td>\n",
       "      <td>-0.301370</td>\n",
       "      <td>-0.072271</td>\n",
       "      <td>0.378945</td>\n",
       "      <td>0.070726</td>\n",
       "      <td>-0.184511</td>\n",
       "      <td>-0.074120</td>\n",
       "      <td>0.327844</td>\n",
       "      <td>0.325883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.188053</td>\n",
       "      <td>-0.365733</td>\n",
       "      <td>0.128107</td>\n",
       "      <td>-0.058815</td>\n",
       "      <td>0.112551</td>\n",
       "      <td>0.065304</td>\n",
       "      <td>-0.214734</td>\n",
       "      <td>0.087087</td>\n",
       "      <td>0.064707</td>\n",
       "      <td>-0.258617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026033</td>\n",
       "      <td>0.340718</td>\n",
       "      <td>-0.110482</td>\n",
       "      <td>0.069005</td>\n",
       "      <td>0.029456</td>\n",
       "      <td>-0.129096</td>\n",
       "      <td>-0.312034</td>\n",
       "      <td>-0.101793</td>\n",
       "      <td>-0.217690</td>\n",
       "      <td>-0.227760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.113986</td>\n",
       "      <td>0.248894</td>\n",
       "      <td>0.027087</td>\n",
       "      <td>0.231291</td>\n",
       "      <td>-0.032114</td>\n",
       "      <td>0.048062</td>\n",
       "      <td>-0.143418</td>\n",
       "      <td>0.060145</td>\n",
       "      <td>0.016227</td>\n",
       "      <td>-0.305819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223890</td>\n",
       "      <td>0.143665</td>\n",
       "      <td>0.066184</td>\n",
       "      <td>-0.426862</td>\n",
       "      <td>0.333553</td>\n",
       "      <td>-0.009735</td>\n",
       "      <td>-0.001862</td>\n",
       "      <td>0.181096</td>\n",
       "      <td>0.035173</td>\n",
       "      <td>0.060070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.101069</td>\n",
       "      <td>0.166036</td>\n",
       "      <td>0.097794</td>\n",
       "      <td>-0.053011</td>\n",
       "      <td>0.014484</td>\n",
       "      <td>-0.277415</td>\n",
       "      <td>-0.094133</td>\n",
       "      <td>0.043053</td>\n",
       "      <td>0.205265</td>\n",
       "      <td>-0.320228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092974</td>\n",
       "      <td>-0.387319</td>\n",
       "      <td>-0.145867</td>\n",
       "      <td>0.229892</td>\n",
       "      <td>0.168001</td>\n",
       "      <td>0.228359</td>\n",
       "      <td>0.145979</td>\n",
       "      <td>-0.175168</td>\n",
       "      <td>-0.121800</td>\n",
       "      <td>-0.119020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.175843</td>\n",
       "      <td>-0.196618</td>\n",
       "      <td>-0.361817</td>\n",
       "      <td>0.185863</td>\n",
       "      <td>0.014346</td>\n",
       "      <td>0.061425</td>\n",
       "      <td>-0.123399</td>\n",
       "      <td>0.137843</td>\n",
       "      <td>-0.129746</td>\n",
       "      <td>-0.283890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174996</td>\n",
       "      <td>0.088702</td>\n",
       "      <td>-0.223863</td>\n",
       "      <td>0.004097</td>\n",
       "      <td>-0.065761</td>\n",
       "      <td>-0.478842</td>\n",
       "      <td>0.188946</td>\n",
       "      <td>0.146038</td>\n",
       "      <td>-0.237623</td>\n",
       "      <td>-0.299823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.148581 -0.036319 -0.018068  0.009148  0.061901  0.120478  0.000085   \n",
       "1  0.095334 -0.339919 -0.069402  0.052371 -0.282315  0.030940 -0.090470   \n",
       "2 -0.101324 -0.059106  0.015652  0.015128  0.030852 -0.377617 -0.230746   \n",
       "3 -0.034708  0.109602  0.150284 -0.061393 -0.019534 -0.011336 -0.143899   \n",
       "4  0.004909 -0.309575  0.045406  0.192595 -0.310284  0.184387 -0.032352   \n",
       "5 -0.217749 -0.081696 -0.159856  0.023670 -0.139118  0.087813  0.286302   \n",
       "6 -0.188053 -0.365733  0.128107 -0.058815  0.112551  0.065304 -0.214734   \n",
       "7  0.113986  0.248894  0.027087  0.231291 -0.032114  0.048062 -0.143418   \n",
       "8 -0.101069  0.166036  0.097794 -0.053011  0.014484 -0.277415 -0.094133   \n",
       "9 -0.175843 -0.196618 -0.361817  0.185863  0.014346  0.061425 -0.123399   \n",
       "\n",
       "         7         8         9   ...        20        21        22        23  \\\n",
       "0 -0.216511  0.146551 -0.333161  ...  0.046125 -0.019234  0.038737 -0.305739   \n",
       "1 -0.162104  0.081100 -0.100262  ... -0.036592 -0.020217  0.274612  0.158418   \n",
       "2  0.177777  0.244334 -0.295227  ... -0.030645 -0.046572 -0.194047  0.151977   \n",
       "3 -0.011558  0.081243 -0.252050  ... -0.078418  0.018511 -0.043188  0.082479   \n",
       "4 -0.138226  0.227460 -0.116081  ... -0.123986  0.069369  0.011410  0.055064   \n",
       "5 -0.192930  0.070609  0.171712  ... -0.153864  0.171508 -0.301370 -0.072271   \n",
       "6  0.087087  0.064707 -0.258617  ... -0.026033  0.340718 -0.110482  0.069005   \n",
       "7  0.060145  0.016227 -0.305819  ...  0.223890  0.143665  0.066184 -0.426862   \n",
       "8  0.043053  0.205265 -0.320228  ...  0.092974 -0.387319 -0.145867  0.229892   \n",
       "9  0.137843 -0.129746 -0.283890  ...  0.174996  0.088702 -0.223863  0.004097   \n",
       "\n",
       "         24        25        26        27        28        29  \n",
       "0  0.201816  0.137147  0.045861  0.066238  0.124265  0.169765  \n",
       "1  0.227180  0.140554 -0.001166  0.191003 -0.091173 -0.195360  \n",
       "2  0.000971  0.041661  0.033858 -0.158417 -0.074691 -0.223420  \n",
       "3  0.605613  0.021741  0.265413  0.100253  0.098343  0.056790  \n",
       "4  0.134197  0.362929  0.153390 -0.150784  0.022146 -0.203349  \n",
       "5  0.378945  0.070726 -0.184511 -0.074120  0.327844  0.325883  \n",
       "6  0.029456 -0.129096 -0.312034 -0.101793 -0.217690 -0.227760  \n",
       "7  0.333553 -0.009735 -0.001862  0.181096  0.035173  0.060070  \n",
       "8  0.168001  0.228359  0.145979 -0.175168 -0.121800 -0.119020  \n",
       "9 -0.065761 -0.478842  0.188946  0.146038 -0.237623 -0.299823  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_vector_tfidf_weight(model):\n",
    "\n",
    "    def mean_vector_compute(sentence, tfidf):\n",
    "        for w in sentence:\n",
    "            if w in model.wv:\n",
    "                try:\n",
    "                    mv += model.wv[w] * tfidf[w][0]\n",
    "                except:\n",
    "                    mv = model.wv[w].copy()\n",
    "        ## L2规范化\n",
    "        import numpy as np\n",
    "        mv /= np.linalg.norm(mv)\n",
    "        return mv\n",
    "\n",
    "    return mean_vector_compute\n",
    "\n",
    "\n",
    "tokenized_data = map(tokenize, data['title'])\n",
    "##首先计算词袋\n",
    "count_vect = CountVectorizer(tokenizer=tokenize)\n",
    "bag_words = count_vect.fit_transform(data['title'])\n",
    "words_names = count_vect.get_feature_names()\n",
    "##计算TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(norm='l2').fit(bag_words)\n",
    "tfidf_words = tfidf_transformer.transform(bag_words)\n",
    "tfidf_words_df = pd.DataFrame.sparse.from_spmatrix(tfidf_words,\n",
    "                                                   columns=words_names)\n",
    "##带入模型\n",
    "tfidf_weight_mean_vec = mean_vector_tfidf_weight(w2v_model)\n",
    "mean_vecs = []\n",
    "for i, sentence in enumerate(tokenized_data):\n",
    "    mean_vecs.append(tfidf_weight_mean_vec(sentence,\n",
    "                                           tfidf_words_df.iloc[i, :]))\n",
    "mean_vecs = pd.DataFrame(mean_vecs)\n",
    "mean_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本距离与相似度\n",
    "\n",
    "距离和相似度在文本的概念上都是文本之间某种相似程度的度量，只不过距离（distance）通常用于比较短小的词汇、句子上的差异性有多大，而相似度则主要针对更长的文档等。\n",
    "\n",
    "## 编辑距离\n",
    "\n",
    "经过适当的向量化之后，我们可以轻松地使用向量进行向量空间的任何距离操作，距离越小代表两个字符串之间越相似。在这里我们额外介绍一种常用的距离，即基于编辑距离的Levenshtein距离，该距离度量了从一个字符串str1需要经过多少步的编辑（替换一个字符、插入一个字符、删除一个字符）才能变成另外一个字符串str2。这个步数的计算可以通过动态规划（dynamic programming）来完成。\n",
    "\n",
    "Python中可以安装Levenshtein包：\n",
    "```shell\n",
    "sudo pip3 install python-Levenshtein\n",
    "```\n",
    "\n",
    "计算Levenshtein距离："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "Levenshtein.distance('色即是空，空即是色', '色不异空，空不异色')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上给出了最小修改次数，不过最好是将其规范化：$$ratio=\\frac{len\\left(str1\\right)+len\\left(str2\\right)-distance}{len\\left(str1\\right)+len\\left(str2\\right)}$$可以使用Levenstein.ratio()计算该比例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5555555555555556"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "Levenshtein.ratio('色即是空，空即是色', '色不异空，空不异色')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编辑距离有很多用处，比如比照不同版本、侦测细微的字符串差异（比如可能存在的地址输入差异「上海松江文汇路」和「上海市松江区文汇路」）、检查抄袭等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.627906976744186"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "xj_jmls = \"观世音菩萨，行深般若波罗蜜时，照见五阴空，度一切苦厄。舍利弗，色空故，无恼坏相，受空故，无受相，想空故，无知相，行空故，无作相，识空故，无觉相。何以故？舍利弗，非色异空，非空异色，色即是空，空即是色，受想行识，亦复如是。\"\n",
    "xj_xz = \"观自在菩萨，行深般若波罗蜜多时，照见五蕴皆空，度一切苦厄。舍利子，色不异空，空不异色，色即是空，空即是色，受想行识亦复如是。\"\n",
    "Levenshtein.ratio(xj_jmls, xj_xz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本相似度\n",
    "\n",
    "不管是使用词袋模型，还是TF-IDF模型，或者使用词嵌入方法通过平均、加权平均的方式，都可以讲一个文本向量化。\n",
    "\n",
    "将文本向量化之后，计算文本之间的相似度就非常简单了：只要将两个文本的向量之间的相似度计算出来即可，常用的度量是余弦相似度（即两个向量的夹角）：$$cs\\left(u,v\\right)=\\frac{u\\cdot v}{\\left\\Vert u\\right\\Vert \\cdot \\left\\Vert v\\right\\Vert }$$其中分子为内积，分母上位L2范数的乘积。我们可以使用NumPy很快的计算出该值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cos_similarity = lambda u, v: np.inner(u, v) / (np.linalg.norm(u) * np.linalg.\n",
    "                                                norm(v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如如果我们使用词袋模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "##计算词袋\n",
    "count_vect = CountVectorizer(tokenizer=tokenize)\n",
    "bag_words = count_vect.fit_transform(data['title'])\n",
    "words_names = count_vect.get_feature_names()\n",
    "bag_words_df = pd.DataFrame.sparse.from_spmatrix(bag_words,\n",
    "                                                 columns=words_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15811388300841897"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity(bag_words_df.iloc[7, :], bag_words_df.iloc[9, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然也可以找出于某一个标题最为相似的，内存限制只算前50000个（如果要算所有的可以分开来算）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沪争取增值税扩围改革试点\n",
      "增值税扩围和资源税改革今年有望取得突破\n"
     ]
    }
   ],
   "source": [
    "##计算词袋\n",
    "count_vect = CountVectorizer(tokenizer=tokenize)\n",
    "bag_words = count_vect.fit_transform(RAW1['title'][:50000])\n",
    "words_names = count_vect.get_feature_names()\n",
    "bag_words_df = pd.DataFrame.sparse.from_spmatrix(bag_words,\n",
    "                                                 columns=words_names)\n",
    "##计算相关系数（自己用循环试一下，奇慢无比，如果不向量化计算，估计要跑的时间按天算）\n",
    "ip = np.array(np.dot(bag_words_df, bag_words_df.iloc[0, :]))  ## 这里用到了广播\n",
    "norm1 = np.array(np.linalg.norm(bag_words_df, axis=1))  ## 第一行的norm都一样，所以不用除\n",
    "corr = ip / norm1\n",
    "corr = corr[1:]\n",
    "##找最大值\n",
    "argi = np.argmax(corr)\n",
    "print(RAW1['title'].iloc[0])\n",
    "print(RAW1['title'].iloc[argi + 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在其他向量化方法下同理，在此不再赘述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类：情感分析\n",
    "\n",
    "情感分析（sentiment analysis）最初指对文本的情感，比如褒义还是贬义，以及文章中的情感倾向进行分析，实际上是文本分类的一种。实际上，我们之前学到过的所有的监督学习方法都可以使用在文本分类中。然而，监督学习需要大量的带有标签的词典，有时这种方法是行不通的，所以也会有根据特定模式对文本进行分类的方法，或者使用预训练模型的方法。比如，一个经常使用的方法是使用情感词典。我们将分别介绍使用情感词典的方法和使用监督学习的方法。\n",
    "\n",
    "## 基于词典\n",
    "\n",
    "情感词典即标记了情感得分的一个词典，这个词典可以看作是一个预训练的模型，模型的训练结果是情感得分，而我们只需要使用这些情感得分就可以得到情感的具体取值。\n",
    "\n",
    "情感词典方法的好处是非常的简单：只需要简单计算得分即可。然而缺点也是非常突出的：情感词典通常不对特殊问题进行优化，此外其标签是固定的。比如，一般情感词典也许会标记正面负面，但是当我们将其用在金融领域时，会发现“降准”这个正向词汇甚至不会出现在情感词典中，虽然这个词在金融领域应该对股票市场是一个正向词汇。\n",
    "\n",
    "此外，不同情境下也许同一个词也有不同的情感倾向，比如如果我们讨论股票，“降准”也许是一个正向词汇，然而如果我们讨论的是债券，“降准”就不见得是什么好词了。\n",
    "\n",
    "当然，情感词典在一般的领域中应用也许也可以达到比较高的精准度。在这里我们先介绍情感词典的使用方法。\n",
    "\n",
    "使用情感词典的第一步是获得情感词典，我们在这里列举了几个比较常用的情感词典：\n",
    "\n",
    "* 清华大学李军中文褒贬义词典（ http://nlp.csai.tsinghua.edu.cn/site2/index.php/13-sms ）（./Chinese/BosonNLP/）\n",
    "* 知网HowNet情感词典（./Chinese/HowNet/）\n",
    "* 玻森公司是情感词典（ http://static.bosonnlp.com/dev/resource ）（./Chinese/Tsinghua/）\n",
    "* 金融领域中文情绪词典（姚加权等，2021）（./Chinese/FinanceSenti）\n",
    "\n",
    "以上词典都可以在括号中的地址，或者括号中的路径里面找到。\n",
    "\n",
    "比如，对于一个句子，我们可以先将其使用词袋模型将其整理为词袋，然后对于每个词都是用其情感值对其进行打分，并处理否定词，最后根据每个词的打分情况汇总为这个句子的情感。\n",
    "\n",
    "比如，以HowNet的情感词典为例，其情感词典的内容大约为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Chinese/HowNet/正面情感词语（中文）.txt\", encoding='gb18030') as f:\n",
    "    posilist = f.readlines()\n",
    "del posilist[0]\n",
    "SentDict = {}\n",
    "for w in posilist:\n",
    "    SentDict[w.strip()] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码我们将所有的正面词汇给以一个数值1，代表正面，后面我们还将加载负面词汇，用-1代表负面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentDict = {}\n",
    "Files = ['面情感词语（中文）.txt', '面情感词语（英文）.txt', '面评价词语（中文）.txt', '面评价词语（英文）.txt']\n",
    "for p in ['正', '负']:\n",
    "    v = (1 if p == '正' else -1)\n",
    "    for f in Files:\n",
    "        with open(\"Chinese/HowNet/\" + p + f, encoding='gb18030') as f:\n",
    "            posilist = f.readlines()\n",
    "        del posilist[0]\n",
    "        for w in posilist:\n",
    "            if w not in SentDict:\n",
    "                SentDict[w.strip()] = v\n",
    "            else:\n",
    "                SentDict[w.strip()] += v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来导入否定词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no', '不', '没', '无', '非', '莫', '弗', '毋', '未', '否', '别', '无', '不够', '不是', '不可', '不曾', '未必', '没有', '不要', '难以', '未曾', '否认', '取消', '撤回']\n"
     ]
    }
   ],
   "source": [
    "NegaList = []\n",
    "with open(\"Chinese/negative.txt\") as f:\n",
    "    for w in f:\n",
    "        NegaList.append(w.strip())\n",
    "print(NegaList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们以大众点评的数据作为例子展示情感词典的用法，首先读入数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cus_id</th>\n",
       "      <th>comment_time</th>\n",
       "      <th>comment_star</th>\n",
       "      <th>cus_comment</th>\n",
       "      <th>kouwei</th>\n",
       "      <th>huanjing</th>\n",
       "      <th>fuwu</th>\n",
       "      <th>shopID</th>\n",
       "      <th>stars</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>comment_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>迷糊泰迪</td>\n",
       "      <td>2018-09-20 06:48:00</td>\n",
       "      <td>sml-str40</td>\n",
       "      <td>南信 算是 广州 著名 甜品店 吧 好几个 时间段 路过 都 是 座无虚席 看着 餐单 上 ...</td>\n",
       "      <td>非常好</td>\n",
       "      <td>好</td>\n",
       "      <td>好</td>\n",
       "      <td>518986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>稱霸幼稚園</td>\n",
       "      <td>2018-09-22 21:49:00</td>\n",
       "      <td>sml-str40</td>\n",
       "      <td>中午 吃 完 了 所谓 的 早茶 回去 放下 行李 休息 了 会 就 来 吃 下午茶 了 服...</td>\n",
       "      <td>很好</td>\n",
       "      <td>很好</td>\n",
       "      <td>很好</td>\n",
       "      <td>518986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>爱吃的美美侠</td>\n",
       "      <td>2018-09-22 22:16:00</td>\n",
       "      <td>sml-str40</td>\n",
       "      <td>冲刺 王者 战队 吃遍 蓉城 战队 有 特权 五月份 和 好 朋友 毕业 旅行 来 了 广州...</td>\n",
       "      <td>很好</td>\n",
       "      <td>很好</td>\n",
       "      <td>很好</td>\n",
       "      <td>518986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>姜姜会吃胖</td>\n",
       "      <td>2018-09-19 06:36:00</td>\n",
       "      <td>sml-str40</td>\n",
       "      <td>都 说来 广州 吃 糖水 就要 来南信 招牌 姜撞奶 红豆 双皮奶 牛 三星 云吞面 一楼 ...</td>\n",
       "      <td>非常好</td>\n",
       "      <td>很好</td>\n",
       "      <td>很好</td>\n",
       "      <td>518986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forevercage</td>\n",
       "      <td>2018-08-24 17:58:00</td>\n",
       "      <td>sml-str50</td>\n",
       "      <td>一直 很 期待 也 最 爱 吃 甜品 广州 的 甜品 很 丰富 很 多样 来 之前 就 一直...</td>\n",
       "      <td>非常好</td>\n",
       "      <td>很好</td>\n",
       "      <td>很好</td>\n",
       "      <td>518986</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cus_id         comment_time comment_star  \\\n",
       "0         迷糊泰迪  2018-09-20 06:48:00    sml-str40   \n",
       "1        稱霸幼稚園  2018-09-22 21:49:00    sml-str40   \n",
       "2       爱吃的美美侠  2018-09-22 22:16:00    sml-str40   \n",
       "3        姜姜会吃胖  2018-09-19 06:36:00    sml-str40   \n",
       "4  forevercage  2018-08-24 17:58:00    sml-str50   \n",
       "\n",
       "                                         cus_comment kouwei huanjing fuwu  \\\n",
       "0  南信 算是 广州 著名 甜品店 吧 好几个 时间段 路过 都 是 座无虚席 看着 餐单 上 ...    非常好        好    好   \n",
       "1  中午 吃 完 了 所谓 的 早茶 回去 放下 行李 休息 了 会 就 来 吃 下午茶 了 服...     很好       很好   很好   \n",
       "2  冲刺 王者 战队 吃遍 蓉城 战队 有 特权 五月份 和 好 朋友 毕业 旅行 来 了 广州...     很好       很好   很好   \n",
       "3  都 说来 广州 吃 糖水 就要 来南信 招牌 姜撞奶 红豆 双皮奶 牛 三星 云吞面 一楼 ...    非常好       很好   很好   \n",
       "4  一直 很 期待 也 最 爱 吃 甜品 广州 的 甜品 很 丰富 很 多样 来 之前 就 一直...    非常好       很好   很好   \n",
       "\n",
       "   shopID  stars  year  month  weekday  hour  comment_len  \n",
       "0  518986    4.0  2018      9        3     6          184  \n",
       "1  518986    4.0  2018      9        5    21          266  \n",
       "2  518986    4.0  2018      9        5    22          341  \n",
       "3  518986    4.0  2018      9        2     6          197  \n",
       "4  518986    5.0  2018      8        4    17          261  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dianping = pd.read_csv(\"csv/dianping.csv\")\n",
    "dianping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，cus_comment里面是已经分好词的句子，此外还有详细的评分数据。我们可以写一个评分函数，然后使用map()方法进行评分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def score(s):\n",
    "    w_list = str(s).split(' ')\n",
    "    multiplier = 1\n",
    "    senti = 0\n",
    "    for w in w_list:\n",
    "        if w in NegaList:\n",
    "            multiplier *= (-1)\n",
    "        if w in SentDict and w not in NegaList:\n",
    "            senti += (multiplier * SentDict[w])\n",
    "            multiplier = 1\n",
    "    return np.sign(senti)\n",
    "\n",
    "\n",
    "dianping['score'] = dianping['cus_comment'].map(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了查看分类效果，简单的可以分类求均值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>797.0</td>\n",
       "      <td>0.225847</td>\n",
       "      <td>0.866277</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>982.0</td>\n",
       "      <td>0.423625</td>\n",
       "      <td>0.817602</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>5152.0</td>\n",
       "      <td>0.629076</td>\n",
       "      <td>0.689405</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>10849.0</td>\n",
       "      <td>0.787169</td>\n",
       "      <td>0.534707</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>9067.0</td>\n",
       "      <td>0.835999</td>\n",
       "      <td>0.465336</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         score                                              \n",
       "         count      mean       std  min   25%  50%  75%  max\n",
       "stars                                                       \n",
       "1.0      797.0  0.225847  0.866277 -1.0 -1.00  1.0  1.0  1.0\n",
       "2.0      982.0  0.423625  0.817602 -1.0  0.00  1.0  1.0  1.0\n",
       "3.0     5152.0  0.629076  0.689405 -1.0  0.75  1.0  1.0  1.0\n",
       "4.0    10849.0  0.787169  0.534707 -1.0  1.00  1.0  1.0  1.0\n",
       "5.0     9067.0  0.835999  0.465336 -1.0  1.00  1.0  1.0  1.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dianping[['stars', 'score']].groupby('stars').describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到评分越高，用户的评星也越高，结果可以接受。不过即使是1星，最终的评分平均也是大于0的，所以在使用的时候也可能会出现问题，即很多评分为正的实际上却是负面评价，比如如果我们根据评分计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">stars</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>2062.0</td>\n",
       "      <td>3.337536</td>\n",
       "      <td>1.200654</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2766.0</td>\n",
       "      <td>3.755965</td>\n",
       "      <td>1.092184</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22019.0</td>\n",
       "      <td>4.072710</td>\n",
       "      <td>0.900757</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         stars                                             \n",
       "         count      mean       std  min  25%  50%  75%  max\n",
       "score                                                      \n",
       "-1      2062.0  3.337536  1.200654  1.0  3.0  3.0  4.0  5.0\n",
       " 0      2766.0  3.755965  1.092184  1.0  3.0  4.0  5.0  5.0\n",
       " 1     22019.0  4.072710  0.900757  1.0  4.0  4.0  5.0  5.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dianping[['stars', 'score']].groupby('score').describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于机器学习\n",
    "\n",
    "如果数据中存在标签，我们可以使用之前学过的机器学习方法，以标签作为输出，以我们上面介绍的提取特征的做法作为输入，通过机器学习算法进行文本分类。\n",
    "\n",
    "但是，由于文本数据的高度复杂性和非线性性、高维性，机器学习方法的选择对结果往往有较大影响。一般可以使用结合Lasso等方法的回归（linear/logistic/ordered/count/...），或者使用支持向量机、贝叶斯方法、决策树、随机森林等非线性、非参数的方法。随着神经网络的兴起，也可以通过构建神经网络的方法，结合LSTM、RNN等网络设计方法进行分类。此外，我们还可以使用BERT等预训练模型进行fine-tuning，这种预训练模型允许我们在小样本的条件下达到更好的预测效果。\n",
    "\n",
    "当然，模型是固定的，模型的好坏也很大程度上取决于特征提取的方式以及参数设定。\n",
    "\n",
    "接下来我们对以上豆瓣评分的数据进行一个简单的有监督学习。Gentzkow、Kelly和Taddy（2017）建议在样本量不是特别大的情况下，尽量少引入决策树、支持向量机等非线性方法，所以这里我们选择使用Logistic回归的方法，将评分在4星及以下的设为0，5星设为1，进行一个简单的二分类预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dianping = pd.read_csv(\"csv/dianping.csv\")\n",
    "dianping['y'] = dianping['stars'] > 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来进行一些简单的处理。虽然数据中已经帮我们做好分词，不过我们这里还是重新进行分词，并进行祛除停用词等的操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cus_id</th>\n",
       "      <th>comment_time</th>\n",
       "      <th>comment_star</th>\n",
       "      <th>cus_comment</th>\n",
       "      <th>kouwei</th>\n",
       "      <th>huanjing</th>\n",
       "      <th>fuwu</th>\n",
       "      <th>shopID</th>\n",
       "      <th>stars</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>comment_len</th>\n",
       "      <th>y</th>\n",
       "      <th>len_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>迷糊泰迪</td>\n",
       "      <td>2018-09-20 06:48:00</td>\n",
       "      <td>sml-str40</td>\n",
       "      <td>南信算是广州著名甜品店吧好几个时间段路过都是座无虚席看着餐单上密密麻麻满满当当好吃的容易范选...</td>\n",
       "      <td>非常好</td>\n",
       "      <td>好</td>\n",
       "      <td>好</td>\n",
       "      <td>518986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>184</td>\n",
       "      <td>False</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>稱霸幼稚園</td>\n",
       "      <td>2018-09-22 21:49:00</td>\n",
       "      <td>sml-str40</td>\n",
       "      <td>中午吃完了所谓的早茶回去放下行李休息了会就来吃下午茶了服务两层楼楼下只能收现金楼上可以微信支...</td>\n",
       "      <td>很好</td>\n",
       "      <td>很好</td>\n",
       "      <td>很好</td>\n",
       "      <td>518986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>266</td>\n",
       "      <td>False</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>爱吃的美美侠</td>\n",
       "      <td>2018-09-22 22:16:00</td>\n",
       "      <td>sml-str40</td>\n",
       "      <td>冲刺王者战队吃遍蓉城战队有特权五月份和好朋友毕业旅行来了广州我们都是双皮奶爱好者搜到啦最火的...</td>\n",
       "      <td>很好</td>\n",
       "      <td>很好</td>\n",
       "      <td>很好</td>\n",
       "      <td>518986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>341</td>\n",
       "      <td>False</td>\n",
       "      <td>238.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>姜姜会吃胖</td>\n",
       "      <td>2018-09-19 06:36:00</td>\n",
       "      <td>sml-str40</td>\n",
       "      <td>都说来广州吃糖水就要来南信招牌姜撞奶红豆双皮奶牛三星云吞面一楼现金二楼微信支付宝位置不少但是...</td>\n",
       "      <td>非常好</td>\n",
       "      <td>很好</td>\n",
       "      <td>很好</td>\n",
       "      <td>518986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>197</td>\n",
       "      <td>False</td>\n",
       "      <td>122.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forevercage</td>\n",
       "      <td>2018-08-24 17:58:00</td>\n",
       "      <td>sml-str50</td>\n",
       "      <td>一直很期待也最爱吃甜品广州的甜品很丰富很多样来之前就一直想着一定要过来吃到腻今天总算实现了先...</td>\n",
       "      <td>非常好</td>\n",
       "      <td>很好</td>\n",
       "      <td>很好</td>\n",
       "      <td>518986</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>261</td>\n",
       "      <td>True</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32478</th>\n",
       "      <td>darayan</td>\n",
       "      <td>2007-04-28 16:57:00</td>\n",
       "      <td>NAN</td>\n",
       "      <td>我觉得姜撞奶一般咯有苦公司岩搬过来东风西呢边下午茶仲会时不时定下甜品不过就无乜惊喜咯都系中意...</td>\n",
       "      <td>一般</td>\n",
       "      <td>一般</td>\n",
       "      <td>一般</td>\n",
       "      <td>521698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2007</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>68</td>\n",
       "      <td>False</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32479</th>\n",
       "      <td>chenjj920</td>\n",
       "      <td>2006-10-27 09:12:00</td>\n",
       "      <td>NAN</td>\n",
       "      <td>味道啦还算正宗值得一试</td>\n",
       "      <td>好</td>\n",
       "      <td>一般</td>\n",
       "      <td>一般</td>\n",
       "      <td>521698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32480</th>\n",
       "      <td>winny311111</td>\n",
       "      <td>2018-05-14 03:09:00</td>\n",
       "      <td>sml-str50</td>\n",
       "      <td>第二次来好吃第二次来好吃第二次来好吃</td>\n",
       "      <td>非常好</td>\n",
       "      <td>非常好</td>\n",
       "      <td>非常好</td>\n",
       "      <td>521698</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32481</th>\n",
       "      <td>Polykat_年嫿</td>\n",
       "      <td>2018-04-28 02:33:00</td>\n",
       "      <td>sml-str50</td>\n",
       "      <td>兒時的味道兒時的味道兒時的味道</td>\n",
       "      <td>非常好</td>\n",
       "      <td>很好</td>\n",
       "      <td>好</td>\n",
       "      <td>521698</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32482</th>\n",
       "      <td>不瘦到120斤不换ID</td>\n",
       "      <td>2018-04-09 01:22:00</td>\n",
       "      <td>sml-str50</td>\n",
       "      <td>好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝</td>\n",
       "      <td>非常好</td>\n",
       "      <td>非常好</td>\n",
       "      <td>非常好</td>\n",
       "      <td>521698</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32474 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            cus_id         comment_time comment_star  \\\n",
       "0             迷糊泰迪  2018-09-20 06:48:00    sml-str40   \n",
       "1            稱霸幼稚園  2018-09-22 21:49:00    sml-str40   \n",
       "2           爱吃的美美侠  2018-09-22 22:16:00    sml-str40   \n",
       "3            姜姜会吃胖  2018-09-19 06:36:00    sml-str40   \n",
       "4      forevercage  2018-08-24 17:58:00    sml-str50   \n",
       "...            ...                  ...          ...   \n",
       "32478      darayan  2007-04-28 16:57:00          NAN   \n",
       "32479    chenjj920  2006-10-27 09:12:00          NAN   \n",
       "32480  winny311111  2018-05-14 03:09:00    sml-str50   \n",
       "32481   Polykat_年嫿  2018-04-28 02:33:00    sml-str50   \n",
       "32482  不瘦到120斤不换ID  2018-04-09 01:22:00    sml-str50   \n",
       "\n",
       "                                             cus_comment kouwei huanjing fuwu  \\\n",
       "0      南信算是广州著名甜品店吧好几个时间段路过都是座无虚席看着餐单上密密麻麻满满当当好吃的容易范选...    非常好        好    好   \n",
       "1      中午吃完了所谓的早茶回去放下行李休息了会就来吃下午茶了服务两层楼楼下只能收现金楼上可以微信支...     很好       很好   很好   \n",
       "2      冲刺王者战队吃遍蓉城战队有特权五月份和好朋友毕业旅行来了广州我们都是双皮奶爱好者搜到啦最火的...     很好       很好   很好   \n",
       "3      都说来广州吃糖水就要来南信招牌姜撞奶红豆双皮奶牛三星云吞面一楼现金二楼微信支付宝位置不少但是...    非常好       很好   很好   \n",
       "4      一直很期待也最爱吃甜品广州的甜品很丰富很多样来之前就一直想着一定要过来吃到腻今天总算实现了先...    非常好       很好   很好   \n",
       "...                                                  ...    ...      ...  ...   \n",
       "32478  我觉得姜撞奶一般咯有苦公司岩搬过来东风西呢边下午茶仲会时不时定下甜品不过就无乜惊喜咯都系中意...     一般       一般   一般   \n",
       "32479                                        味道啦还算正宗值得一试      好       一般   一般   \n",
       "32480                                 第二次来好吃第二次来好吃第二次来好吃    非常好      非常好  非常好   \n",
       "32481                                    兒時的味道兒時的味道兒時的味道    非常好       很好    好   \n",
       "32482             好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝好喝    非常好      非常好  非常好   \n",
       "\n",
       "       shopID  stars  year  month  weekday  hour  comment_len      y  \\\n",
       "0      518986    4.0  2018      9        3     6          184  False   \n",
       "1      518986    4.0  2018      9        5    21          266  False   \n",
       "2      518986    4.0  2018      9        5    22          341  False   \n",
       "3      518986    4.0  2018      9        2     6          197  False   \n",
       "4      518986    5.0  2018      8        4    17          261   True   \n",
       "...       ...    ...   ...    ...      ...   ...          ...    ...   \n",
       "32478  521698    NaN  2007      4        5    16           68  False   \n",
       "32479  521698    NaN  2006     10        4     9           16  False   \n",
       "32480  521698    5.0  2018      5        0     3           24   True   \n",
       "32481  521698    5.0  2018      4        5     2           15   True   \n",
       "32482  521698    5.0  2018      4        0     1           38   True   \n",
       "\n",
       "       len_comment  \n",
       "0             97.0  \n",
       "1            175.0  \n",
       "2            238.0  \n",
       "3            122.0  \n",
       "4            176.0  \n",
       "...            ...  \n",
       "32478         55.0  \n",
       "32479         11.0  \n",
       "32480         18.0  \n",
       "32481         15.0  \n",
       "32482         38.0  \n",
       "\n",
       "[32474 rows x 16 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dianping['cus_comment'] = dianping['cus_comment'].str.replace(' ', '')\n",
    "dianping['len_comment'] = dianping['cus_comment'].str.len()\n",
    "dianping = dianping[dianping['len_comment'] > 1]\n",
    "dianping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "##接下来处理文本数据，我们使用简单的词袋模型作为预测特征\n",
    "import jieba\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "with open('Chinese/stopword.txt', 'rt') as f:\n",
    "    stoplist = f.readlines()\n",
    "    stoplist = [w.replace('\\n', '') for w in stoplist]\n",
    "\n",
    "\n",
    "def not_digit(w):\n",
    "    w = w.replace(',', '')\n",
    "    if re.match(r'\\d+', w) != None or re.match(r'\\d%', w) != None or re.match(\n",
    "            r'\\d*\\.\\d+', w) != None:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def tokenize(w):\n",
    "    cut_w = jieba.cut(w)\n",
    "    ## 去除停用词\n",
    "    cut_w = [\n",
    "        w.strip().lower() for w in cut_w\n",
    "        if ((w not in stoplist) and not_digit(w) and len(w.strip()) > 0)\n",
    "    ]\n",
    "    return cut_w\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer(tokenizer=tokenize, min_df=5)\n",
    "bag_words = count_vect.fit_transform(dianping['cus_comment'])\n",
    "words_names = count_vect.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们做一个简单降维操作（由于我们使用了Lasso降维，这一步也许可以不做，不过做的话可以降低运行时间）。我们将TF-IDF值比较低的特征剔除掉，从而达到降维。\n",
    "\n",
    "或者，这一步也可以考虑其他的降维方法，比如Fan(2007)的screening方法等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>一丁点</th>\n",
       "      <th>一万个</th>\n",
       "      <th>一上</th>\n",
       "      <th>一下下</th>\n",
       "      <th>一下子</th>\n",
       "      <th>一不小心</th>\n",
       "      <th>一丝</th>\n",
       "      <th>一丝丝</th>\n",
       "      <th>一两个</th>\n",
       "      <th>一两口</th>\n",
       "      <th>...</th>\n",
       "      <th>齁</th>\n",
       "      <th>齐全</th>\n",
       "      <th>齐名</th>\n",
       "      <th>齿</th>\n",
       "      <th>龍</th>\n",
       "      <th>龙</th>\n",
       "      <th>龙津</th>\n",
       "      <th>龜苓</th>\n",
       "      <th>龟</th>\n",
       "      <th>龟苓膏</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32469</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32470</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32471</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32472</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32473</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32474 rows × 11158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       一丁点  一万个   一上  一下下  一下子  一不小心   一丝  一丝丝  一两个  一两口  ...    齁   齐全   齐名  \\\n",
       "0      0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1      0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2      0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3      0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4      0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...    ...  ...  ...  ...  ...   ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "32469  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "32470  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "32471  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "32472  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "32473  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "         齿    龍    龙   龙津   龜苓    龟  龟苓膏  \n",
       "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...    ...  ...  ...  ...  ...  ...  ...  \n",
       "32469  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "32470  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "32471  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "32472  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "32473  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[32474 rows x 11158 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(bag_words)  ##使用L2范数，并fit模型（如计算IDF等）\n",
    "tfidf_words = tfidf_transformer.transform(bag_words)  ##变换数据\n",
    "tfidf_words_df = pd.DataFrame.sparse.from_spmatrix(tfidf_words,\n",
    "                                                   columns=words_names)\n",
    "tfidf_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9101416620357716\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>一丁点</th>\n",
       "      <th>一下下</th>\n",
       "      <th>一下子</th>\n",
       "      <th>一不小心</th>\n",
       "      <th>一丝</th>\n",
       "      <th>一两个</th>\n",
       "      <th>一两次</th>\n",
       "      <th>一两碗</th>\n",
       "      <th>一个个</th>\n",
       "      <th>一个多</th>\n",
       "      <th>...</th>\n",
       "      <th>齁</th>\n",
       "      <th>齐全</th>\n",
       "      <th>齐名</th>\n",
       "      <th>齿</th>\n",
       "      <th>龍</th>\n",
       "      <th>龙</th>\n",
       "      <th>龙津</th>\n",
       "      <th>龜苓</th>\n",
       "      <th>龟</th>\n",
       "      <th>龟苓膏</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32469</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32470</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32471</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32472</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32473</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32474 rows × 7810 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       一丁点  一下下  一下子  一不小心   一丝  一两个  一两次  一两碗  一个个  一个多  ...    齁   齐全   齐名  \\\n",
       "0      0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1      0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2      0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3      0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4      0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...    ...  ...  ...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "32469  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "32470  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "32471  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "32472  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "32473  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "         齿    龍    龙   龙津   龜苓    龟  龟苓膏  \n",
       "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...    ...  ...  ...  ...  ...  ...  ...  \n",
       "32469  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "32470  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "32471  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "32472  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "32473  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[32474 rows x 7810 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_sum = tfidf_words_df.sum(axis=0)\n",
    "quantile_tfidf = tfidf_sum.quantile(0.3)\n",
    "print(quantile_tfidf)\n",
    "tfidf_sub = tfidf_sum[tfidf_sum > quantile_tfidf]\n",
    "tfidf_words_df = tfidf_words_df.loc[:, tfidf_sub.index]\n",
    "tfidf_words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们训练模型，选取前30000条作为训练集，剩下的作为测试集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练模型......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型训练完成......\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##训练模型\n",
    "print(\"开始训练模型......\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(tfidf_words_df.iloc[:30000,:], dianping['y'].iloc[:30000])  ## 训练模型\n",
    "LR.coef_\n",
    "print(\"模型训练完成......\")\n",
    "## 预测及概率\n",
    "dianping['prob']=LR.predict_proba(tfidf_words_df)[:,1]\n",
    "dianping['pred']=dianping['prob']>=0.5\n",
    "# 画图\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, threshold = roc_curve(dianping['y'][30000:], dianping['prob'][30000:])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set()\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0)\n",
    "plt.plot(fpr,tpr,label='ROC, AUC=%.2f' % roc_auc)\n",
    "plt.legend(loc='upper left', frameon=True)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，以上模型没有进行调参，模型仍有改进的空间。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
