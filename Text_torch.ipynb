{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络与文本分析\n",
    "\n",
    "如果像以上的点评数据，有星级、正负面的信息，我们当然可以使用机器学习的所有方法，结合词袋、TF-IDF以及词嵌入、深度学习等方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = []\n",
    "with open(\"Chinese/stopword.txt\") as f:\n",
    "    for w in f:\n",
    "        stop_list.append(w.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_words = {}\n",
    "for sentence in dianping['cus_comment']:\n",
    "    ws = str(sentence).split(' ')\n",
    "    for w in ws:\n",
    "        wstrip = w.strip()\n",
    "        if wstrip not in stop_list:\n",
    "            if wstrip not in ALL_words:\n",
    "                ALL_words[wstrip] = 1\n",
    "            else:\n",
    "                ALL_words[wstrip] += 1\n",
    "print(len(ALL_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_code = {}\n",
    "code_len = 1\n",
    "for k in ALL_words:\n",
    "    if ALL_words[k] > 5:\n",
    "        if k not in word_code:\n",
    "            word_code[k] = code_len\n",
    "            code_len += 1\n",
    "\n",
    "print(code_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_series = []\n",
    "word_len = []\n",
    "for sentence in dianping['cus_comment']:\n",
    "    sentence_series = []\n",
    "    ws = str(sentence).split(' ')\n",
    "    for w in ws:\n",
    "        wstrip = w.strip()\n",
    "        if wstrip in word_code:\n",
    "            sentence_series.append(word_code[wstrip])\n",
    "    word_series.append(sentence_series)\n",
    "    word_len.append(len(sentence_series))\n",
    "dianping['word_series'] = word_series\n",
    "dianping['word_len'] = word_len\n",
    "dianping = dianping[dianping['word_len'] > 0]\n",
    "sub_dianping = dianping[['stars', 'word_series']]\n",
    "sub_dianping = sub_dianping.dropna()\n",
    "sub_dianping['random'] = np.random.random(sub_dianping.shape[0])\n",
    "sub_dianping = sub_dianping.sort_values('random')\n",
    "del sub_dianping['random']\n",
    "sub_dianping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个比较棘手的问题是，由于不同文本的长度是不一样的，所以在输入给神经网络的时候很容易出问题，这里我们可以使用pad的方法将短的句子用0进行填充，从而得到一个等长的序列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([1])\n",
    "c = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "# 记录长度\n",
    "tensors_with_len = [(i, len(i)) for i in [a, b, c]]\n",
    "tensors_with_len.sort(key=lambda t: t[1], reverse=True)\n",
    "print(tensors_with_len)\n",
    "tensors = [t[0] for t in tensors_with_len]\n",
    "lens = [t[1] for t in tensors_with_len]\n",
    "# 进行padding\n",
    "padded_tensor = nn.utils.rnn.pad_sequence(tensors)\n",
    "padded_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到评价为负和正的之间，平均星级并没有很大差别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad以后可以进行embedding操作，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(7, 2, padding_idx=0)  # 指定0为pad\n",
    "embedded = embedding(padded_tensor)\n",
    "print(embedded.shape)\n",
    "embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将序列交给RNN或者LSTM模型时，可以将以上padded tensor进行打包（pack）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 长度必须从大到小排序\n",
    "packed_tensor1 = nn.utils.rnn.pack_padded_sequence(embedded, lengths=lens)\n",
    "packed_tensor1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如此，在RNN或者LSTM中，就不会对如果需要从packed还原，只需要使用unpack就可以了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.utils.rnn.pad_packed_sequence(packed_tensor1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用如上特性，我们可以定义数据了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "Y_train = sub_dianping.iloc[:20000, 0] >= 4\n",
    "X_train = sub_dianping.iloc[:20000, 1]\n",
    "Y_test = sub_dianping.iloc[20000:, 0] >= 4\n",
    "X_test = sub_dianping.iloc[20000:, 1]\n",
    "\n",
    "\n",
    "class dianping_data(Dataset):\n",
    "\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = (self.X.iloc[i], len(self.X.iloc[i]), self.Y.iloc[i])\n",
    "        return x\n",
    "\n",
    "\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不过值得注意的是，如果使用默认的Dataloader，会在最终将每一条数据合并为一个Tensor，而由于我们这里的数据是变长的，还需要进行padding等操作，所以我们先不用原来的dataloader的collate_fn，而是定义一个新的clolate_fn，在其中完成排序、padding的过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_data = dianping_data(X_train, Y_train)\n",
    "\n",
    "\n",
    "def collate(x):\n",
    "    x.sort(key=lambda t: t[1], reverse=True)\n",
    "    X = [torch.tensor(t[0]).long() for t in x]\n",
    "    L = [t[1] for t in x]\n",
    "    Y = [t[2] for t in x]\n",
    "    X = nn.utils.rnn.pad_sequence(X)\n",
    "    Y = torch.tensor(Y).long()\n",
    "    return X, L, Y\n",
    "\n",
    "\n",
    "dl = DataLoader(train_data, batch_size=batch_size, collate_fn=collate)\n",
    "for x, l, y in dl:\n",
    "    print(x)\n",
    "    print(l)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "train_data = dianping_data(X_train, Y_train)\n",
    "dl = DataLoader(train_data,\n",
    "                shuffle=True,\n",
    "                batch_size=batch_size,\n",
    "                pin_memory=True,\n",
    "                num_workers=15,\n",
    "                collate_fn=collate)\n",
    "\n",
    "test_data = dianping_data(X_test, Y_test)\n",
    "tdl = DataLoader(test_data,\n",
    "                 shuffle=False,\n",
    "                 batch_size=batch_size,\n",
    "                 collate_fn=collate,\n",
    "                 drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来就可以定义模型了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# device=torch.device(\"cpu\")\n",
    "class classifier(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 word_count,\n",
    "                 batch_size,\n",
    "                 embedding_size=30,\n",
    "                 lstm_hidden_size=56,\n",
    "                 num_nurons=128):\n",
    "        super(classifier, self).__init__()\n",
    "        self.batch_size = batch_size  #批大小\n",
    "        self.embedding_size = embedding_size  #嵌入层词向量大小\n",
    "        self.lstm_hidden_size = lstm_hidden_size  #隐藏状态大小\n",
    "\n",
    "        self.embedding = nn.Embedding(word_count,\n",
    "                                      embedding_size,\n",
    "                                      padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size,\n",
    "                            hidden_size=lstm_hidden_size,\n",
    "                            num_layers=3)\n",
    "        self.layer3 = nn.Sequential(nn.Linear(lstm_hidden_size, num_nurons),\n",
    "                                    nn.LayerNorm(num_nurons), nn.Sigmoid(),\n",
    "                                    nn.LayerNorm(num_nurons), nn.Dropout(0.5),\n",
    "                                    nn.Linear(num_nurons, 2), nn.Tanh())\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "\n",
    "# nn.Linear(num_nurons,num_nurons),nn.LeakyReLU(inplace=True),\n",
    "\n",
    "    def init_hidden(self):\n",
    "        h = torch.zeros(3, self.batch_size, self.lstm_hidden_size).to(device)\n",
    "        c = torch.zeros(3, self.batch_size, self.lstm_hidden_size).to(device)\n",
    "        return (h, c)\n",
    "\n",
    "    def forward(self, x, l):\n",
    "        x = self.embedding(x)\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, lengths=l)\n",
    "        lstm_out, (h, c) = self.lstm(x, self.hidden)\n",
    "        y = self.layer3(h[-1]) * 8  # 为避免出现nan，做一个截断，要求y只能在-8到8之间\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，定义学习率和损失函数，进行求解："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classifier(code_len, batch_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "losses = []\n",
    "Exp_Smoothed_Loss = 0\n",
    "for i in range(1000):\n",
    "    model.train()\n",
    "    for x, l, y in dl:\n",
    "        # 将x计算预测值\n",
    "        y_pred = model(x.to(device), l)\n",
    "        # 计算损失\n",
    "        loss = criterion(y_pred, y.to(device))\n",
    "        losses.append(loss.item())\n",
    "        if i == 0:\n",
    "            Exp_Smoothed_Loss = loss.item()\n",
    "        else:\n",
    "            Exp_Smoothed_Loss = 0.01 * loss.item() + 0.99 * Exp_Smoothed_Loss\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    if i % 20 == 0:\n",
    "        model.eval()\n",
    "        pred = torch.tensor([]).to(device)\n",
    "        true_value = torch.tensor([])\n",
    "        for x, l, y in tdl:\n",
    "            p = model(x.to(device), l)\n",
    "            true_value = torch.cat([true_value, y])\n",
    "            pred = torch.cat([pred, p])\n",
    "        pred = torch.exp(pred)\n",
    "        pred = pred / (torch.sum(pred, axis=1).unsqueeze(1))\n",
    "        result = pd.DataFrame({\n",
    "            'true_value':\n",
    "            true_value.squeeze(-1).numpy(),\n",
    "            'Predicted':\n",
    "            pred[:, 1].squeeze(-1).cpu().detach().numpy()\n",
    "        })\n",
    "        oos_los = -np.mean(result['true_value'] * np.log(result['Predicted']) +\n",
    "                           (1 - result['true_value']) *\n",
    "                           np.log(1 - np.log(result['Predicted'])))\n",
    "        print(\"第%s次epoch，Smoothed Loss=%s，LR=%s，out-of-sample Loss=%s\" %\n",
    "              (i, Exp_Smoothed_Loss, lr_scheduler.get_last_lr(), oos_los))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15.0, 5.0)\n",
    "\n",
    "i = np.arange(len(losses)) + 1\n",
    "plt.plot(i, np.array(losses))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
